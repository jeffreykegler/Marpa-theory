% Copyright 2015 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt,openany,twoside]{amsbook}
% \RequirePackage[l2tabu, orthodox]{nag}
\usepackage{ragged2e}
\usepackage{mathtools}
\usepackage{tikz-qtree}
\usepackage{amssymb}
\usepackage{amsmidx}
\usepackage{url}
\usepackage{amsfonts}% to get the \mathbb alphabet
\usepackage{float}
% \usepackage[columns=1]{idxlayout}
\usepackage[pdfpagelabels]{hyperref}
\usepackage{algpseudocode}
\usepackage{algorithm}

% for escaping in index commands ...
%    the quote sign does not seem to work in amsmidx
\newcommand{\Pipe}{\charâ€˜\|}
\makeindex{recce-general}
\makeindex{recce-algorithms}
\makeindex{recce-theorems}
\makeindex{recce-definitions}
\makeindex{recce-notation}

% This is used to find the font size if need be
% its uses are usually commented out
\makeatletter
\newcommand\thefontsize[1]{{#1 The current font size is: \f@size pt\par}}
\makeatother

% \DeclareMathSizes{6}{6}{6}{6}
% \DeclareMathSizes{8}{8}{8}{8}
% \DeclareMathSizes{10}{10}{10}{10}
% \DeclareMathSizes{10.95}{10.95}{10.95}{10.95}
% \DeclareMathSizes{12}{12}{12}{12}
% \DeclareMathSizes{14.4}{14.4}{14.4}{14.4}
% \DeclareMathSizes{20.74}{20.74}{20.74}{20.74}
% \DeclareMathSizes{24.88}{24.88}{24.88}{24.88}

\newcommand{\todo}[1]{\par{\large\textbf Todo: #1}\par}
\newcommand{\myfnname}[1]{\texttt{#1}}
\newcommand{\myopname}[1]{\texttt{#1}}
\newcommand{\myfn}[2]{\ensuremath{\myfnname{#1}(#2)}}
% definitions for "elements"
\newcommand{\el}[2]{\ensuremath{\texttt{#1}[#2]}}
\newcommand{\Vel}[2]{\el{#1}{\var{#2}}}
\newcommand{\op}[2]{\ensuremath{\texttt{#1}(#2)}}
\newcommand{\bigop}[2]{\ensuremath{\texttt{#1}\big(#2\big)}}
\newcommand{\Vop}[2]{\op{#1}{\var{#2}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{#1}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\xdfn}[2]{\textbf{#1}\index{recce-definitions}{#2}}
\newcommand{\dfn}[1]{\xdfn{#1}{#1}}
\newcommand{\sdfn}[1]{{\textbf{#1}}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{}
\newcommand{\Cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\LRk}{\ensuremath{\var{LR}(\var{k})}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,{}-\,}
\newcommand{\xxsubtract}[2]{\ensuremath{#1\,{}-\,#2}}
\newcommand{\decr}[1]{\xxsubtract{#1}{1}}
\newcommand{\Vdecr}[1]{\decr{\var{#1}}}
\newcommand{\incr}[1]{\ensuremath{#1 + 1}}
\newcommand{\Vincr}[1]{\incr{\var{#1}}}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\de}{\rightarrow}
\newcommand{\derives}{\Rightarrow}
\newcommand{\nderives}{\not\Rightarrow}
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\Rightarrow\!}\:$}}}
\newcommand{\nxderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\not\Rightarrow\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\ndestar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\not\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\ndeplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\not\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}
\newcommand{\drv}[3]{\ensuremath{\texttt{#1}[#2][#3]}}
\newcommand{\drvVV}[3]{\drv{#1}{\var{#2}}{\var{#3}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\leo}[1]{#1_{LEO}}
\newcommand{\Vleo}[1]{\ensuremath{\var{#1}_{LEO}}}
\newcommand{\inst}[1]{#1_{INST}}
\newcommand{\Vinst}[1]{\ensuremath{\var{#1}_{INST}}}
\newcommand{\Ees}[1]{\ensuremath{#1_{ES}}}
\newcommand{\loc}[1]{\ensuremath{{#1}_{LOC}}}
\newcommand{\Vloc}[1]{\loc{\var{#1}}}
\newcommand{\Ves}[1]{\Ees{\var{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vruleset}[1]{\ensuremath{\var{#1}_{\set{RULE}}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\str}[1]{\ensuremath{#1_{STR}}}
\newcommand{\Vstr}[1]{\ensuremath{\langle\langle\var{#1}\rangle\rangle}}
\newcommand{\sym}[1]{\ensuremath{#1_{SYM}}}
\newcommand{\Vsym}[1]{\ensuremath{\langle\var{#1}\rangle}}
\newcommand{\Vorig}[1]{\ensuremath{\var{#1}_{ORIG}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\mk}[1]{\ensuremath{\lfloor#1\rceil}}
\newcommand{\bigmk}[1]{\big\lfloor#1\big\rceil}
% \newcommand{\mkdelim}{\,\,}
\newcommand{\mkdelim}{}
\newcommand{\mkr}[1]{\mkdelim \mk{#1}}
\newcommand{\mkl}[1]{\mk{#1} \mkdelim}
\newcommand{\mkm}[1]{\mkdelim \mk{#1} \mkdelim}
\newcommand{\Vmk}[1]{\mk{\var{#1}}}
\newcommand{\Vmkl}[1]{\Vmk{#1} \mkdelim}
\newcommand{\Vmkr}[1]{\mkdelim \Vmk{#1}}
\newcommand{\Vmkm}[1]{\mkdelim \Vmk{#1} \mkdelim}

\newcommand{\firstix}[1]{\ensuremath{\lfloor{#1}\rfloor}}
\newcommand{\Vfirstix}[1]{\firstix{\var{#1}}}
\newcommand{\lastix}[1]{\ensuremath{\lceil{#1}\rceil}}
\newcommand{\Vlastix}[1]{\lastix{\var{#1}}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}

\newcommand{\Alt}[1]{\ensuremath{\myfnname{Alt}(#1)}}
\newcommand{\Cdr}{\var{dr}}
\newcommand{\Cg}{\var{g}}
\newcommand{\Crules}{\var{rules}}
\newcommand{\Current}[1]{\ensuremath{\myfnname{Current}(#1)}}
\newcommand{\CVw}[1]{\ensuremath{\sym{\Cw[\var{#1}]}}}
\newcommand{\CW}{\ensuremath{\mathbb W}}
\newcommand{\Cw}{\var{w}}
\newcommand{\Dotix}[1]{\ensuremath{\myfnname{Dotix}(#1)}}
\newcommand{\DR}[1]{\ensuremath{\myfnname{DR}(#1)}}
\newcommand{\GOTO}{\ensuremath{\myfnname{GOTO}}}
\newcommand{\ID}[1]{\myfnname{ID}(#1)}
\newcommand{\Layer}[1]{\ensuremath{\myfnname{Layer}(#1)}}
\newcommand{\Left}[1]{\ensuremath{\myfnname{Left}(#1)}}
\newcommand{\LHS}[1]{\ensuremath{\myfnname{LHS}(#1)}}
\newcommand{\Memoable}[1]{\ensuremath{\myfnname{Memoable}(#1)}}
\newcommand{\Memoized}[1]{\ensuremath{\myfnname{Memoized}(#1)}}
\newcommand{\myL}[1]{\myfnname{L}(#1)}
\newcommand{\Next}[1]{\myfnname{Next}(#1)}
\newcommand{\Origin}[1]{\ensuremath{\myfn{Origin}{#1}}}
\newcommand{\Penult}[1]{\myfnname{Penult}(#1)}
\newcommand{\Postdot}[1]{\ensuremath{\myfnname{Postdot}(#1)}}
\newcommand{\Predict}[1]{\myfnname{Predict}(#1)}
\newcommand{\Predot}[1]{\ensuremath{\myfnname{Predot}(#1)}}
\newcommand{\Prev}[1]{\myfnname{Prev}(#1)}
\newcommand{\PSL}[2]{\myfnname{PSL}[#1][#2]}
\newcommand{\RHS}[1]{\ensuremath{\myfnname{RHS}(#1)}}
\newcommand{\Right}[1]{\ensuremath{\myfnname{Right}(#1)}}
\newcommand{\Symbol}[1]{\ensuremath{\myfnname{Symbol}(#1)}}
\newcommand{\Valid}[1]{\ensuremath{\myfnname{Valid}(#1)}}

\newcommand\Ctables{\var{tables}}
\newcommand\Vtables[1]{\Ctables[\alg{#1}]}

\newcommand\Etable[1]{\ensuremath{\myfnname{table}[#1]}}
\newcommand\bigEtable[1]{\ensuremath{\myfnname{table}\bigl[#1\bigr]}}
\newcommand\Rtable[1]{\ensuremath{\myfnname{table}[#1]}}
\newcommand\Rtablesize[1]{\ensuremath{\bigl| \myfnname{table}[#1] \bigr|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\myfnname{table}[#1,#2]}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

% My macros for definition & theorem titles.
% The 'q' forms are 'quiet' -- they do not index
% Don't use definition titles for index -- use \xdfn instead.
% \newcommand\dtitle[1]{\textbf{#1}:\index{recce-definitions}{#1}}
\newcommand\dtitle[1]{\textbf{#1}:}
\newcommand\qdtitle[1]{\textbf{#1}:}
\newcommand\ttitle[1]{\textbf{#1}:\index{recce-theorems}{#1}}
\newcommand\ltitle[1]{\textbf{#1}:\index{recce-theorems}{#1}}

% I don't like to put whole paragraphs in italics,
% so I make this simple variation on the "plain" theoremstyle
\newtheoremstyle{myplain}
  {10pt}   % ABOVESPACE
  {10pt}   % BELOWSPACE
  {\normalfont}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC

\theoremstyle{myplain}

% \newtheorem{oldtheorem}{Theorem}[chapter]
\newtheorem{oldtheorem}{Theorem}
\newenvironment{theorem}
{
  \begin{oldtheorem}
}
{
  % \begin{center}
  % \vspace{-.4\baselineskip}
  % \rule{3em}{.5pt}
  % \end{center}
  \end{oldtheorem}
}

\newtheorem{lemma}[oldtheorem]{Lemma}

\newtheorem{baredefinition}[oldtheorem]{Definition}
\newenvironment{definition}{ \begin{baredefinition}
}{ \begin{center}
\vspace{-.4\baselineskip}
\rule{3em}{.5pt}
\end{center} \end{baredefinition} }

\newtheorem{bareobservation}[oldtheorem]{Observation}
\newenvironment{observation}{ \begin{bareobservation}
}{ \begin{center}
\vspace{-.4\baselineskip}
\rule{3em}{.5pt}
\end{center} \end{bareobservation} }

\newtheorem{bareconstruction}[oldtheorem]{Construction}
\newenvironment{construction}{ \begin{bareconstruction}
}{ \begin{center}
\vspace{-.4\baselineskip}
\rule{3em}{.5pt}
\end{center} \end{bareconstruction} }

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}
\hyphenation{al-ti-tude}

% I use parboxes in equations.  This sets a useful width for them.
\newlength{\mathparwidth}
\newlength{\longtagwidth}
\settowidth{\longtagwidth}{(999)\qquad}
\setlength{\mathparwidth}{\dimexpr\textwidth-\longtagwidth}
\newcommand{\myparbox}[1]{
  \parbox{\mathparwidth}{ \begin{FlushLeft}
#1 \par
\end{FlushLeft} }
}

% \makeindex
\begin{document}

\date{\today}

\title{The Marpa recognizer: a simplification}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2015 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
\textbf{
This is a draft.
}
Marpa is
a practical and fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
The Marpa recognizer is the first
practical implementation
of the improvements
to Earley's algorithm found in
Joop Leo's%
\index{recce-general}{Leo, Joop}
1991 paper.
Marpa has a new parse engine that
allows the user to alternate
between Earley-style syntax-driven parsing,
and procedural parsing of the type available
with recursive descent.
Marpa is left-eidetic so that,
unlike recursive descent,
Marpa's procedural logic has available
full information about
the state of the parse so far.
The Marpa recognizer described
is a simplification of that
in our 2013 paper~\cite{Marpa-2013}.
\end{abstract}

\maketitle

\chapter{Status of this paper}
This paper is a draft.
Please use the date at the bottom of the
first page
to make sure this is the latest revision.
If this revision is not the latest, please ignore it.
If this does seems to be the latest version,
and you are adventurous,
then read on.
Chapters
\ref{ch:introduction},
\ref{ch:preliminaries},
\ref{ch:rewrite},
\ref{ch:dotted},
\ref{ch:earley-items},
\ref{ch:tethers}
and
\ref{ch:earley-tables}
are in the ``advanced draft'' stage.
Advanced draft chapters are still subject to revision,
but the author hopes they are stable enough
to make comments and corrections useful.
Readers should note that changes in early draft chapters
sometimes require changes
to chapters whose content was thought to
be settled.
Therefore, it is possible that
chapters in advanced draft status
will change dramatically.

Chapters
\ref{ch:silos},
\ref{ch:altitude},
\ref{ch:leo},
\ref{ch:pseudocode},
\ref{ch:correctness},
\ref{ch:linear},
and
\ref{ch:other-complexity}
are in \dfn{early draft} stage.
``Early draft'' means that the author's thoughts are not well settled,
and the chapters are likely to contain inconsistencies and errors.
Comments and corrections on early draft
chapters are not encouraged ---
the material may be already slated for deletion,
rewriting or rethinking.

\chapter{Introduction}
\label{ch:introduction}

The Marpa project was intended to create
a practical, highly available, tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite a devoted academic literature,
no such tool had existed for context-free parsing.

The first stable version of Marpa was uploaded to
a public archive on Solstice Day 2011.
This paper describes the algorithm used
in the most recent version of Marpa,
Marpa::R2~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in our earlier paper~\cite{Marpa-2013}.

\section{A proven algorithm}

While the presentation in this document is theoretical,
the approach is practical.
The Marpa::R2 implementation has been widely available
for some time,
and has seen considerable use,
including in production environments.
Many of the ideas in the parsing literature
satisfy theoretical criteria,
but in practice turn out to face significant obstacles.
An algorithm may be as fast as reported, but may turn
out not to allow
adequate error reporting.
Or a modification may speed up the recognizer,
but require additional processing at evaluation time
which undoes the speed advantage,
leaving no compensating advantage for
its additional complexity.

In this document, we describe the Marpa
algorithm,
as it has been implemented in Marpa::R2.
In many cases,
we believe there are approaches better than those we
have described.
From our point of view, these techniques,
however solid their theory,
are conjectures.
When we mention a technique
that is not implemented in
Marpa::R2,
we will always explicitly state that
that technique is not in Marpa as implemented.

\section{Features}

\subsection{General context-free parsing}
As implemented,
Marpa parses
all ``proper'' context-free grammars.
The
proper context-free grammars are those which
are free of cycles,
unproductive symbols,
and
inaccessible symbols.
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},
and therefore never worse than $\order{\var{n}^3}$.

\subsection{Linear time for practical grammars}
Currently, the grammar suitable for practical
use are thought to be a subset
of the determistic context-free grammars.
Using a technique discovered by
Leo~\cite{Leo1991},
Marpa parses all of these in linear time.
Leo's modification of Earley's algorithm is
\On{} for LR-regular grammars.
Leo's modification
also parses many ambiguous grammars in linear
time.

\subsection{Left-eidetic}
The original Earley algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
Marpa's parse engine makes available
its
information about the state of the parse so far.
This information is
in useful form,
and can be accessed efficiently.

\subsection{Recoverable from read errors}
When
Marpa reads a token which it cannot accept,
the error is fully recoverable.
An application can try to read another
token.
The application can do this repeatedly
for as long as the token is rejected.
Once the application provides
an acceptable token,
parsing will continue
as if the rejected scan attempt had never been made.

\subsection{Ambiguous tokens}
Marpa allows ambiguous tokens.
These are often useful in natural language processing
where, for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
with recovery from rejected tokens so that,
for example, an application could react to the
rejection of a token by reading two others,
and letting the parser determine which one is
correct.

\section{Using the features}

\subsection{Error reporting}
An obvious application of left-eideticism is error
reporting.
Marpa's abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar, or at least in the input.
Marpa, as currently implemented,
will detect an ambiguity and report
specifically where it occurred
and what the alternatives were.

\subsection{Event driven parsing}
As implemented,
Marpa::R2~\cite{Marpa-R2},
allows the user to define events.
Events can defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\subsection{Ruby slippers parsing}
Left-eideticism, efficient error recovery
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
Unlike in traditional parser practice,
where error detection is an act of desperation,
Marpa's error detection can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of the Marpa::R2~\cite{Marpa-R2},
the author has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\subsection{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very unlike the practice in the languages humans choose
when communicating with each other.
Human languages exploit ambiguity in order to design highly flexible,
powerfully expressive languages.
For example,
the language of this document, English, is notoriously
ambiguous.

Ambiguity of course can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Syntax errors, for example, are undesired, but nobody tries
to design languages to make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be cumbersome and even dangerous:
all typos in such a language would be meaningful,
and parser would never warn the user about errors, because
there would be no such thing.

With Marpa, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
The language can be designed to be ambiguous,
but any actual ambiguity can be detected
and reported at parse time.
This exploits Marpa's ability
to report exactly where
and what the ambiguity is.
Marpa::R2 own parser description language, the SLIF,
uses ambiguity in this way.

\subsection{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LR-regular languages
opens the way to auto-generated languages.
In particular,
\cite{Culik1973} notes that a parser which
can parse any LR-regular language will be
able to parse a language generated using syntax macros.

\subsection{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
which are useful for second-order programming.
True second-order languages --- languages which
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With Marpa, this barrier is raised.
As an example,
Marpa::R2's own parser description language, the SLIF,
allows precedenced rules,
rules which are specified in an extended BNF,
where the extension allows precedence and associativity
to be specified for each RHS.

Marpa::R2's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to create a BNF grammar which
is equivalent and which
has the desired precedence.
Essentially, the SLIF does the usual textbook
transformation of rules with precedence and
associativity specified,
into pure BNF.
The SLIF's advantage is that it is powered by Marpa,
and therefore can expect the grammar it auto-generates to
parse in linear time.

Notationally, Marpa's precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison, but in the SLIF there are two important
differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity above three.

Second, and more important, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies,
while the user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\section{How to read this document}

Chapter
\ref{ch:preliminaries} describes the notation and conventions
of this document.
Chapter \ref{ch:rewrite} deals with Marpa's
grammar rewrites.
The next three sections develop the ideas for Earley's algorithm.
Chapter \ref{ch:dotted} describes dotted rules.
Chapter \ref{ch:earley-items} describes Earley items.
Chapter \ref{ch:tethers} introduces tethers,
which are chains of top-down causation.
Chapter \ref{ch:earley-tables} describes
the remaining ideas behind
basic Earley implementations,
including
Earley sets
and Earley tables.
Chapter \ref{ch:silos} introduces silos.
Like tethers, silos are chains of causation,
but unlike tethers, in silos the causation is
largely bottom-up.
Chapter \ref{ch:altitude} introduces the related idea
of the ``altitude'' of Earley items.

Chapter \ref{ch:leo} describes Leo's modification
to Earley's algorithm.
Chapter \ref{ch:pseudocode} presents the pseudocode
for Marpa's recognizer.
Chapter
\ref{ch:correctness}
contains a proof of Marpa's correctness.
Chapter \ref{ch:linear} sets out our linear
time and space complexity results.
Chapter \ref{ch:other-complexity} presents out
other complexity results.

Because of its immediate practical applications,
we expect this document to be of interest to many
who do not ordinarily read documents with this
level of mathematical apparatus.
For those readers, we offer some suggestions
which will be well known to our more mathematical
readers.

In most fields, texts are intended to be,
and often are, read through to the end,
starting at page one.
A monograph of this kind is rarely
read that way.
Even the most mathematical sophisticated
reader will skip most or all
of the proofs
on a first reading.
And a mathematically inclined
reader will usually
not read a proof line-by-line
unless and until her previous readings
have convinced her that the proof
is of sufficient
interest to deserve this kind of
attention.

This is not to say that we think
that the proofs are unimportant.
The proofs explore how our ideas
and claims
are connected to each other.
There is a theoretical satisfaction in
this deeper level of knowledge.
And the proofs
increase our
assurance that our claims are, in fact, true.

But the proofs are also of practical use,
even to the programmer who is willing to
take our word for everything in these pages.
If, when coding,
the programmer only knows ``what'' and ``how'',
he will find it hard to keep all of this materinal
in his mind at once.
If the programmer knows,
not just ``what'' and ``how'',
but ``why'',
he will understand the connections among
these ideas.
When the programmer
understands ``why'',
the book is always open in front of him,
turned to whatever page it is that
he needs.

We expect
most readers of this monograph
to have a practical bent.
For those readers,
one way to start is to read the preliminary
material for as long as it seems relevant,
skipping the proofs.
When impatience for something ``closer to
the metal'' arises,
this reader should
jump ahead
to the pseudocode in
Chapter \ref{ch:pseudocode}.

\chapter{Preliminaries}
\label{ch:preliminaries}

We assume familiarity with the theory of parsing.
Earley's algorithm is described in full,
but previous familiarity will be helpful.

\section{Notation}

This document will
use subscripts to indicate commonly occurring types.
\begin{center}
\begin{tabular}{ll}
$\var{X}_T$ & The variable \var{X} of type \type{T} \\
$\var{set-one}_\set{T}$ & The variable \var{set-one} of type set of \type{T} \\
\type{SYM} & The type for a symbol \\
\type{STR} & The type for a string \\
\type{EIM} & The type for an Earley item \\
\sym{\var{a}} & A variable \var{a} of type \type{SYM} \\
\str{\var{a}} & A variable \var{a} of type \type{STR} \\
\Veim{\var{a}} & A variable \var{a} of type \type{EIM} \\
\Vsymset{set-two} & The variable \var{set-two}, a set of strings \\
\Veimset{set-two} & The variable \var{set-two}, a set of Earley items
\end{tabular}
\end{center}
Strings and symbols occur frequently and have a special
notation:
\begin{center}
\begin{tabular}{ll}
\Vsym{a} & \var{a}, a symbol variable \\
\Vstr{a} & \var{a}, a string variable
\end{tabular}
\end{center}%
\index{recce-notation}{<<sym>>@\Vstr{sym}}%
\index{recce-notation}{<str>@\Vsym{sym}}
Subscripts may be omitted when the type
is obvious from the context.
The notation for
constants is the same as that for variables.
Multi-character variable names will be common.
\begin{center}
\begin{tabular}{ll}
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \Cat \var{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\end{tabular}
\end{center}
Operations are never implicit, with the exception of concatenation.
Concatenation is not shown when its use is obvious.
Type names are often used in the text
as a convenient way to refer to
their type.

Where $\myfnname{f}$ is a function,
we use the notation $\myfnname{f}^{\displaystyle \var{n}}$
for the iterated function.
\begin{align*}
\myfnname{f}^0(\var{x}) \quad & \defined \quad \var{x} \\
\myfnname{f}^1(\var{x}) \quad & \defined \quad \myfnname{f}(\var{x}) \\
\myfnname{f}^2(\var{x}) \quad & \defined \quad
\myfnname{f}(\myfnname{f}(\var{x})), \quad \text{etc.} \\
\myfnname{f}^\ast \quad & \defined \quad
\myfnname{f}^\var{n} \quad \text{for some $\var{n} \ge 0$} \\
\myfnname{f}^+ \quad & \defined \quad
\myfnname{f}^\var{n} \quad \text{for some $\var{n} \ge 1$}
\end{align*}

The statements of this document often require us to introduce
many new variables at once,
so that we might say,
``for some \var{a}, \var{b}, \var{c}, \ldots{} \var{z},
let \ldots{}''.
When we introduce an definition, and it
contains new variables
which cause no loss of generality,
we will prefer to simply say so,
noting any exceptions.
In cases where brevity is important,
such as in some of the proofs,
we will abbreviate
``without loss of generality'' as \dfn{WLOG},
``assumption'' as \dfn{ASM},
``theorem'' as \dfn{Th},
and
``definition'' as \dfn{Def}.

Unless otherwise specified, the indexes
of a sequence are consecutive integers.
Where \var{seq} is a sequence,
we write \Vfirstix{seq}
for its first index,
and \Vlastix{seq}
for its last index.

\section{Grammars}

Where \Vsymset{syms} is non-empty set of symbols,
let $\var{syms}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Let $\var{syms}^+$ be
\begin{equation*}
\bigl\{ \Vstr{x}
\bigm| \Vstr{x} \in \var{syms}* \;\; \land \;\; \Vstr{x} \neq \epsilon
\bigr\}.
\end{equation*}

In this document we use,
without loss of generality,
the grammar \Cg{},
where \Cg{} is the 4-tuple
\begin{equation*}
    (\Vsymset{nt}, \Vsymset{term}, \var{rules}, \Vsym{accept}).
\end{equation*}
\Vsymset{nt} is a set of symbols called non-terminals,
and \Vsymset{term} is a set of symbols called terminals.
Here $\Vsym{accept} \in \var{nt}$.
The vocabulary of the grammar is the union of
the sets of terminals and non-terminals:
\[ \Vsymset{vocab} = \Vsymset{nt} \cup \Vsymset{term}. \]
If a string of symbols contains only terminal symbols,
that string is called a \dfn{sentence}.
When a string contains only terminals, we also write its length
in symbols as
\Vsize{input}.%
\index{recce-notation}{\Pipe{}str\Pipe{}@\Vsize{str} (size of a string of terminals)}
The length of a string which contains non-terminals will be defined later,
when we discuss inputs.

\Vruleset{rules} is a set of rules (type \type{RULE}),
where a rule is a duple
of the form $[\Vsym{lhs} \de \Vstr{rhs}]$,
such that
\begin{equation*}
\Vsym{lhs} \in \var{nt} \quad \text{and}
\quad \Vstr{rhs} \in \var{vocab}^\ast
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\Vrule{r}}$%
\index{recce-notation}{LHS(r)@\Vop{LHS}{rule}}
and
$\RHS{\Vrule{r}}$,%
\index{recce-notation}{RHS(r)@\Vop{RHS}{rule}}
and
respectively.

Where \Vrule{r} is a rule,
\Vsize{\Vrule{r}}%
\index{recce-notation}{\Pipe{}rule\Pipe{}@\Vsize{rule} (size of a rule)}
is the length of its RHS, in symbols.
Occasionally, we will need to look at the RHS of a rule
symbol by symbol.
\op{RHS}{\Vrule{r}, \var{i}}%
\index{recce-notation}{RHS(r, ix)@\op{RHS}{\Vrule{r}, \var{ix}}}
is the \var{i}'th symbol on the RHS
of \Vrule{r}.
The numbering is zero-based,
so that
\op{RHS}{\Vrule{r}, 0}
is the first symbol on the RHS of \Vrule{r},
and
\op{RHS}{\Vrule{r}, \Vdecr{\Vsize{\Vrule{r}}}}
is the last symbol on the RHS of \Vrule{r}.

The rules imply the traditional rewriting system.
We write
$\Vstr{x} \derives \Vstr{y}$
to say that
\Vstr{x} derives \Vstr{y} in exactly one step.
If a superscript is placed over the arrow,
it indicates
the number of derivation steps.
So we can also write
$\Vstr{x} \xderives{1} \Vstr{y}$
to say that
\Vstr{x} derives \Vstr{y} in one step.
We write
$\Vstr{x} \xderives{\var{n}} \Vstr{y}$
to say that
\Vstr{x} derives \Vstr{y} in \var{n} steps.
$\Vstr{x} \xderives{0} \Vstr{y}$
is a derivation in zero steps,
and is called a
\xdfn{trivial derivation}{trivial (derivation)}.

We write $\Vstr{x} \destar \Vstr{y}$
to say that
\Vstr{x} derives \Vstr{y} in zero or more steps,
and
$\Vstr{x} \deplus \Vstr{y}$
to say that
\Vstr{x} derives \Vstr{y} in one or more steps.
Note that
\begin{gather*}
\Vstr{x} \destar \Vstr{y} \implies
\exists \; \var{n},
\Vstr{x} \xderives{\var{n}} \Vstr{y}
\\
\text{and} \quad
\Vstr{x} \deplus \Vstr{y} \implies
\exists \; \var{n},
\big(\var{n} \ge 1 \; \land \;
\Vstr{x} \xderives{\var{n}} \Vstr{y}
\big),
\end{gather*}
where \var{n} is a non-negative integer~
\cite[Vol. 1, p. 86]{AU1972}.

The literature does not always distinguish between two
meanings of the term ``derivation step''.
It can sometimes mean a single string in a derivation,
and at other times means the action of one string
deriving another.
In this paper, we will say ``step'' to mean a single
string in a derivation,
and we will call the transition from one string to another,
a \dfn{derivation move} or, when it is clear in context,
a \dfn{move}.
For example,
\begin{equation}
\label{eq:def-move-1}
   \Vstr{x} \derives \Vstr{y},
\end{equation}
By ``step'' we mean
\Vstr{x} and \Vstr{y} considered separately,
so that there are two steps in
\eqref{eq:def-move-1}.
By ``move''
we mean \eqref{eq:def-move-1} considered as a whole,
so that
\eqref{eq:def-move-1} is a single ``move'',
with \Vstr{x} as its left side,
and
\Vstr{y} as its right side.

We say that \Vstr{desc} is a direct descendant of \Vsym{A} if
it is \Vstr{A-rhs} where $\Vsym{A} \de \Vstr{A-rhs}$ is a rule,
or if it is the empty string where \Vsym{A} is a nulling terminal.
We say that a derivation is \dfn{leftmost} if at each of its steps,
its leftmost nonterminal is replaced by one of its direct descendants.
We say that a derivation is \dfn{rightmost} if at each of its steps,
its rightmost nonterminal is replaced by one of its direct descendants.

When we want to make clear which step a symbol instance is from,
we will write $\var{si}@\var{x}$ to indicate
the symbol instance \Vinst{si} at Step \var{x}.
We will sometimes write the symbol instance using only
the symbol, so that
$\var{A}@\var{x}$ will indicate
an instance of symbol \Vsym{A} at Step \var{x}.
For example, if
\begin{align}
    & \Vsym{A} && \text{Step 1} \\
    \derives & \Vsym{A} && \text{Step 2}
\end{align}
is a derivation,
$\var{A}@1$ indicates the instance of \Vsym{A} in Step 1,
while
$\var{A}@2$ indicates the instance of \Vsym{A} in Step 2.

Consider the derivation
\begin{equation}
\Vsym{A} \derives \Vstr{rhs} \destar \Vstr{left} \cat \Vsym{A} \cat \Vstr{right}
\end{equation}
We say that the rule $\Vsym{A} \de \Vstr{rhs}$
and the symbol \Vsym{A} are
\begin{center}
\begin{tabular}{ll}
\dfn{middle-recursive} & if
  $\Vstr{left} \neq \epsilon \land \Vstr{right} \neq \epsilon$ \\
\dfn{left-recursive} &  if $\Vstr{left} = \epsilon$ \\
\dfn{right-recursive} &  if $\Vstr{right} = \epsilon$ \\
\dfn{cyclic} & if
  $\Vstr{left} = \epsilon \land \Vstr{right} = \epsilon$.
\end{tabular}
\end{center}

The language of \var{g} is $\myL{\Cg}$, where
\begin{equation}
\label{eq:def-L-g}
\myL{\Cg} \defined \left\lbrace
\Vstr{z} \mid \Vstr{z} \in \var{term}^\ast \land \Vsym{accept} \destar \Vstr{z}
\right\rbrace
\end{equation}
In this document,
\Earley{} will refer to the Earley's original
recognizer~\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the parser described in
this document.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Cg}$ will be the language accepted by $\alg{Recce}$
when parsing \Cg{}.

\section{Marpa internal grammars}
In traditional parsing theory,
a symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} = \epsilon$.
We say that symbol \Vsym{x} is \dfn{nulling} if and only if
\[
  \forall \Vstr{y} \mid \Vsym{x} \destar \Vstr{y}
  \implies \Vstr{y} = \epsilon.
\]
A symbol is a
\xdfn{proper nullable}{proper nullable (symbol)}
if it is nullable, but not nulling.

Following Aycock and Horspool~\cite{AH2002},
Marpa grammars use a rewrite to eliminate proper nullables,
and nulling rules.
The pre-rewrite grammar is called the
\dfn{external grammar},
and the post-rewrite grammar is Marpa's
\dfn{internal grammar}.
The external, pre-rewrite grammars
are the only ones visible to
the users of Marpa::R2.
The internal, post-rewrite grammars
are the ones actually used by the parse engine,
and therefore are
the ones described in this paper.
Details of this rewrite are given
in Chapter \ref{ch:rewrite}.

In discussing Marpa's internal grammars,
we say that a symbol is
\xdfn{nulling}{nulling (Marpa internal symbol)}
if and only if
\[
  \Vsym{x} = \epsilon.
\]
We say that \Vstr{y} is nulling,
if and only
if every symbol in \Vstr{y} is nulling.
We also write ``\Vstr{y} is nulling'' as
$\Vstr{y} = \epsilon$.
This can be the case vacuously ---
if \Vstr{y} is the empty string of symbols,
then
$\Vstr{y} = \epsilon$.

If a symbol is not nullable, we may say that it is
\dfn{non-nullable},
but more often we say it is
\xdfn{telluric}{telluric (symbol)}.
We will find telluric less confusing in the context of
Marpa's internal grammars,
because
it can be used as an antonym of ``nulling''.
For Marpa's external grammars,
and in traditional parsing theory,
``non-nullable'' is not, strictly speaking,
an antonym of nulling.

Since there are no nulling rules in Marpa's internal grammars,
a symbol is not nulled as the result of a derivation step.
Therefore, where \Vsym{nulling} is a nulling symbol,
\begin{align*}
    \Vsym{nulling} & \nxderives{1} \epsilon \quad \text{and} \\
    \Vsym{nulling} & \ndeplus \epsilon.
\end{align*}
Strictly speaking, it is not wrong to say that
\begin{align*}
    \Vsym{nulling} & \xderives{0} \epsilon \quad \text{or} \\
    \Vsym{nulling} & \destar \epsilon
\end{align*}
although it is misleading,
and we avoid it in favor of
\[
    \Vsym{nulling} = \epsilon,
\]
which more clearly conveys what it means for a
Marpa internal symbol to be ``nulling''.

A Marpa internal grammar always has
a dedicated acceptance rule, \Vrule{accept}
and a dedicated acceptance symbol,
\[
  \Vsym{accept} = \LHS{\Vrule{accept}},
\]
such that
for all \Vrule{x},
\[
\Vsym{accept} \notin \RHS{\Vrule{x}}
\]
and
\[
\Vsym{accept} = \LHS{\Vrule{x}} \implies \Vrule{accept} = \Vrule{x}.
\]

We assume that a Marpa grammar is cycle-free ---
that none of its rules are cyclic.
We assume that every symbol is productive ---
that is, that it derives a sentence.
We assume that every symbol is accessible ---
that is, that it is derivable from the start symbol.

\section{Input}

Let the actual input to
the parse be \Cw{} such that $\Cw \in \myL{\Cg}$.
Locations in the input will be of type \type{LOC}.
Let \Vsize{w}%
\index{recce-notation}{\Pipe{}input\Pipe{}@\Vsize{input} (size of an input)}
be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.
Let \CVw{i} be character
at position \var{i}
of the input.
String position is zero-based,
so that
$0 \le \Vloc{i} < \Vsize{w}$.
Let $\var{w}[\var{a}, \var{b}]$
be the contiguous substring
from position \var{a} to
position \var{b}, inclusive,
so that
\[ \bigsize{\var{w}[\var{a}, \var{b}]} = (\var{b} \subtract \var{a}) + 1. \]

Our definition
of \Cw{} does not allow zero-length inputs.
The Marpa parser
deals with null parses
and nulling grammars as special cases,
and this document will not consider them.
(Nulling grammars are those that recognize only the null string.)

Parsers typically do work while examining their input,
so that they are, in effect, working with a set of possible inputs,
of which the actual input is just one element.
Reasoning about the set of inputs possible based on what has been
seen so far plays little role in
traditional deterministic parsers,
which do limited tracking
of input already seen,
and have even less of an idea of the inputs not yet seen.
But Marpa is left-eidetic ---
it has a full, exact idea of the input already seen ---
and Earley parsers
also have a very exact idea of what the unseen portion of the input
could be.

We will call the current set of inputs, \CW{}.
\CW{} will always be such that
\begin{equation*}
\Cw \in \CW \quad \text{and} \quad \CW{} \subseteq \myL{\Cg}.
\end{equation*}
We say that \CW{} is
\xdfn{seen between}{seen between \var{i} to \var{j} (re a set of inputs)}
from \Vloc{i} to \Vloc{j} if and only if,
for all \Vstr{w1}, \Vstr{w2}
\begin{equation*}
\Vstr{w1} \in \CW \land \Vstr{w2} \in \CW
\implies
\el{w1}{\var{i}, \, (\Vdecr{j}) \, } =
\el{w2}{\var{i}, \, (\Vdecr{j}) \, }.
\end{equation*}

Most parsing, including Earley parsing, takes place from left-to-right,
and Marpa examines its input from left to right.
We say that
\CW{} is
\xdfn{seen to}{seen to \var{j} (re a set of inputs)}
\var{j},
or that
\CW{} is
\xdfn{seen as far as}{seen as far as \var{j} (re a set of inputs)}
\var{j},
if \CW{} is seen between locations 0 and \Vloc{j}.
In this document we will usually speak of input sets that are seen
as far as some \Vloc{j}.
If \CW{} is seen to location 0, none of its input symbols have been
seen.
If \CW{} is seen to location \Vsize{\Cw},
where \Cw{} is the actual input,
then
$\CW = \lbrace \Cw \rbrace$,
and all of its input symbols
have been seen.
In most contexts, the current set of inputs will be assumed to be \CW{}.
For example,
instead of saying that \CW{} is seen as far as \Vloc{k},
we may say the ``the input is seen as far as \Vloc{k}''.

We will say that a derivation is
\xdfn{fully seen}{fully seen (of a derivation)},
or more simply
\xdfn{full}{full (of a derivation)},
if its last step is \Cw{}.
Intuitively, a \dfn{symbol instance} is
a symbol in the context of a parse.
In a fully seen derivation,
the right and left locations are well-defined
for every symbol instance of every step.

More formally,
a symbol instance is a triple whose elements
are a left location, a symbol name and a right location.
We often represent symbol instances in the form
\begin{equation*}
\Vinst{inst} = \Vmkl{j} \Vsym{up} \Vmkr{k}.
\end{equation*}%
\index{recce-notation}{[]inst[]@\Vmkl{j} \Vsym{up} \Vmkr{k} (symbol instance)}
\Vmk{j} and \Vmk{k} are always optional.
We also write
\begin{itemize}
\item
\Left{\Vinst{inst}}%
\index{recce-notation}{Left(x)@\Left{\Vinst{inst}}}
for \var{j},
the left location of \Vinst{inst};
\item
\Right{\Vinst{inst}}%
\index{recce-notation}{Right(x)@\Right{\Vinst{inst}}}
for \var{k},
the right location of \Vinst{inst};
\item
\Symbol{\Vinst{inst}}%
\index{recce-notation}{Symbol(x)@\Symbol{\Vinst{inst}}}
for \Vsym{up},
the symbol name of \Vinst{inst};
\item
\Vsize{\Vsym{inst}}%
\index{recce-notation}{\Pipe{}inst\Pipe{})@\Vsize{\Vinst{inst}}}
for \xxsubtract{\var{k}}{\var{j}},
the length of \Vinst{inst} in terms of input symbols; and
\item
\Vsize{\Vsym{A}}%
\index{recce-notation}{\Pipe{}sym\Pipe{})@\Vsize{\Vsym{A}}}
for \Vsize{\Vinst{inst}},
where $\Symbol{\Vinst{inst}} = \Vsym{A}$,
and the left and right locations of \Vinst{inst}
are understood from the context.
\end{itemize}

When \Vstr{si} is a sequence of symbol instances of length \Vsize{si}
whose indexes are 0 \ldots{} \var{last},
we will write
\Symbol{\Vstr{si}}%
\index{recce-notation}{Symbol(str)@\Symbol{\Vstr{str}}}
for
\begin{equation*}
\Symbol{\el{si}{0}}, \;\;
\Symbol{\el{si}{1}}, \;\;
\ldots \;\;
\Symbol{\Vel{si}{last}}.
\end{equation*}
We will write \Vsize{\Vstr{si}}%
\index{recce-notation}{\Pipe{}str\Pipe{})@
	\Vsize{\Vstr{str}} (length of a sentential form in terms of the input)}
for
\begin{equation*}
\sum_{\var{i}=0}^\var{last} \Symbol{\Vel{si}{i}}.
\end{equation*}
Note that \Vsize{\Vstr{si}} is the length of the string
in terms of input symbols, and is, in general,
not a count of the
symbols in \Vstr{si}.

At some points, such as when we translate a derivation
to other notation,
we will want to justify the conversion to and from
a derivation carefully.
To do this, we will treat a derivation as a two-dimensional
ragged array.
The rows will be derivation steps,
and the columns of these variable-length rows will be symbol
instances.
We will write \drvVV{d}{s}{i} for
the \var{i}'th symbol instance of the \var{s}'th step
of the derivation \var{d}.
Steps will be numbered from 0, starting at the root.
Symbol instances will be numbered from 0, starting at the left.

Let \var{d} be a fully seen derivation,
and let $\var{wlen} = \Vsize{\Cw}$.
For every \var{d}, we will have
\begin{equation}
\drv{d}{0}{0} = \mk{0} \Vsym{accept} \Vmk{wlen}
\end{equation}
and where the length of the derivation is \var{dlen},
for all \var{a} such that $0 \le \var{a} < \var{wlen}$,
\[
  \drv{d}{\Vdecr{dlen}}{\var{a}} = \Vinst{a} = \Vmk{a} \Vsym{a} \mk{\Vincr{a}}.
\]
where \Vsym{a} is $\Cw[\var{a}]$,
the symbol at location \Vloc{a} of the input.

We will use type \type{STR} for sequences of symbol instances
as well as symbols.
We write \Vel{s1}{i} to refer
to refer to the \var{i}'th symbol or symbol instance
in a string,
where the first symbol or symbol instance is at
\el{s1}{0}.
We write \el{s1}{\var{i} \ldots \var{j}} to refer
to the contiguous substring of \Vstr{s1} which starts with
\Vel{s1}{i} and ends with \Vel{s1}{j}.
The range is inclusive, so that
the length of \el{s1}{\var{i} \ldots \var{j}}
is $(\xxsubtract{\var{j}}{\var{i}})+1$.
We will find it convenient to write
\drv{d}{\var{s}}{\var{a} \ldots \var{z}}
for the sequence of symbol instances
\[
    \drvVV{d}{s}{a}, \;\;
    \drv{d}{\var{s}}{\Vincr{a}}, \;\;
    \ldots \;\;
    \drvVV{d}{s}{z}
\]


\section{Focused derivations}

We now
recall our previous definition of a rightmost
derivation.
Following ~\cite[Vol. 1, page 141, Lemma 2.12]{AU1972},
a rightmost derivation is defined in terms of a series of
expansions of a derivation tree, beginning at the root.
A rightmost derivation is one that always expands the rightmost
non-terminal into its direct descendants.

\begin{definition}
\dtitle{Focused derivation}
We say that the derivation \var{d}
is
\xdfn{focused}{focused (derivation)}
at \Vloc{k},
if \CW{} is seen to \Vloc{k},
and
if in every step \var{s},
\begin{itemize}
\item
if there is a non-terminal symbol instance
$\Vinst{ki} = \drvVV{d}{s}{x}$
such that $\Left{\Vinst{ki}} \le \Vloc{k} < \Right{\Vinst{ki}}$,
\var{d} expands
\drvVV{d}{s}{x}; and
\item
otherwise, \var{d} expands the rightmost non-terminal symbol
instance.
\end{itemize}
We say that \var{d} is
\xdfn{focused within}{focused within (a symbol instance, said of a derivation}
a symbol instance \Vinst{si}
if and only if it is
focused at some \Vloc{k} such
that
\[
\Left{\var{si}} \le \Vloc{k} < \Right{\var{si}}.
\]
We say that \var{d} is
\xdfn{focused within}{focused within (an EIM, said of a derivation)}
\Veim{eim}
if and only if it is
focused at some \Vloc{k} such that
\[
\Left{\LHS{\Veim{eim}}} \le \Vloc{k} < \Right{\LHS{\Veim{eim}}}.
\]
\end{definition}

Recall from parsing theory that every derivation has an equivalent rightmost
derivation, and that the rightmost derivation is unique in its derivation
tree.
Similarly, for every \Vloc{k},
every derivation which is seen to \Vincr{k}
has an equivalent derivation that is focused at \Vloc{k},
and that \Vloc{k}-focused derivation is unique in its
derivation tree.

\begin{theorem}
\ttitle{Properties of focused derivations}
\label{t:focusing-props}
Let \var{d} be a derivation focused within
the symbol instance
\Vinst{i1}, where
\Vinst{i1} is at derivation step \var{s},
and \Symbol{\var{i1}} is
a non-terminal other than \Vsym{accept}.
Then \Vinst{i1} is a direct descendant
of a symbol instance \Vinst{i2},
such that
\begin{gather}
\label{eq:focusing-props-10}
\text{\var{i2} is at derivation step \Vdecr{s},} \\
\label{eq:focusing-props-13}
\text{\Symbol{\var{i2}} is a non-terminal,} \\
\label{eq:focusing-props-16}
\Right{\var{i2}} \ge \Right{\var{i1}}, \\
\label{eq:focusing-props-19}
\Left{\var{i2}} \le \Left{\var{i1}}, \\
\label{eq:focusing-props-22}
\text{and \var{d} is focused within \var{i2}.}
\end{gather}
In addition,
for some \Vstr{pre}, \Vstr{post},
where $\Symbol{\var{i1}} = \Vsym{i1}$ and
$\Symbol{\var{i2}} = \Vsym{i2}$,
\begin{equation}
\label{eq:focusing-props-25}
[\Vsym{i2} \de \Vstr{pre} \Vsym{i1} \Vstr{post} ] \in \Crules,
\end{equation}
and
\begin{equation}
\label{eq:focusing-props-27}
\begin{aligned}
& \Vsym{i2} && \text{Step \Vdecr{s}} \\
\derives \quad & \Vstr{pre} \Vsym{i1} \Vstr{post} \qquad && \text{Step \var{s}}
\end{aligned}
\end{equation}
\end{theorem}

\begin{proof}
Since \Vinst{i1} is not the accept symbol,
there is a derivation step \Vdecr{s}.
In each derivation step, a symbol instance is either
copied over into the next step or expanded into
its direct descendants.
Therefore,
in step \Vdecr{s}, there is either another copy of symbol
instance \Vinst{i1}, or \Vinst{i1} is the direct descendent
of another symbol instance.
Either way, call that other symbol instance, \Vinst{i2}.
\Vinst{i2} is in derivation step
\Vdecr{s} by definition, which gives us
\eqref{eq:focusing-props-10}.

We show
\eqref{eq:focusing-props-16}
and
\eqref{eq:focusing-props-19}
by cases.
In the first case, where \Vinst{i1} is a direct descendant
of \Vinst{i2}, we recall that
\begin{equation}
\label{eq:focusing-props-29}
\myparbox{
In every parse using a context-free grammar,
the right and left locations of
a direct descendant are always inside
of the single symbol from which the
direct descendant was expanded.
}
\end{equation}
The input may not have been seen to \Right{\var{i2}},
but
\eqref{eq:focusing-props-30}
is true of all factorings of all inputs,
and we may conclude that
\eqref{eq:focusing-props-16}
and
\eqref{eq:focusing-props-19}
hold for every input in \CW{}.
If \Vinst{i1} is a copy of \Vinst{i2},
\eqref{eq:focusing-props-16}
and
\eqref{eq:focusing-props-19}
follow trivially.

By assumption, \var{d} is focused within \Vinst{i1},
and therefore at some \Vloc{k} such that
\begin{equation}
\label{eq:focusing-props-30}
\Left{\var{i1}} \le \Vloc{k} < \Right{\var{i1}}.
\end{equation}
\eqref{eq:focusing-props-22}
follows from
\eqref{eq:focusing-props-16},
\eqref{eq:focusing-props-19}
and
\eqref{eq:focusing-props-30}.

We show \eqref{eq:focusing-props-13}
by cases.
In the first case,
\Vinst{i2} is a copy of \Vinst{i1}
so that
$\Symbol{\var{i2}} = \Symbol{\var{i1}}$,
and since
\Symbol{\var{i1}} is a non-terminal,
\Symbol{\var{i2}} is a non-terminal.
In the second case,
\Vinst{i2} is not a copy of \Vinst{i1},
so that \Vinst{i1} is a direct descendant of
\Vinst{i2}.
In this second case,
\Vinst{i2} is a non-terminal
by definition.
This gives us both cases,
and
\eqref{eq:focusing-props-13}.

We have already shown
\eqref{eq:focusing-props-13}
and
\eqref{eq:focusing-props-22},
so we know that,
by the definition of a focused derivation,
\Vinst{i2} is expanded into
its direct descendants in step \var{s}.
\eqref{eq:focusing-props-25}
and \eqref{eq:focusing-props-27}
follow from this observation and the definition
of a derivation.

For convenience the step numbers are shown in
\eqref{eq:focusing-props-27}.
The labeling of Step \var{s} follows from assumption
for the theorem.
The labeling of Step \Vdecr{s} follows from
\eqref{eq:focusing-props-10}.
\end{proof}

\section{Location markers}

\subsection{Definition}

In the context of a grammar \Cg{} and an input \Cw{},
we will often use location-marked derivations.
Location-marked derivation steps are like the derivation steps
of the traditional rewriting system except that they also contain
location markers of the form \Vmk{x}, where \var{x} is a
location in \Cw{}.\footnote{
Our use of the location marker notation
was inspired by~\cite{Wich2005}.}
When not otherwise stated,
use of the location marker \Vmk{x}
implies that \CW{} has been seen to \Vmk{x}.
In its most general form,
a derivation step with a single location marker is
\begin{equation}
\label{eq:location-marker-def-2}
\Vstr{pre} \Vmkm{x} \Vstr{post}.
\end{equation}
\eqref{eq:location-marker-def-2}
means
\begin{equation*}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vstr{pre} \cat \Vstr{post} \cat \Vstr{after} \\
&  \land \quad \Vstr{before} \cat \Vstr{pre} \destar \var{w}[0, (\Vdecr{x})] \\
&  \land \quad \Vstr{post} \cat \Vstr{after} \destar \var{w}[\var{x}, (\Vsize{\Cw} \subtract 1)]
\end{split}
\end{equation*}

Derivations may have many location markers.
The meaning of a derivation with \var{j} different location markers,
\[ \var{m}[1], \var{m}[2] \ldots \var{m}[\var{j}], \]
is the same as the meaning of the conjunction of an ordered set of \var{j} derivations,
where the \var{i}'th member has all the markers removed except for $\var{m}[\var{i}]$.
For example,
\begin{equation}\label{eq:location-marker-def-11}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \Vmkm{i} \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \Vmkm{i} \Vstr{predot} \Vmkm{j} \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{equation}
is the equivalent of the logical conjunction of two derivations:
\begin{gather}
\label{eq:location-marker-def-12}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \,[\var{i}]\, \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \,[\var{i}]\, \Vstr{predot} \Vstr{postdot} \cat \Vstr{after}
\end{split} \\
\intertext{and}
\label{eq:location-marker-def-13}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \cat \Vstr{predot} \,[\var{j}]\, \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{gather}
In this example,
\eqref{eq:location-marker-def-12} and
\eqref{eq:location-marker-def-13}
imply that
\begin{equation}\label{eq:location-marker-def-14}
\Vstr{predot} \destar \var{w}[\var{i}, (\var{j} \subtract 1)]
\end{equation}
and therefore
\eqref{eq:location-marker-def-11}
also implies
\eqref{eq:location-marker-def-14}.
Derivations with location markers may be
composed in the same way as derivations without them,
as long as the location markers in the combined
derivation are consistent.

\subsection{Transformations}

The location marker notation is intuitive and based on the
traditional notation for derivations,
but, since it is new, we will present examples.
For the examples of this section,
we
start with the traditional derivation
\begin{gather*}
\Vstr{A} \\
\derives \Vstr{Aa} \Vsym{B} \Vsym{D} \Vstr{Aa} \\
\derives \Vstr{Aa} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vsym{D} \Vstr{Az}
\end{gather*}

\textbf{Insert new location markers}:
If a location marker is not currently used, we can insert it
to mark any location that has been seen.
In this example,
we insert
new markers
\Vmk{i}, \Vmk{j}, \Vmk{k}, \mk{\ell} and \Vmk{m}:
\begin{gather*}
\Vstr{A} \\
\derives \Vmk{i} \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Aa} \Vmk{m} \\
\derives \Vstr{Aa} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vsym{D} \Vstr{Az}
\end{gather*}

Introduction of a location marker \Vmk{x}, unless otherwise stated,
assumes that the \CW{} has been seen to \Vmk{x},
so in this example, we assumed that input has been seen to
the rightmost marker, \Vmk{m}.
The examples to follow deal with movement of existing
location markers, so that it will have already been assumed
that \CW{} has been seen as far as those location markers.

\textbf{Move location markers from direct descendants to their parents}:
If a location marker is before the first of its direct descendants, we can move
it to before its parent in the previous step.
In this example, \Vmk{i} is moved:
\begin{gather*}
\Vmk{i} \Vstr{A} \\
\derives \Vmk{i} \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Aa} \Vmk{m} \\
\derives \Vstr{Aa} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vsym{D} \Vstr{Az}
\end{gather*}
Also, if a location marker is after the last of its direct desendants,
we can move it to after its parent in the previous step.
In this example, \Vmk{m} is moved:
\begin{gather*}
\Vmk{i} \Vstr{A} \Vmk{m} \\
\derives \Vmk{i} \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Aa} \Vmk{m} \\
\derives \Vstr{Aa} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vsym{D} \Vstr{Az}
\end{gather*}

\textbf{Move location markers from parents to direct descendants}:
Similarly if a location marker is before a parent, we can
move it to before the first of its direct descendants in the next step;
and,
if a location marker is after a parent, we can
move it to after the last of its direct descendants in the next step.
In this example, \Vmk{j} and \Vmk{k} are moved:
\begin{gather*}
\Vmk{i} \Vstr{A} \Vmk{m} \\
\derives \Vmk{i} \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az} \Vmk{m} \\
\derives \Vstr{Aa} \Vmk{j} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vmk{k} \Vsym{D} \Vstr{Az}
\end{gather*}

\textbf{Delete location markers}:
Location markers which are no longer of interest may be removed.
In this example, \Vmk{i} and \Vmk{m} are deleted:
\begin{gather*}
\Vstr{A} \\
\derives \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az} \\
\derives \Vstr{Aa} \Vmk{j} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vmk{k} \Vsym{D} \Vstr{Az}
\end{gather*}

\textbf{Move location marker to before or after same symbol instance}:
A location marker which is before (after) a symbol instance in one derivation
step may be moved to before (after) the same symbol instance in another derivation step.
In this example, \mk{\ell} is moved:
\begin{gather*}
\Vstr{A} \\
\derives \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az} \\
\derives \Vstr{Aa} \Vmk{j} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az}
\end{gather*}

\textbf{Simplification}:
A derivation may be simplified from the bottom up,
by removing the symbol instances outside of two markers.
In this example, the last step is simplified outside of \Vloc{j} and \Vloc{k}:
\begin{gather*}
\Vstr{A} \\
\derives \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az} \\
\derives \ldots \Vmk{j} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vmk{k} \ldots
\end{gather*}
When it is clear what is happening, the dots may be omitted:
\begin{gather*}
\Vstr{A} \\
\derives \Vstr{Aa} \Vmk{j} \Vsym{B} \Vmk{k} \Vsym{D} \mk{\ell} \Vstr{Az} \\
\derives \Vmk{j} \Vstr{Ba} \Vsym{C} \Vstr{Bz} \Vmk{k}
\end{gather*}

\textbf{Null passing}:
A location marker may be duplicated from one side of a nulling symbol
to the other.
For this example,
assume that
$\Vstr{nul1} = \epsilon$,
$\Vstr{tel1} \neq \epsilon$,
and
$\Vstr{tel2} \neq \epsilon$.
In
\begin{equation}
\Vstr{tel1} \Vmk{x} \Vstr{nul1}  \Vstr{tel2},
\end{equation}
we can duplicate the \Vmk{x}
\begin{equation}
\Vstr{tel1} \Vmk{x} \Vstr{nul1} \Vmk{x} \Vstr{tel2}.
\end{equation}
If one of the two \Vmk{x} markers is now clutter,
we may also delete it:
\begin{equation}
\Vstr{tel1} \Vstr{nul1} \Vmk{x} \Vstr{tel2}.
\end{equation}

\subsection{Simplification and nulling symbols}

Caution must be exercised because nulling symbols
can fall on either side of a location-marker.
The following theorem shows that what looks like a cycle
in location-marked notation is,
in fact, a cycle,
and therefore cannot occur in Marpa grammar.

\begin{theorem}
\ttitle{Location marked cycles}
\label{t:location-marker-cycle}
Let \Vstr{sf} be a sentential form containing at
least one telluric symbol.
Then
\begin{equation}
\label{eq:location-marker-cycle-10}
  \Vmk{i} \Vstr{sf} \Vmk{j} \ndeplus \Vmk{i} \Vstr{sf} \Vmk{j}
\end{equation}
\end{theorem}

\begin{proof}
We represent
the possible nulls
outside our location markers in
\eqref{eq:location-marker-cycle-10}
in full generality as
\begin{align}
\label{eq:location-marker-cycle-12}
& \Vstr{nul1L} \Vmk{i} \Vstr{sf} \Vmk{j} \Vstr{nul1R} && \text{Step \var{x}} \\
\label{eq:location-marker-cycle-14}
  \deplus & \Vstr{nul2L} \Vmk{i} \Vstr{sf} \Vmk{j} \Vstr{nul2R}. && \text{Step \var{y}}
\end{align}
where
$\Vstr{nul1L} = \Vstr{nul1R} = \Vstr{nul2L} = \Vstr{nul2R} = \epsilon$.
We assume
\eqref{eq:location-marker-cycle-12}--\eqref{eq:location-marker-cycle-14}
for an outer reductio.

By assumption for the theorem there is at least
one telluric symbol in \Vstr{sf}.
Let \Vsym{tell} be one of those telluric symbols in Step \var{x},
and let \Vsym{descs} be its descendants in Step \var{y}.

We will write \Vop{T}{x} for the number of telluric symbols in \Vstr{x}
or \Vsym{x}.
No telluric symbol is nulling, so that
$\bigop{T}{\Vsym{tell}} \le \bigop{T}{\Vstr{dd}}$,
and therefore
$\bigop{T}{\Vstr{dd}} \ge 1$.

Assume for an inner reductio, that
there is some \Vsym{tell2} in
Step \var{x} of
\Vstr{sf},
such that $\bigop{T}{\Vstr{descs2}} > 1$,
where \Vstr{descs2} are the descendants of \Vsym{tell2}
in Step \var{y}.

Let \var{tell-cnt} be the number of telluric symbols in
\eqref{eq:location-marker-cycle-12}:
\[
\var{tell-cnt} =
\bigop{T}{ \Vstr{nul1L} } +
\bigop{T}{ \Vstr{sf}@\var{x}} +
\bigop{T}{ \Vstr{nul1R} }.
\]
Since $\Vstr{nul1L} = \Vstr{nul1R} = \epsilon$,
we have
$\var{tell-cnt} = \bigop{T}{\Vstr{sf}@\var{x}}$.
We have
\begin{equation}
\label{eq:location-marker-cycle-17}
\var{tell-cnt} = \bigop{T}{\Vstr{sf}@\var{x}}
= \bigop{T}{\Vstr{sf}@\var{y}}
\end{equation}
by the self-identity of \Vstr{sf}.

$\Vstr{nul2L} = \Vstr{nul2R} = \epsilon$,
so that if there is any
telluric symbol
in Step \var{y},
it will be in
$\var{sf}@\var{y}$.
Using
\eqref{eq:location-marker-cycle-17}
we see that if one telluric symbol in $\var{sf}@\var{x}$
derives
\var{n} telluric symbols in $\var{sf}@\var{y}$,
then the other $\Vdecr{tell-cnt}$ telluric symbols
in
$\var{sf}@\var{x}$ must derive
\xxsubtract{\var{tell-cnt}}{\var{n}}
telluric symbols.
This means that, if $\var{n} > 1$,
at least one telluric symbol in
$\var{sf}@\var{x}$
derives
zero telluric symbols in $\var{sf}@\var{y}$.
In other words,
if $\var{n} > 1$,
at least one telluric symbol in
$\var{sf}@\var{x}$ must be nulling.

But telluric symbols are never nulling,
which shows the inner reductio.
We conclude from the inner reductio that
no telluric symbol in
$\bigop{T}{\Vstr{sf}@\var{x}}$
derives more than one telluric symbol in
$\bigop{T}{\Vstr{sf}@\var{y}}$.

We have already shown that every telluric symbol in
Step \var{x}
derives at least one telluric symbol in Step \var{y}.
So we have that,
\begin{equation}
\label{eq:location-marker-cycle-20}
\myparbox{
for any telluric symbol
$\Vsym{tell2}@\var{x}$,
if \Vstr{descs2}@\var{y} are its direct descendants in
Step \var{y},
then $\bigop{T}{\Vstr{descs2}@\var{y}} = 1$.
}
\end{equation}

Recall our earlier assumption
in the outer reductio
that \Vsym{tell} is an arbitrary
telluric symbol in \var{sf}@\var{x}
and
\var{descs}@\var{y}
are its descendants.
We know from
\eqref{eq:location-marker-cycle-20}
that
$\bigop{T}{\Vstr{descs}@\var{y}} = 1$.
That is, each telluric symbol in Step \var{x}
maps one-to-one to a telluric descendant in
Step \var{y}.
What is the telluric symbol that \Vsym{tell}
maps to in \Vstr{descs}?

All the telluric symbols in
\eqref{eq:location-marker-cycle-12}
and
\eqref{eq:location-marker-cycle-14}
must be in
$\var{sf}@\var{x}$ and $\var{sf}@\var{y}$,
and
$\var{sf}@\var{x} = \var{sf}@\var{y}$,
so that the telluric symbols in
$\var{sf}@\var{x}$
must be the same, and in
the same order,
as those in
$\var{sf}@\var{y}$.
So a one-to-one mapping of telluric symbols from
$\var{sf}@\var{x}$
to telluric symbols in $\var{sf}@\var{y}$
must map each telluric symbol to itself.
Therefore, the telluric symbol in
$\Vstr{descs}@\var{y}$
is $\Vsym{tell}@\var{x}$.

We can therefore write the derivation of
\Vstr{descs} from
$\Vsym{tell}@\var{x}$,
without loss of generality,
as
\begin{equation}
\label{eq:location-marker-cycle-24}
\Vsym{tell} \deplus \Vstr{nulL} \Vsym{tell} \Vstr{nulR} = \Vstr{descs}
\end{equation}
where
$\Vstr{nulL} = \Vstr{nulR} = \epsilon$.
But, by the definition of cyclic,
\eqref{eq:location-marker-cycle-24}
is a cycle.
Cycles are not allowed in Marpa grammars,
which shows the outer reductio,
and the theorem.
\end{proof}

\chapter{Rewriting the grammar}
\label{ch:rewrite}

We have already noted
that no rules of \Cg{}
have a zero-length RHS,
and that all symbols must be either nulling or telluric.
These restrictions follow Aycock and Horspool~\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
\cite{AH2002} shows how to do this
without loss of generality.

Because Marpa claims to be a practical parser,
it is important to emphasize
that all grammar rewrites in this document
allow the original grammar to be reconstructed
simply and efficiently at evaluation time.
As implemented,
the Marpa parser allows users to associate
semantics with an external grammar
that has none of the restrictions imposed
on the internal grammars.
As his external grammar,
the Marpa::R2 user
may specify any proper context-free grammar.
A ``proper'' grammar is one which is cycle-free,
and which contains no unproductive or inaccessible
symbols.\footnote{
In fact, as of this writing, Marpa::R2 has options
which allow grammars with cycles and inaccessible symbols,
but there is very little interest in these,
and future Marpa versions are likely to remove this support.
}

Marpa external grammars allow nullable,
properly nullable and nulling symbols,
as well as empty rules.
The user specifies his semantics in terms
of the external grammar.
Marpa rewrites the external grammar into
an internal grammar.
Parsing and evaluation
are performed
in such a way as to keep the internal grammar
invisible to
the user.
From the user's point of view,
the external grammar is the
one being used for the parse,
and the one to which
his semantics is applied.

The rewrite currently used by Marpa is an improvement over
that of~\cite{AH2002}.
Rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
(A ``proper nullable'' is a nullable symbol which is not nulling.)
This is similar to a rewrite into Chomsky form.

The proper nullable symbol is then cloned into two others:
one nulling and one telluric.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols,
and new rules are created as necessary to ensure that all possible combinations
of nulling and telluric symbols are accounted for.

The rewrite in~\cite{AH2002} was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the internal grammar could be
an exponential function of numbers in the external grammar.
In our version, the worst case growth in the number of rules in linear.

This rewrite can be undone easily.
In fact,
in the current implementation of Marpa,
the reverse rewrite,
from internal to external,
is often done ``on the fly'',
as the parse proceeds.
This translation back and forth is
efficient,
and is done for error reporting, tracing,
and in the implementation of Marpa::R2's event
mechanism.

Future plans for Marpa include more aggressive use
of rewrites.
It should be possible, not only to eliminate proper
nullables from the internal grammar,
but also to eliminate nulling symbols.
We conjecture that elimination of nulling symbols
from the internal grammar will greatly simplify the implementation.
The reader may observe that it would
simplify this document if it did not have to deal with nulling
symbols.

Not all rewrites lend themselves to easy translation
and reversal.
As a future direction, we will look at a general schema
for ``safe'' grammar rewrites.
In this schema, Marpa's internal grammar will have
``brick'' and ``mortar'' symbols.
Internal brick symbols correspond, many-to-one, to
external symbols.
Internal mortar symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.

Assume that we have a parse, using internal symbols.
We define a ``brick traversal'' from a ``root'' brick non-terminal instance.
The ``brick traversal'' is pre-order
and stops traversing any path when it hits
a brick symbol instance other than the ``brick root''.
In this way, it traces a subtree of the parse,
where the root of the subtree is the brick root symbol instance,
and the leaves of the subtree are a sequence of other brick
symbol instances.
The leaves of the subtree, as encountered in pre-order,
constitute its ``brick frontier''.
Mortar symbols will only occur in the interior of this ``brick'' subtree,
never as its root or its leaf.

An internal symbol instance \textbf{matches}
an external symbol instance if and only if
\begin{itemize}
\item they have the same symbol;
\item they have the same left location; and
\item they have the same right location.
\end{itemize}

For a rewrite to be ``safe'':
\begin{itemize}
\item Every brick symbol must translate to exactly one
external symbol
\item Every terminal symbol instance must be a brick symbol.
\item The internal and external input sequences must be the same length.
\item The internal and external input sequences must allow
a shared indexing
scheme, which indexes the input symbols consecutively from
left to right.
\item
For every index \var{i},
the \var{i}'th symbol instance in the internal input sequence
must match
the \var{i}'th symbol instance in the external input sequence.
\item Every brick traveral must translate to an external rule
instance,
and vice versa,
as follows:
\begin{itemize}
\item The brick root symbol instance must match
the LHS symbol instance of the external rule instance.
\item
The brick frontier must be the same length as
the RHS of the rule.
\item
The brick frontier and the external rule RHS must allow a shared indexing
scheme,
which indexes both of them
consecutively
from left to right.
\item
For every index \var{i},
the \var{i}'th
instance in the brick frontier must
match the \var{i}'th symbol instance
of the external rule RHS.
\end{itemize}
\end{itemize}

\chapter{Dotted rules}
\label{ch:dotted}

\section{Definition}

Let $\Vrule{r} \in \Crules$
be a rule.
Recall that \Vsize{r}
is the length of its RHS.
A dotted rule (type \type{DR}) is a duple, $[\Vrule{r}, \var{dotix}]$,
where $0 \le \var{dotix} \le \size{\Vrule{r}}$.
The
\dfn{dot RHS index}
or
\dfn{dot index},
\var{dotix}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation}
\label{eq:def-dotted-rule-10}
\Vdr{dr} = [\Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z}]
\end{equation}
is the dotted rule with the dot at
$\var{dotix} = 2$,
that is,
between \Vsym{Y} and \Vsym{Z}.

We write
\Vop{Dotix}{x}%
\index{recce-notation}{Dotix(x)@\Vop{Dotix}{x}}
for the dot index of
\Vdr{x},
and \Vop{Rule}{x}%
\index{recce-notation}{Rule(x)@\Vop{Rule}{x}}
for the rule of \Vdr{x}.
For example, where \Vdr{dr} is as in
\eqref{eq:def-dotted-rule-10},
\begin{gather*}
  \op{Dotix}{\Vdr{dr}} = 2 \\
  \text{and} \quad
\op{Rule}{\Vdr{dr}} = [\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}].
\end{gather*}

In the discussions to follow
we will often refer to the
``dot location''.
The dot location should not be confused with the
dot index.
Dot locations will be locations in the input,
and will require the dotted rule
to be placed in the context of an input,
as will be explained in
Chapter \ref{ch:earley-items}.

We will sometimes write the dotted rule as a duple,
for example,
we might write
\eqref{eq:def-dotted-rule-10}
as
\[
[ [ \Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z} ], 2 ] \\
\]
or redundantly, as
\[
[ [ \Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z} ], 2 ].
\]

\section{Properties}

Let
\begin{align*}
%
\Postdot{\Vdr{x}} & \defined
\begin{cases}
\begin{aligned}
& \Vsym{B}, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{s1} \mydot \Vsym{B} \cat \Vstr{s2}]$} \\
& \Lambda, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{rhs} \mydot]$}
\end{aligned}
\end{cases} \\
%
\Predot{\Vdr{x}} & \defined
\begin{cases}
\begin{aligned}
& \Vsym{B}, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{s1} \cat \Vsym{B} \mydot \Vstr{s2}]$} \\
& \Lambda, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \mydot \Vstr{rhs} ]$}
\end{aligned}
\end{cases} \\
%
\Next{\Vdr{x}} & \defined
\begin{cases}
[\Vsym{A} \de \Vstr{s1} \cat \Vsym{B} \mydot \Vstr{s2}],  \\
\begin{aligned}
& \qquad && \text{if $\Vdr{X} =
[\Vsym{A} \de \Vstr{s1} \mydot \Vsym{B} \cat \Vstr{s2}]$} \\
& \Lambda, && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{rhs} \mydot]$}
\end{aligned}
\end{cases} \\
%
\Prev{\Vdr{x}} & \defined
\begin{cases}
[\Vsym{A} \de \Vstr{s1} \mydot \Vsym{B} \cat \Vstr{s2}],  \\
\begin{aligned}
& \qquad && \text{if $\Vdr{X} =
[\Vsym{A} \de \Vstr{s1} \cat \Vsym{B} \mydot \Vstr{s2}]$} \\
& \Lambda, && \text{if $\Vdr{x} = [\Vsym{A} \de \mydot \Vstr{rhs} ]$}
\end{aligned}
\end{cases} \\
%
\end{align*}

In addition, if a dotted rule is
\[
[\Vsym{A} \de \Vstr{prefix} \mydot \Vstr{suffix}]
\]
we say that \Vstr{prefix} is the
\dfn{dot prefix},
and that
\Vstr{suffix} is the
\dfn{dot suffix}.

\section{Types}

The \dfn{start dotted rule} is
\begin{equation}
\label{eq:start-rule-def}
\Vdr{start} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation}
The \dfn{accept dotted rule} is
\begin{equation*}
\label{eq:accept-rule-def}
\Vdr{accept} = [\Vsym{accept} \de \Vsym{start} \mydot ].
\end{equation*}

We divide all dotted rules into five disjoint types:
start, prediction, null-scan, read and reduction.

\begin{baredefinition}
\qdtitle{Start}
\label{def:start-dr}
The
start dotted rule was defined
in \eqref{eq:start-rule-def}.
Its type is
\xdfn{start}{start (dotted rule)}.
\end{baredefinition}

\begin{baredefinition}
\qdtitle{Prediction}
\label{def:prediction-dr}
If a rule does not have a predot symbol and is not the start dotted rule,
it is a
\xdfn{predicted dotted rule}{predicted (dotted rule)}
or a
\xdfn{prediction}{prediction (dotted rule)}.
\end{baredefinition}

\begin{baredefinition}
\qdtitle{Null-scan}
\label{def:null-scan-dr}
If a rule does have a predot symbol and that symbol is a nulling terminal,
that rule
is a
\xdfn{null-scan}{null-scan (dotted rule)}
dotted rule.
\end{baredefinition}

\begin{baredefinition}
\qdtitle{Read}
\label{def:read-dr}
If a rule does have a predot symbol and that symbol is a telluric terminal,
the rule is a
\xdfn{read}{read (dotted rule)}
dotted rule.
\end{baredefinition}

\begin{definition}
\qdtitle{Reduction}
\label{def:reduction-dr}
If a rule does have a predot symbol and that symbol is a non-terminal
it is a
\xdfn{reduced}{reduced (dotted rule)}
dotted rule,
or a
\xdfn{reduction}{reduction (dotted rule)}.
\end{definition}

Prediction and null-scan dotted rules are called
\xdfn{ethereal}{ethereal (dotted rule)}
dotted rules.
All other dotted rules are called
\xdfn{telluric}{telluric (dotted rule)}
dotted rules.

A predicted dotted rule
always has a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{alpha} ].
\end{equation*}
A
\xdfn{confirmed dotted rule}{confirmed (dotted rule)},
or
\xdfn{confirmation}{confirmation (dotted rule)},
is a dotted rule
with a dot position greater than zero.
A
\xdfn{completed dotted rule}{completed (dotted rule)},
or a
\xdfn{completion}{completion (dotted rule)},
is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{alpha} \mydot ].
\end{equation*}
A
\xdfn{penultimate dotted rule}{penultimate (dotted rule)},
or a
\xdfn{penult}{penult (dotted rule)},
is a dotted rule with exactly one symbol
between its
dot position and the end of its RHS,
for example,
\begin{equation*}
\Vdr{penult} = [\Vsym{A} \de \Vstr{alpha} \mydot \Vsym{B} ].
\end{equation*}

\section{Quasi-types}

When classifying dotted rules,
it is often convenient
to ignore the effect of nulling symbols.
Intuitively, if a dotted rule is of the kind ``X'',
then a quasi-X dotted rule is a dotted rule that would be
of kind X, if it were not for its nulling symbols.

A dotted rule which has only nulling symbols in its dot
suffix
is
\xdfn{quasi-complete}{quasi-complete (dotted rule)}.
A quasi-complete dotted rule is called an
\xdfn{quasi-completion}{quasi-completion (dotted rule)}.
A dotted rule which has exactly one telluric symbol in
its dot suffix is
\xdfn{quasi-penultimate}{quasi-penultimate (dotted rule)}.
A quasi-penultimate dotted rule is called an
\xdfn{quasi-penult}{quasi-penult (dotted rule)}.
If a dotted rule is not quasi-complete,
it is said to be
\xdfn{quasi-incomplete}{quasi-incomplete (dotted rule)},
or a
\xdfn{quasi-incompletion}{quasi-incompletion (dotted rule)}.
These definitions may be satisfied vacuously:
for example,
all completions are quasi-completions.

If
\begin{equation*}
\begin{split}
\Vdr{quasi} & = [\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} ] \\
\Vdr{completion} & = [\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot ] \\
& \qquad \text{where} \quad \Vstr{nulls} = \epsilon
\end{split}
\end{equation*}
is a pair of dotted rules,
we say that \Vdr{completion} is the
\xdfn{completion dotted rule}{completion (dotted rule, of another dotted rule)}
of the quasi-completion \Vdr{quasi}.

A dotted rule which has only nulling symbols before the dot
is a
\xdfn{quasi-predicted}{quasi-predicted (dotted rule)}
dotted rule,
or a
\xdfn{quasi-prediction}{quasi-prediction (dotted rule)}.
This definition may be satisfied vacuously:
all predictions are quasi-predictions.
If a dotted rule is not a
quasi-prediction,
then it is a
\xdfn{quasi-confirmed}{quasi-confirmed (dotted rule)}
dotted rule,
or a
\xdfn{quasi-confirmation}{quasi-confirmation (dotted rule)}.

\begin{theorem}
\title{Quasi-predictions and quasi-completions are disjoint}
\label{t:quasi-drs-disjoint}.
In Marpa grammars,
no quasi-completion is a quasi-prediction.
\end{theorem}

\begin{proof}
The rewrite of
Marpa grammars
eliminates all nullable rules.
So every rule must have a telluric symbol.
In a dotted rule, therefore,
there must be
at least one telluric symbol
and it must come either before the dot
or after it.
If a telluric symbol comes before the dot,
the dotted rule might be a quasi-completion,
but it cannot be a quasi-prediction.
If a telluric symbol comes after the dot,
the dotted rule might be a quasi-prediction,
but it cannot be a quasi-completion.
\end{proof}

\section{Fleeting and lasting}

We also divide dotted rules into two disjoint classes
based on their postdot symbol.
A dotted rule with a null postdot symbol is called \dfn{fleeting}.
Any other dotted rule is called \dfn{lasting}.
A completion is always a lasting dotted rule.

Every dotted rule has a lasting equivalent.
If the dotted rule is lasting, it is its own \dfn{lasting equvialent}.
If the dotted rule is fleeting and quasi-incomplete,
so that without loss of generality it is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} \cat \Vsym{B} \cat \Vstr{after} ]
\end{equation}
where $\Vstr{nulls} = \epsilon$,
and $\Vsym{B}$ is a telluric symbol,
then its \dfn{lasting equivalent} is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot \Vsym{B} \cat \Vstr{after} ].
\end{equation}
If the dotted rule is fleeting and quasi-complete,
so that without loss of generality it is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} ]
\end{equation}
where $\Vstr{nulls} = \epsilon$,
then its \dfn{lasting equivalent} is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot ].
\end{equation}

\section{The transition function}

We define
a partial transition function from
pairs of dotted rule and symbol
to sets of dotted rules.
\begin{equation*}
\GOTO: \Cdr, (\epsilon \cup \var{vocab}) \mapsto 2^\Cdr.
\end{equation*}
$\GOTO(\Vdr{from}, \epsilon)$ is a
\dfn{null transition}
and its result is a \dfn{null transition set}.
``null'' is an overloaded term,
so we more often call the null transition
an \dfn{ethereal transition}
and the null transition set
an \dfn{ethereal transition set}.
If a transition is not an ethereal transition,
it is a \dfn{telluric transition},
and if a transition set
is not an ethereal transition set,
it is a \dfn{telluric transition set}.

A telluric transition set is always the empty set
or a singleton set.
Only ethereal transition sets have
cardinalities greater than one.
The dotted rules in the set that results from an ethereal transition
will be either predictions or confirmed rules with
a nulling predot symbol.

Where the transition is over a symbol,
call it \Vsym{A},
\begin{multline*}
\GOTO(\Vdr{from}, \Vsym{A}) = \\
\begin{cases}
\begin{aligned}
& \left\lbrace \Next{\Vdr{from}} \right\rbrace,
  && \text{if $\Vsym{A} = \Postdot{\Vdr{from}}$} \\
& \emptyset,
  && \text{otherwise}
\end{aligned}
\end{cases}
\end{multline*}

Ethereal transitions are more complicated,
but their analysis will come in useful later.
Let \var{null-scan-dr-op} be the set of
pairs of dotted rules
\begin{gather*}
\left\lbrace \Vdr{cause}, \Vdr{effect} \right\rbrace \quad \text{such that} \\
\Vdr{cause} = [ \Vsym{A} \de \Vstr{before-B} \mydot \Vsym{B} \cat \Vstr{after-B} ] \quad \text{and} \\
\Vdr{effect} = [ \Vsym{A} \de \Vstr{before-B} \cat \Vsym{B} \mydot \Vstr{after-B} ] \quad \text{and} \\
\Vsym{B} \derives \epsilon.
\end{gather*}
We say that \Vdr{cause} is the \dfn{top-down cause} of \Vdr{effect},
and that \Vsym{B} is the \dfn{bottom-up cause}.

We can use \var{null-scan-dr-op} to define an equivalence relation.
Intuitively, two dotted rules, \Vdr{dr1}
and \Vdr{dr2} are \dfn{ethereally equivalent} if
\Vdr{dr1} can be changed into \Vdr{dr2}
by iteration of \var{null-scan-dr-op}.
More formally, we define \var{eth-eq} to be the reflexive, symmetric
and transitive closure of
\var{null-scan-dr-op}.
We say that \Vdr{dr1} is
\dfn{ethereally equivalent} to \Vdr{dr2} if
and only if
\Vdr{dr1} is an element of the
equivalence class of \var{eth-eq} under \Vdr{dr2}.

Let \var{predict-dr-op} be the set of
pairs of dotted rules
\begin{gather*}
\quad \left\lbrace \Vdr{cause}, \Vdr{effect} \right\rbrace \,\, \text{such that} \\
 \quad \Vdr{cause} = [ \Vsym{A} \de \Vstr{before-B} \mydot \Vsym{B} \cat \Vstr{after-B} ] \quad \text{and} \\
 \quad \Vdr{effect} = [ \Vsym{B} \de \mydot \Vsym{C} \cat \Vstr{after-C} ]
\end{gather*}
We say that \Vdr{cause} is the \dfn{top-down cause} of \Vdr{effect}.
For symmetry, we say that \Vdr{cause}
has a bottom-up cause,
but that the \dfn{bottom-up cause} is ethereal.

\begin{FlushLeft}
Let
\begin{equation}
\var{epsilon-dr-op} = \var{null-scan-dr-op} \cup \var{predict-dr-op}.
\end{equation}
We are now in a position to define the ethereal transition of \GOTO{}
from the dotted rule \Vdr{base}.
It is the transitive closure of \var{epsilon-op}
over the singleton set containing the dotted rule argument of \GOTO{}
if there is a postdot symbol.
Otherwise it is the empty set.
\begin{multline*}
\GOTO(\Vdr{from}, \epsilon) = \\
\begin{cases}
\begin{aligned}
& \var{epsilon-op}^+(\lbrace \Vdr{base} \rbrace),
  && \text{if $\Postdot{\Vdr{from}} \neq \Lambda$} \\
& \emptyset,
  && \text{otherwise}
\end{aligned}
\end{cases}
\end{multline*}

\end{FlushLeft}

The
\xdfn{ethereal closure}{ethereal closure (of a dotted rule)}
is the reflexive and transitive closure of
\var{epsilon-dr-op}:
\[\var{ethereal-dr-closure} \defined \var{epsilon-dr-op}^\ast.\]
We say that
the ethereal closure for a dotted rule is the ethereal closure of the singleton
set containing that dotted rule:
\begin{multline*}
\var{ethereal-dr-closure}(\Vdr{base}) \defined  \var{ethereal-dr-closure}(\lbrace \Vdr{base} \rbrace)
\end{multline*}

Let
\begin{equation}
\Vdrset{ec} = \var{ethereal-dr-closure}(\lbrace \Vdr{base} \rbrace).
\end{equation}
We also call \Vdrset{ec} an \dfn{ethereal closure}
and we say that \Vdr{base} is its \dfn{base}.
If \Vdr{base} is telluric, we say that
\Vdr{base} is a \dfn{telluric base}.
We call \Vdr{tell} a telluric base
of a dotted rule \Vdr{dr2} if and only if
\[ \Vdr{dr2} \in \var{ethereal-dr-closure}(\lbrace \Vdr{tell} \rbrace). \]

\section{Ethereal closure}

\begin{theorem}
\ttitle{Ethereal equivalents have same telluric base}
\label{t:eth-eq-share-telluric-base}
If the dotted rules \Vdr{dr1}
and \Vdr{dr2} are ethereally equivalent,
and \Vdr{dr1} is quasi-confirmed,
then \Vdr{dr1}
and \Vdr{dr2} have the same telluric base.
\end{theorem}

\begin{proof}
By assumption for the theorem,
\Vdr{dr1} is quasi-confirmed,
so that
by Theorem \ref{t:quasi-drs-disjoint},
\Vdr{dr1}
is not a quasi-prediction.
Therefore, \Vdr{dr1} has a telluric symbol in its
dot prefix.
Therefore,
\Vdr{dr1} has a telluric base.
Without loss of generality,
we let
\begin{equation}
\label{eq:eth-eq-share-telluric-base-10}
\Vdr{dr1} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1} \mydot \Vstr{post1} ],
\end{equation}
and let the telluric base be
\begin{equation}
\label{eq:eth-eq-share-telluric-base-20}
\Vdr{tell} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \mydot \Vstr{nulls1} \cat \Vstr{post1} ],
\end{equation}
where $\Vstr{nulls1} = \epsilon$.

We now proceed by overlapping cases.
In the first case,
the dot in \Vdr{dr2} comes at or after the dot in \Vdr{dr1}.
Since \Vdr{dr2} is ethererally equivalent
to
\eqref{eq:eth-eq-share-telluric-base-10},
we have,
if we rewrite \Vstr{post1}
as $\Vstr{nulls2} \cat \Vstr{post2}$,
\begin{multline}
\label{eq:eth-eq-share-telluric-base-23}
\Vdr{dr2} =
  [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1} \cat \Vstr{nulls2} \mydot \Vstr{post2} ].
\end{multline}
Therefore, from
\eqref{eq:eth-eq-share-telluric-base-20},
\eqref{eq:eth-eq-share-telluric-base-23}
and the definition of ethereal closure,
\[ \Vdr{dr2} \in \var{ethereal-dr-closure}(\lbrace \Vdr{tell} \rbrace).\]
By the definition of telluric base,
\Vdr{tell} is the telluric base of \Vdr{dr2}.

In the second case
the dot in \Vdr{dr2} comes at or before the dot in \Vdr{dr1}.
We may write
\begin{equation*}
\Vdr{dr2} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1a} \mydot \Vstr{nulls1b} \cat \Vstr{post1} ],
\end{equation*}
where $\Vstr{nulls1} = \Vstr{nulls1a} \cat \Vstr{nulls1b}.$
Again,
by the definition of telluric base,
\Vdr{tell} is the telluric base of \Vdr{dr2}.
In both cases,
we have shown that
\Vdr{tell} is the telluric base of \Vdr{dr2}.
\end{proof}

Predicted dotted rules may have more than one telluric base,
but for null-scan dotted rules the telluric base
will be unique.

\begin{theorem}
\ttitle{Telluric base of a quasi-confirmed dotted rule is unique}
\label{quasi-confirmed-unique-telluric-base}
If a dotted rule is quasi-confirmed,
its telluric base is unique.
\end{theorem}

\begin{proof}
Let the dotted rule be \Vdr{dr}.
This theorem follows
directly
from Theorem \ref{t:eth-eq-share-telluric-base},
if you set both of its dotted rules to \Vdr{dr}.
\end{proof}

The complexity of the ethereal closure is of interest:
we may want to compute it on the fly,
and in any case,
we certainly want to show that
the ethereal closure has finite time complexity.
\begin{algorithm}[!htp]
\caption{Add a generation to the ethereal closure}
\label{alg:ethereal-generation}
\begin{algorithmic}[1]
\Procedure{Ethereal next}{\Vdr{this}, \Vdrset{results}, \Vdrset{work}}
\If{\Vdr{this} has no postdot symbol}
\State return
\EndIf
\State Here \Vdr{this} is $[ \Vsym{lhs} \de \Vstr{before} \mydot \Vsym{A} \cat \Vstr{after} ]$
\label{line:ethereal-generation-20}
\State \Comment We can state this without loss of generality
\If{$\Vsym{A}$ is a nulling symbol}
\State $\Vdr{new} \gets [ \Vsym{lhs} \de \Vstr{before} \cat \Vsym{A} \mydot \Vstr{after} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \ldots
\State $\qquad$ but only if it has never been added before
\State return
\EndIf
\label{line:ethereal-generation-40}
\State Here \Vsym{A} must be a telluric symbol
\For{ each \Vrule{r} in \Cg{}}
\If{ $\LHS{\Vrule{r}} = \Vsym{A}$ }
\State Here \Vrule{r} is $[ \Vsym{A} \de \Vstr{rhs} ]$
\State \Comment We can state this without loss of generality
\State $\Vdr{new} \gets [ \Vsym{A} \de \mydot \Vstr{rhs} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \dots
\label{line:ethereal-generation-60}
\State $\qquad$ but only if it has never been added before
\EndIf
\EndFor
\State return
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Add a generation to the ethereal closure}

\begin{algorithm}[!htp]
\caption{Create ethereal closure}
\label{alg:ethereal-closure}
\begin{algorithmic}[1]
\Function{Create ethereal closure}{\Vdr{base}}
\State $\Vdrset{result} \gets \emptyset$
\State $\Vdrset{work} \gets \emptyset$
\State \Call{Ethereal next}{\Vdr{base}}
\While{$\Vdrset{work} \neq \emptyset$}
\State Remove a dotted rule from \Vdrset{work}, call it \Vdr{this}
\State \Call{Ethereal next}{\Vdr{this}, \Vdrset{result}, \Vdrset{work}}
\EndWhile
\State return \Vdrset{result}
\EndFunction
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Create ethereal closure}

Algorithm \ref{alg:ethereal-closure}
is not actually used by any
of Marpa's versions ---
it is chosen because it is
convenient for exploring the theory.
In the actual implementation,
null-scans are dealt with implicitly,
while predictions are explicitly computed after
each Earley set is otherwise complete.

\begin{theorem}\label{t:ethereal-closure-Oc}
\ttitle{Ethereal closure is constant time}
Ethereal closure has time complexity \Oc{}.
\end{theorem}

\begin{proof}
We consider Algorithm \ref{alg:ethereal-closure}.
This clearly runs in \Oc{} time if there is a constant
number of calls to
Algorithm \ref{alg:ethereal-generation}.

To finish the proof, we need to show
that
Algorithm \ref{alg:ethereal-generation}
is called a constant number
of times.
Algorithm \ref{alg:ethereal-generation}
is called
once for the base dotted rule of the computation.
It is called again for every dotted rule added to the working set
of dotted rules, \Vdrset{work}.
We know that no dotted rule is added to
\Vdrset{work} twice.
Therefore
Algorithm \ref{alg:ethereal-generation}
is called
at most once for each dotted rule.
\Cg{} has a fixed number of dotted rules,
so
that Algorithm \ref{alg:ethereal-generation}
is called
at most \Oc{} times.
\end{proof}

\begin{theorem}\label{t:ethereal-closure-dr-correct}
\ttitle{Ethereal closure algorithm is correct}
Algorithm \ref{alg:ethereal-closure} is correct.
\end{theorem}

\begin{proof}
From examining
Algorithm \ref{alg:ethereal-closure},
in particular
lines
\ref{line:ethereal-generation-20}-\ref{line:ethereal-generation-40}
of
Algorithm \ref{alg:ethereal-generation},
we see that
\begin{equation}
\label{ethereal-closure-correct-2}
\myparbox{
the null transitions
for nulling postdot symbols are complete and consistent,
and therefore correct.
}
\end{equation}

From examining
Algorithm \ref{alg:ethereal-closure},
in particular
lines
\ref{line:ethereal-generation-20}-\ref{line:ethereal-generation-40}
of
Algorithm \ref{alg:ethereal-generation},
we see that the null transitions for predictions are
properly made,
so that
\begin{equation}
\label{ethereal-closure-correct-3}
\myparbox{
the set of predictions is consistent.
}
\end{equation}

It remains to show that the set of predictions is complete.
Algorithm \ref{alg:ethereal-closure}
clearly adds all predictions derivable in a single step
to its results.
It also
calls the ``Ethereal next'' function
repeatedly, so that indirect predictions will be added.
But it will refuse to
add a dotted rule to its working set more than once.
We need to consider whether this means some predictions
will not be derived.

Consider a prediction
\begin{equation}
\label{ethereal-closure-correct-5}
\Vsym{lhs-pred} \de \mydot \Vsym{pred-rhs}
\end{equation}
which is derived through a series of dotted rule predictions
added to the work list at line
\ref{line:ethereal-generation-60}
of Algorithm \ref{alg:ethereal-generation}.
For a reductio,
assume that one prediction,
call it
\begin{equation}
\label{ethereal-closure-correct-15}
[ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ],
\end{equation}
occurs twice.
Without loss of generality, let that chain be
\begin{align*}
& [ \Vsym{lhs0} \de \mydot \Vsym{rhs0} ] && \text{Step 0} \\
& [ \Vsym{lhs1} \de \mydot \Vsym{rhs1} ] && \text{Step 1} \\
& \ldots && \\
& [ \Vsym{lhs-predup} \de \mydot \Vsym{lhs-dup} \cat \Vstr{after-predup} ] && \text{Step \Vdecr{i}} \\
& [ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ] && \text{Step \var{i}} \\
& \ldots && \\
& [ \Vsym{lhs-predup2} \de \mydot \Vsym{lhs-dup} \cat \Vstr{after-predup2} ] && \text{Step \Vdecr{j}} \\
& [ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ] && \text{Step \var{j}}\\
& \ldots && \\
& [ \Vsym{lhs-penult} \de \mydot \Vsym{rhs-penult} ] && \\
& [ \Vsym{lhs-last} \de \mydot \Vsym{rhs-last} ]
\end{align*}
where Step \var{i} is the first occurrence of
\eqref{ethereal-closure-correct-15},
and Step \var{j} is the last.
We can create a shorter chain of predictions by removing the steps in
the chain from Step $\var{i}+1$ to Step \var{j}.
Call this process of removing steps, ``pruning duplicates''.

By pruning duplicates for every prediction which occurs twice in
the chain,
we see that we can create a chain that results in
\eqref{ethereal-closure-correct-5},
but which does not contain any prediction more than once.
We can also see that, since
Algorithm \ref{alg:ethereal-closure} follows all chains
that contain no duplicate predictions,
that
Algorithm \ref{alg:ethereal-closure} will add
\eqref{ethereal-closure-correct-5} to its result.

Since
\eqref{ethereal-closure-correct-5} was chosen without loss
of generality,
we see that every prediction can be reached by following
a chain of predictions with no duplicate predictions,
and that therefore
\begin{equation}
\label{ethereal-closure-correct-40}
\myparbox{
Algorithm \ref{alg:ethereal-closure} adds a complete
set of predictions.
}
\end{equation}

We now summarize our results.
By definition, the ethereal closure is the
transitive closure of the union
of predictions and null-scans.
In \eqref{ethereal-closure-correct-2}
we showed that the sets of
null-scans added are correct and,
in \eqref{ethereal-closure-correct-3},
that the set of predictions added is consistent.
In
\eqref{ethereal-closure-correct-40}
we showed
that the set of predictions
added is complete.
This shows the theorem.
\end{proof}

\chapter{Earley items}
\label{ch:earley-items}

\section{Definition}

An Earley item (type \type{EIM}) is a triple
\[
    [\Vdr{dotted-rule}, \Vorig{x}, \Vloc{current} ]
\]
of dotted rule, origin, and current location.

The \dfn{origin} is the location where recognition of the rule
started.
(It is sometimes called the ``parent''.)
The \dfn{current} or \dfn{dot location} is the location
in the input, \Cw{}, of the dot position in \Vdr{dotted-rule}.
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.  Where
\begin{gather*}
    \Veim{x} = [\Vdr{x}, \Vorig{x}, \Vloc{x} ] \\
\text{we say that $\DR{\Veim{x}} = \Vdr{x}$,} \\
\Origin{\Veim{x}} = \Vorig{x} \\
\text{and $\Current{\Veim{x}} = \Vloc{x}$.}
\end{gather*}

Traditionally, an Earley item is shown as a duple,
\[
    [\Vdr{dotted-rule}, \Vorig{x} ]
\]
with \Vloc{current} omitted,
and we will sometimes use this form.
When the duple form is used,
the current location is specified by the context,
either explicitly or implicitly.

\section{Types}

Whenever a dotted rule notion is applied to an EIM,
it refers to the dotted rule of the EIM.
For example,
a completion EIM is an EIM with a completion
dotted rule,
and a predicted EIM is an EIM with a predicted dotted rule.
If
\[\Veim{quasi} = [ \Vdr{quasi}, \var{i}, \var{j} ]\]
is a quasi-complete EIM,
then its lasting completion EIM is
\[[ \Vdr{complete}, \var{i}, \var{j} ],\]
where \Vdr{complete} is the completion dotted
rule of \Vdr{quasi}.

The \dfn{start EIM} is
\begin{equation}
\label{eq:def-start-eim}
\Veim{start} = [ [\Vsym{accept} \de \mydot \Vsym{start} ], 0, 0 ].
\end{equation}
The \dfn{accept EIM} is
\begin{equation}
\label{eq:def-accept-eim}
\Vdr{accept} = [ [\Vsym{accept} \de \Vsym{start} \mydot ], 0, \Vsize{\Cw} ].
\end{equation}

\begin{theorem}
\ttitle{Earley item types}
\label{eim-types-correct}
Every EIM falls into one of these
five disjoint types:
start, prediction, read, null-scan and reduction.
\end{theorem}

\begin{proof}
Recall that EIM's take their type from their dotted rule.
The proof then follows directly from definitions
\ref{def:start-dr},
\ref{def:prediction-dr},
\ref{def:null-scan-dr},
\ref{def:read-dr}
and \ref{def:reduction-dr}
above.
\end{proof}

\begin{definition}
Following the convention for dotted rules,
EIM's are
\xdfn{telluric}{telluric (of an EIM)}
if they are the start EIM,
a read EIM or a reduction EIM.
An EIM is
\xdfn{ethereal}{ethereal (of an EIM)}
if it is not telluric.
\end{definition}

The idea is that telluric dotted rules are ``grounded'' either
in the input or in the initial state of the parse,
while
ethereal dotted rules emerge out of an invisible realm.

\section{Validity}

\begin{definition}
\label{def:eim-valid}
\dtitle{EIM Validity}
We say that
an Earley item
\begin{equation}
\label{eq:def-eim-valid-5}
\bigl[[\Vsym{A} \de \Vstr{predot} \mydot \Vstr{postdot}], \var{i}, \var{j} \bigr]
\end{equation}
is \dfn{valid}
if and only if
\begin{equation}
\label{eq:def-eim-valid-10}
\Vsym{A} \derives [\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot}.
\end{equation}
Note that
\eqref{eq:def-eim-valid-10}
implies that \CW{} is seen as far as \Vloc{j}.
If \Veim{x} is valid, we also say \Valid{\Veim{x}}.
\eqref{eq:def-eim-valid-10}
is called the
\dfn{derivation validity equivalent},
of \eqref{eq:def-eim-valid-5}.
\eqref{eq:def-eim-valid-5}
is called either the
\dfn{parse instance validity equivalent}
or the \dfn{EIM validity equivalent}
of \eqref{eq:def-eim-valid-10}.
When it is clear in context,
we will use a more simple term,
\dfn{validity equivalent}, for
a derivation validity equivalent,
a EIM validity equivalent,
or a parse instance validity equivalent.
\end{definition}

The following definitions will be useful
when relating EIM's to their validity equivalents.
\begin{definition}
\dtitle{EIM validity equivalent}
\label{def:eim-validity-equivalent}
Without loss of generality,
let
\begin{equation}
\label{eq:eim-validity-equivalent-5}
\Veim{eim} = \bigl[[\Vsym{A} \de \Vstr{predot} \mydot \Vstr{postdot}], \var{i}, \var{j} \bigr]
\end{equation}
be an Earley item and
\begin{align}
\label{eq:eim-validity-equivalent-10}
& \Vsym{A} && \text{Step \var{s}} \\
& \quad \derives \; \Vmk{i} \Vstr{predot} \Vmk{j} \Vstr{postdot} \Vmk{k}
&& \text{Step \Vincr{s}}
\end{align}
be its validity equivalent.

Then we say that the derivation step
Step \var{s}
is the
\xdfn{LHS step}{LHS step (of an EIM)}
of \Veim{eim}.
We say that the derivation step
Step \Vincr{s}
is the
\xdfn{RHS step}{RHS step (of an EIM)}
of \Veim{eim}.
When we refer to the ``step'' associated with an
EIM without specifing whether LHS or RHS,
we will mean the RHS step.
For example,
when we count derivation steps
between EIM's,
we will regard an EIM
as being located at its RHS step.

We say that
\[
\Vmk{i} \Vsym{A} \Vmk{k}
\]
is the
\xdfn{LHS derivation symbol instance}{LHS derivation symbol instance (of an EIM)},
or more simply
\xdfn{LHS derivation instance}{LHS derivation instance (of an EIM)},
of \Veim{eim}.
We say that the derivation symbol instances in
\[
\Vmk{i} \Vstr{predot} \Vstr{postdot} \Vmk{k}
\]
are the
\xdfn{RHS derivation symbol instances}{RHS derivation symbol instances (of an EIM)},
or more simply
\xdfn{RHS derivation instances}{RHS derivation instances (of an EIM)},
of \Veim{eim}.
And we say that \Vloc{j} is the
\xdfn{dot location in the derivation}{dot location in the derivation (of an EIM)},
or when it is clear in context,
the \xdfn{dot location}{dot location (of an EIM in a derivation)},
of \Veim{eim}.

We will write \Vop{Valid-eq}{x}%
\index{recce-notation}{Valid-eq(x)@\Vop{Valid-eq}{x}}
for the
validity equivalent of \var{x}.
This notation is overloaded ---
\var{x} may be a derivation move or a parse
instance,
and \Vop{Valid-eq}{x} may mean
a derivation validity equivalent,
an EIM validity equivalent,
or a parse instance validity equivalent,
according to its usage in context.
\end{definition}

\begin{definition}
\label{def:4-tuple-eim}
\dtitle{EIM 4-tuple notation}
We will sometimes find it convenient to express EIM's in terms of
derivations,
as 4-tuples:
\begin{equation}
\label{eq:def-4-tuple-eim-10}
\Veim{eim} = \big\langle \var{s}, \var{rha}, \var{rhz}, \var{dotix} \big\rangle
\end{equation}
where \var{s} is the RHS derivation step of \Veim{eim},
the sequence
\[
\drv{d}{\var{s}}{\var{rha} \ldots{} \var{rhz}}
\]
is the RHS of \Veim{eim},
and \var{dotix} is the dot index of \Veim{eim}.

When $\var{dotix} > 0$,
\Veim{eim} expressed in our more usual notation will be
\begin{equation}
\label{eq:def-4-tuple-eim-20}
\Veim{eim} = \big[ [ \Vsym{LH} \de \Vstr{prefix} \mydot \Vstr{suffix} ], \Vloc{i}, \Vloc{j} ],
\end{equation}
where
\eqref{eq:def-4-tuple-eim-20}
\begin{align}
\Vstr{prefix} & =
\begin{cases}
\begin{aligned}
& \Symbol{\drv{d}{\var{s}}{\var{rha} \ldots \var{rha}+\Vdecr{dotix}}} \\
& \qquad \text{if $\var{dotix} > 0$,} \\
& \epsilon, \quad \text{otherwise},
\end{aligned}
\end{cases} \\
\Vstr{suffix} & = \Symbol{\drv{d}{\var{s}}{\var{rha}+\var{dotix} \ldots \var{rhz}}}, \\
 \Vloc{i} & = \Left{\drvVV{d}{s}{rha}} \quad \text{and} \\
 \Vloc{j} & = \Left{\drv{d}{\var{s}}{\var{rha}+\var{dotix}}}.
\end{align}
Also, if \var{s} is the first derivation step at which the
symbol instances
\drv{d}{\var{s}}{\var{rha} \ldots{} \var{rhz}}
appear,
so that they are direct descendants of a parent symbol instance
in derivation step \Vdecr{s},
then
\begin{equation}
\Vsym{LH} = \Symbol{\drv{d}{\Vdecr{s}}{\var{rha}}}
\end{equation}
\end{definition}


\begin{theorem}\label{t:start-eim-is-valid}
\ttitle{Start Earley item is valid}
The start Earley item is valid.
\end{theorem}

\begin{proof}
By the definition of EIM validity,
to show that the start EIM
\eqref{eq:def-start-eim}
is valid,
we need to show
that
\begin{equation}
\label{eq:start-eim-is-valid-15}
\Vsym{accept} \derives \mk{0}\; \Vsym{start} \\
\end{equation}
By the definition of \Vsym{accept},
it is on the LHS of only one rule,
\eqref{eq:accept-rule-def}.
All symbols in \Cg{} are productive,
so that
\begin{equation}
\label{eq:start-eim-is-valid-16}
\Vsym{accept} \derives \Vsym{start} \destar \Vstr{sent}
\end{equation}
where \Vstr{sent} is a sentence.
Since \Vstr{sent} is a sentence,
using \eqref{eq:def-L-g},
we have that
\begin{equation*}
\Vstr{sent} \in \myL{\Cg}.
\end{equation*}
Our convention is that \Cw{}
is the input, or sentence, of interest in
context, so here we assume that,
without loss of generalization,
\begin{equation}
\label{eq:start-eim-is-valid-18}
\Vstr{sent} = \Cw = \Cw[0, \Vdecr{\Vsize{\Cw}}].
\end{equation}
From
\eqref{eq:start-eim-is-valid-16}
and
\eqref{eq:start-eim-is-valid-18},
we have
\begin{align}
& \Vsym{accept} \derives \Vstr{start} \destar \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\therefore \quad & \Vsym{accept} \derives \Vstr{start} \destar \mk{0} \; \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\therefore \quad & \Vsym{accept} \derives \mk{0} \Vstr{start} \destar \mk{0} \; \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\label{eq:start-eim-is-valid-25}
\therefore \quad & \Vsym{accept} \derives \mk{0} \Vstr{start}
\end{align}
Where
\eqref{eq:start-eim-is-valid-25}
is
\eqref{eq:start-eim-is-valid-15},
which is what we needed to show for the theorem.
\end{proof}

\section{Parse instances}

A \dfn{parse instance} is either a symbol instance or an EIM.
When it is clear in context, we will often say ``symbol''
when we mean ``symbol instance''.
A parse instance is often called simply an \dfn{instance}.

\section{Top-down and bottom-up causes}

We referred to top-down and bottom-up causes earlier,
when introducing dotted rules.
We now revisit these concepts in the context of Earley items.

\begin{sloppypar}
If \var{inst} is an Earley item,
call it \Veim{inst}, then
\begin{gather*}
\Left{\Veim{inst}} = \Origin{\Veim{inst}} \\
\text{and} \quad \Right{\Veim{inst}} = \Current{\Veim{inst}}.
\end{gather*}
Let, without loss of generality,
\var{inst} be the a symbol instance
\begin{gather*}
\Vinst{inst} = \Vmk{i} \Vsym{A} \Vmk{j}. \\
\text{Then} \quad \Left{\Veim{inst}} = \Vloc{i} \\
\text{and} \quad \Right{\Veim{inst}} = \Vloc{j}.
\end{gather*}

\end{sloppypar}

\begin{definition}
\dtitle{Causes}
\label{def:causes}
We first consider an Earley item
with a predot symbol instance.
Without loss of generality,
let the Earley item be
\begin{equation*}
\Veim{effect} = [ [ \Vsym{A} \de \Vstr{prefix} \cat \Vsym{up} \mydot \Vstr{suffix} ], \var{i}, \var{k} ].
\end{equation*}
and let the instance be
\[\Vmkl{j} \Vsym{up} \Vmkr{k}.\]
Then we say that
\begin{equation*}
\Veim{down} = [ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{up} \cat \Vstr{suffix} ], \var{i}, \var{j} ]
\end{equation*}
is a \dfn{top-down cause} of \Veim{effect};
we say that \Vsym{up} is a \dfn{bottom-up cause} of \Veim{effect};
and we say that \Veim{effect} is the \dfn{effect} of \Veim{down} and \Vsym{up}.

We now consider an Earley item
with no predot symbol instance.
Without loss of generality,
call it \Veim{effect},
and let
\begin{gather*}
\Vdr{effect} = [ \Vsym{A} \de \mydot \Vstr{rhs} ], \\
\Veim{effect} = [ \Vdr{effect}, \var{k}, \var{k} ] \quad \text{and} \\
\Veim{down} = [ \Vdr{down}, \var{i}, \var{k} ]
\end{gather*}
where \Vdr{down}
is any top-down dotted rule cause of \Vdr{effect}.
Then we call \Veim{down}
a \dfn{top-down cause} of \Veim{effect};
and we say that the \dfn{bottom-up cause} of \Veim{effect} is ethereal.
We call \Veim{effect} the \dfn{effect} of \Veim{down}.

We say that \Veim{anc} is the \dfn{top-down ancestor}
of \Veim{desc},
if \Veim{anc} is a top-down cause of \Veim{desc},
or if \Veim{anc} is the top-down cause of an ancestor of \Veim{desc}.
If \Veim{anc} is a top-down ancestor of \Veim{desc},
then we say the
\Veim{desc}
is a \dfn{top-down descendant} of \Veim{anc}.

\end{definition}

An effect is always an Earley item.
A top-down cause is always an Earley item.
A bottom-up cause may be ethereal or telluric.

If a bottom-up cause is ethereal, it may be because the effect has no
predot symbol,
or because the bottom-up cause is a nulling terminal symbol instance.
If a bottom-up cause is telluric, it may be because it is
an Earley item,
because it is a non-terminal symbol instance,
or because it is a telluric terminal symbol instance.
Recall that rules in \Marpa{} cannot be
nulling,
and therefore Earley items
and non-terminal symbol instances cannot be nulling.

We have already defined validity for Earley items.
We now extend the concept of validity to other types of
causes.
An ethereal cause is always valid.

\begin{definition}
\qdtitle{Validity of terminal symbols}
If \Vsym{T} is a telluric terminal symbol,
and if \CW{} has been seen as far as \Vloc{j}.
we say that
\begin{equation}
\label{eq:def-terminal-validity-10}
[\var{i}]\, \Vsym{T} \,[\var{j}]
\end{equation}
is
\xdfn{valid}{valid (terminal symbol)}
if and only if
\begin{equation}
\label{eq:def-terminal-validity-15}
\Vsym{T} = \CVw{i} \; \land \; \var{j} = \var{i} + 1.
\end{equation}
\eqref{eq:def-terminal-validity-10}
is considered to be the
\xdfn{derivation validity equivalent}{derivation validity equivalent (terminal symbol, of itself)}
of itself.
When it is clear in context,
we will also call
\eqref{eq:def-terminal-validity-10}
the
\xdfn{validity equivalent}{validity equivalent (terminal symbol, of itself)}
of itself.
Overloading our previous use of \myfnname{Valid-eq},
we write \Vop{Valid-eq}{x}%
\index{recce-notation}{Valid-eq(x)@\Vop{Valid-eq}{x}}
for the
validity equivalent
of the terminal symbol instance \Vinst{x}.
\end{definition}

\begin{definition}
\label{def:validity-of-non-terminal}
\qdtitle{Validity of non-terminal symbols}
If \Vsym{N} is a non-terminal symbol,
we say that
\begin{equation}
\label{eq:def-non-terminal-validity-10}
\Vinst{N-ins} = [\var{i}]\, \Vsym{N} \,[\var{j}]
\end{equation}
is
\xdfn{valid}{valid (non-terminal symbol)}
if and only if
it is the LHS of a valid, completed EIM.
\Vinst{N-ins} is considered to be the
\xdfn{derivation validity equivalent}{derivation validity equivalent
  (of non-terminal symbol)}
of itself.
When it is clear in context,
we will use the term,
\xdfn{validity equivalent}{validity equivalent},
for the derivation validity equivalent of a
non-terminal symbol.
Also,
overloading our previous use of \myfnname{Valid-eq},
we will write \Vop{Valid-eq}{x}%
\index{recce-notation}{Valid-eq(x)@\Vop{Valid-eq}{x}}
for validity equivalent
of a non-terminal symbol.
\end{definition}

\section{Confirmed Earley items}

\begin{definition}
\dtitle{Symbolic and EIM equivalent}
If a parse instance is a symbol instance or
a complete EIM,
its ``symbolic equivalent'' is defined.
If the parse instance is a symbol instance,
it is its own \dfn{symbolic equivalent}.
If the parse instance is a complete EIM,
its \dfn{symbolic equivalent} is the instance
of its LHS symbol
with the same left and right locations
as the EIM.
We also write the symbolic
equivalent of \var{x} as
$\var{Sym-Eq}(\var{x})$.%
\index{recce-notation}{Sym-Eq(x)@\Vop{Sym-Eq}{x}}
We say that \Veim{eim} is the
\dfn{EIM equivalent}
of \Vinst{x} if and only if
\Vinst{x} is the symbolic equivalent of
\Veim{eim}.
We also write the EIM
equivalent of \Vinst{x} as
$\var{EIM-Eq}(\var{x})$.%
\index{recce-notation}{EIM-Eq(x)@\Vop{EIM-Eq}{x}}
\end{definition}

By way of illustration,
and with full generality, if
a complete EIM is
\begin{equation}
\label{eq:def-symbolic-equivalent-10}
\Veim{up} = \big[
[ \Vsym{up} \de \Vstr{up-rhs} \mydot ], \var{i}, \var{j}
\big].
\end{equation}
then the symbolic equivalent of \Veim{up} is
\begin{equation}
\label{eq:def-symbolic-equivalent-20}
\Vinst{ins} = \Vmkl{i} \Vsym{up} \Vmkr{j}.
\end{equation}

\begin{theorem}
\ttitle{Symbolic equivalent from complete EIM}
\label{t:valid-symbolic-equivalent-from-eim}
A valid, complete EIM has a valid symbolic equivalent,
\Vinst{symeq};
and \Symbol{\Vinst{symeq}} is
a non-terminal.
\end{theorem}

\begin{proof}
Without loss of generality,
let the complete  EIM be
\begin{equation}
\label{eq:valid-symbolic-equivalent-from-eim-10}
\Veim{up} = \big[
[ \Vsym{up} \de \Vstr{up-rhs} \mydot ], \var{i}, \var{j}
\big].
\end{equation}
\eqref{eq:valid-symbolic-equivalent-from-eim-10}
is valid by assumption for the theorem,
so we have the valid derivations,
\begin{alignat}{3}
& \Vsym{up} && \derives
[ \var{i}]\, \Vstr{up-rhs} \,[\var{j}] \\
\therefore \quad
& [\var{i}]\, \Vsym{up} \,[\var{j}] && \derives
[\var{i}]\, \Vstr{up-rhs} \,[\var{j}] \\
\label{eq:valid-symbolic-equivalent-from-eim-20}
\therefore \quad
& [\var{i}]\, \Vsym{up} \,[\var{j}] &&
\end{alignat}
We have shown that
\eqref{eq:valid-symbolic-equivalent-from-eim-20}
is valid.
Let \Vinst{symeq} be
the parse instance of
\eqref{eq:valid-symbolic-equivalent-from-eim-20},
so that
$\Vinst{symeq} 
= [\var{i}]\, \Vsym{up} \,[\var{j}].$
From the definition of symbolic equivalent we
see that
\Vinst{symeq}
is the symbolic equivalent of
\Veim{up}.

It remains to show that
\Symbol{\Vinst{symeq}} is a non-terminal.
We note that
$\Symbol{\Vinst{symeq}} = \LHS{\Veim{up}}$,
and with this, we have the theorem.
\end{proof}

\begin{theorem}
\ttitle{Symbolic equivalent from complete EIM}
\label{t:eim-equivalent-from-non-terminal}
A valid instance, \Vinst{x}, has an valid EIM equivalent if
and only if \Symbol{\Vinst{x}} is a non-terminal.
\end{theorem}

\begin{proof}
We have the ``if'' direction directly from the
definition of validity for non-terminal parse instances,
Theorem
\ref{t:valid-symbolic-equivalent-from-eim}
shows the ``only if'' direction.
\end{proof}

We say that a bottom-up cause is \dfn{symbolic},
if it has a symbolic equivalent.
The bottom-up causes of predictions
do not have a symbolic equivalent,
and are therefore not symbolic.
All other bottom-up causes are symbolic.

\begin{definition}
\dtitle{Matching causes}
\label{def:matching-causes}
Let \Veim{down},
be a top-down cause and
let \Vinst{up}
be a symbolic bottom-up cause.
We say that \Veim{down} and \Vinst{up}
are
\dfn{matching causes}
if and only if
\begin{gather*}
\Postdot{\Veim{down}} = \Symbol{\Vinst{up}} \quad \text{and} \\
\Right{\Veim{down}} = \Left{\Vinst{up}}.
\end{gather*}
When \Veim{down} and \Vinst{up} are matching causes,
we also say that
\Veim{down} and \Vinst{up} \dfn{match}.
\end{definition}

\begin{theorem}
\ttitle{Effect from symbolic causes}
\label{t:effect-from-symbolic-causes}
If a symbolic bottom-up cause
and a top-down cause match
and are both valid,
their effect is valid.
\end{theorem}

\begin{proof}
Without loss of generality,
let the top-down cause be
\begin{equation}
\label{eq:effect-from-symbolic-causes-3}
\begin{split}
& \Veim{down} =  \\
& \qquad \qquad [ [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j} ].
\end{split}
\end{equation}
and let the symbolic equivalent of the bottom-up cause be
\begin{equation}
\label{eq:effect-from-symbolic-causes-6}
\,[\var{j}]\, \Vsym{A} \,[\var{k}]\, .
\end{equation}

To show the theorem, we must show that
\begin{equation}
\label{eq:effect-from-symbolic-causes-9}
\begin{split}
& \Veim{effect} =  \\
& \qquad \qquad [ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ]
\end{split}
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}
\label{eq:effect-from-symbolic-causes-12}
\Vsym{down} \derives [\var{i}] \Vstr{pre} \cat \Vstr{A} [\var{k}] \Vstr{post}.
\end{equation}

By assumption,
\eqref{eq:effect-from-symbolic-causes-3}
is valid.
From the definition of validity for an Earley item:
\begin{equation}
\label{eq:effect-from-symbolic-causes-20}
\Vsym{down} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \cat \Vstr{post}
\end{equation}

We see that the location markers in
\eqref{eq:effect-from-symbolic-causes-6}
and
\eqref{eq:effect-from-symbolic-causes-20}
are compatible:
\var{i} and \var{k} are unrestricted,
while the use of \var{j} in both derivations is compatible.
Composing them we have
\begin{alignat}{2}
\label{eq:effect-from-symbolic-causes-25}
& \Vsym{down} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \,[\var{k}]\, \Vstr{post} \\
\therefore \quad & \Vsym{down} \derives [\var{i}]\, \Vstr{pre} \cat \Vsym{A} \,[\var{k}]\, \Vstr{post}
\label{eq:effect-from-symbolic-causes-28}
\end{alignat}
Where
\eqref{eq:effect-from-symbolic-causes-28}
is
\eqref{eq:effect-from-symbolic-causes-12},
which is what we needed to show for the theorem.
\end{proof}

\begin{theorem}
\ttitle{Null-scan from top-down cause}
\label{t:null-scan-from-down-cause}
Let
\begin{equation}
\label{eq:null-scan-from-down-cause-5}
\Veim{down} = \big[ [ \Vsym{A} \de \Vstr{prf} \mydot \Vsym{nul} \Vstr{suf} ],
\var{i}, \var{k} \big],
\end{equation}
be a valid EIM
where $\Vsym{nul} = \epsilon$.
Then \Veim{down} has a
unique valid null-scan effect:
\begin{equation}
\label{eq:null-scan-from-down-cause-8}
\Veim{down} = \big[ [ \Vsym{A} \de \Vstr{prf} \Vsym{nul} \mydot \Vstr{suf} ],
\var{i}, \var{k} \big].
\end{equation}
\end{theorem}

\begin{proof}
\Veim{down} is valid by assumption, so
from
\eqref{eq:null-scan-from-down-cause-5}
we have
\begin{equation}
\label{eq:null-scan-from-down-cause-12}
\Vsym{down} \derives \Vmk{i} \Vstr{prf} \Vmk{j} \Vsym{nul} \Vstr{suf}.
\end{equation}
Moving \Vmk{j} in
\eqref{eq:null-scan-from-down-cause-12}
to the other side of the nulling symbol produces
\begin{equation}
\label{eq:null-scan-from-down-cause-10}
\Vsym{down} \derives \Vmk{i} \Vstr{prf} \Vsym{nul} \Vmk{j} \Vstr{suf},
\end{equation}
\eqref{eq:null-scan-from-down-cause-8}
follows from
\eqref{eq:null-scan-from-down-cause-10},
by EIM validity.
By the definition of top-down cause,
all the effects of
\eqref{eq:null-scan-from-down-cause-5}
must have take the exact form of
\eqref{eq:null-scan-from-down-cause-8},
which shows that it is unique.
\end{proof}

\begin{theorem}
\ttitle{Iterative null-scan from top-down cause}
\label{t:iterative-null-scan-from-down-cause}
Let
\begin{equation}
\label{eq:iterative-null-scan-from-down-cause-5}
\Veim{down} = \big[ [ \Vrule{r}, \var{lo} ], \var{i}, \var{k} \big],
\end{equation}
be a valid EIM.
Let the series \var{ns} be such that
\begin{equation}
\begin{aligned}
\label{eq:iterative-null-scan-from-down-cause-8}
& \el{ns}{0} = \Veim{down} \quad \text{and} \\
& \qquad \forall \; \var{a} : 0 \le \var{a} < \xxsubtract{\var{hi}}{\var{lo}}
\implies \\
& \qquad \qquad \left(
\begin{gathered}
\Vop{RHS}{\Vrule{r}, \var{lo}+\var{a}} = \epsilon \\
\land \quad \el{ns}{\Vincr{a}} =
\big[ [ \Vrule{r}, \var{lo}+\var{a}+1 ], \var{i}, \var{k} \big]
\end{gathered}
\right).
\end{aligned}
\end{equation}
Then
every EIM in \var{ns} is
valid.
Also,
for all \var{a}
such that 
$0 \le \var{a} \le \decr{\Vlastix{ns}}$,
\Vel{ns}{a} is the top-down cause of
\el{ns}{\Vincr{a}};
and \el{ns}{\Vincr{a}}
is the unique effect of \Vel{ns}{a}.
\end{theorem}

\begin{proof}
This theorem follows from Theorem
\ref{t:null-scan-from-down-cause}
by iteration.
\end{proof}

\begin{theorem}
\ttitle{Iterative completion from null-scan cause}
\label{t:iterative-completion-from-null-scan-cause}
Let
\begin{equation}
\label{eq:iterative-completion-from-null-scan-cause-5}
\Veim{down} = \big[ [ \Vrule{r}, \var{lo} ], \var{i}, \var{k} \big],
\end{equation}
be a valid quasi-complete EIM.
Let the series \var{ns} be such that
\begin{equation}
\begin{aligned}
\label{eq:iterative-completion-from-null-scan-cause-8}
& \el{ns}{0} = \Veim{down} \quad \text{and} \\
& \qquad \forall \; \var{a} : 0 \le \var{a} < (\xxsubtract{\Vsize{\Vrule{r}}}{\var{lo}})
\implies \\
& \qquad \qquad
\el{ns}{\Vincr{a}} =
\big[ [ \Vrule{r}, \var{lo}+\var{a}+1 ], \var{i}, \var{k} \big].
\end{aligned}
\end{equation}
Then we have all of the following:
\begin{align}
\label{eq:iterative-completion-from-null-scan-cause-9}
& \text{\var{ns} contains at most \Vsize{\Vrule{r}}
  elements
}
\\
\label{eq:iterative-completion-from-null-scan-cause-10}
& \text{$\var{ns}\big[ \Vlastix{ns} \big]$
  is a completion
}
\\
\label{eq:iterative-completion-from-null-scan-cause-12}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} < \Vlastix{ns} \\
& \quad \implies \text{\Vel{ns}{a} is an incomplete EIM}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-13}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{ns} \\
& \quad \implies \text{\Vel{ns}{a} is an quasi-complete EIM}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-14}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{ns}
  \implies \Valid{\Vel{ns}{a}}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-16}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{ns} \\
& \qquad \implies \Right{\el{ns}{0}} = \Right{\Vel{ns}{a}}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-18}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{ns} \\
& \qquad \implies \op{Rule}{\el{ns}{0}} = \op{Rule}{\Vel{ns}{a}}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-20}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \decr{\Vlastix{ns}} \\
& \qquad \implies \text{\Vel{ns}{a} is the top-down cause of
	\el{ns}{\Vincr{a}}
}
\end{aligned}
\\
\label{eq:iterative-completion-from-null-scan-cause-22}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \decr{\Vlastix{ns}} \\
& \qquad \implies \text{\el{ns}{\Vincr{a}}
	is the unique effect of \Vel{ns}{a}
}
\end{aligned}
\end{align}
\end{theorem}

\begin{proof}

Since
\Veim{down} is, by assumption for the theorem,
quasi-complete,
we know that
\begin{equation}
\label{eq:iterative-completion-from-null-scan-cause-30}
\forall \; \var{a} : 0 \le \var{a} < \xxsubtract{\Vsize{\Vrule{r}}}{\var{lo}}
\implies
\Vop{RHS}{\Vrule{r}, \var{lo}+\var{a}} = \epsilon
\end{equation}
In the requirements for the theorem,
\eqref{eq:iterative-completion-from-null-scan-cause-14},
\eqref{eq:iterative-completion-from-null-scan-cause-20}
and
\eqref{eq:iterative-completion-from-null-scan-cause-22}
follow from
\eqref{eq:iterative-completion-from-null-scan-cause-30}
and Theorem
\ref{t:iterative-null-scan-from-down-cause}.

From
\eqref{eq:iterative-completion-from-null-scan-cause-8},
we can see that
\begin{gather*}
\Vlastix{ns} = \xxsubtract{\Vsize{\Vrule{r}}}{\var{lo}}, \quad 
\text{so that}  \\
\begin{aligned}
\op{Dotix}{\var{ns}\big[\Vlastix{ns}\big]} 
& = \var{lo} + (\decr{( \xxsubtract{\Vsize{\Vrule{r}}}{\var{lo}}) }) + 1 \\
& = \Vsize{\Vrule{r}}
\end{aligned}
\end{gather*}
which shows
\eqref{eq:iterative-completion-from-null-scan-cause-10}.
Simlarly,
we see
from
\eqref{eq:iterative-completion-from-null-scan-cause-8},
\begin{equation*}
\begin{aligned}
&& \var{ix} & < \Vlastix{ns} \\
\implies && \op{Dotix}{\Vel{ns}{ix}}
& < \var{lo} + (\decr{( \xxsubtract{\Vsize{\Vrule{r}}}{\var{lo}}) }) + 1 \\
\implies && \op{Dotix}{\Vel{ns}{ix}}
& < \Vsize{\Vrule{r}}
\end{aligned}
\end{equation*}
shows
\eqref{eq:iterative-completion-from-null-scan-cause-12}.

We see from
\eqref{eq:iterative-completion-from-null-scan-cause-8}
that, for every element of \var{ns},
is an EIM which varies only it its dot index,
which gives us
\eqref{eq:iterative-completion-from-null-scan-cause-16}
and
\eqref{eq:iterative-completion-from-null-scan-cause-18}.
From
\eqref{eq:iterative-completion-from-null-scan-cause-8},
we also see that
\begin{equation}
\var{ix} > 0 \implies \Dotix{\Vel{ns}{ix}} > \Dotix{\el{ns}{0}}
\end{equation}
so that using
\eqref{eq:iterative-completion-from-null-scan-cause-30},
\begin{equation}
\label{eq:iterative-completion-from-null-scan-cause-40}
\begin{gathered}
\forall \; \var{nsix}, \var{rhix} :
0 \le \var{nsix} \le \Vlastix{ns} \\
\land \;
\Dotix{\Vel{ns}{nsix}} \le \var{rhix} \le \Vsize{\Vrule{r}} \\
\implies
\Vop{RHS}{\Vrule{r}, \var{rhix}} = \epsilon
\end{gathered}
\end{equation}
and since every layer of \var{ns} shares the same rule,
\eqref{eq:iterative-completion-from-null-scan-cause-40}
shows
\eqref{eq:iterative-completion-from-null-scan-cause-13}.

From \eqref{eq:iterative-completion-from-null-scan-cause-8},
we know that every layer of \var{ns} differs in its dot index.
As we already noted, the layer of \var{ns} differs
only in their dot index.
There are at most
distinct dot \incr{\Vsize{\Vrule{r}}} indexes.
By 
\eqref{eq:iterative-completion-from-null-scan-cause-13},
every layer is quasi-complete,
so that no layer is a prediction,
and therefore no layer uses dot index 0.
So that there are at most
\Vsize{\Vrule{r}} layers in \var{ns},
which shows
\eqref{eq:iterative-completion-from-null-scan-cause-9}.
\end{proof}

\begin{theorem}
\ttitle{Causes of confirmed effect}
\label{t:symbolic-causes-from-effect}
If
\[\Veim{effect} = [ \Vdr{effect}, \var{i}, \var{k} ],\]
is a valid, confirmed EIM,
then
\begin{enumerate}
\RaggedRight
\item
\label{item:symbolic-causes-from-effect-3}
\Veim{effect} must have a valid symbolic bottom-up cause,
call it $\Vinst{up}$,
\item
\label{item:symbolic-causes-from-effect-6}
\Veim{effect} must have a valid top-down cause,
call it $\Veim{down}$,
\item
\label{item:symbolic-causes-from-effect-9}
\Vinst{up} and \Veim{down} must be matching causes,
\item
\label{item:symbolic-causes-from-effect-12}
$\Right{\Vinst{up}}  = \Vloc{k}$,
\item
\label{item:symbolic-causes-from-effect-15}
$\Left{\Veim{down}} = \Vloc{i}$,
\item
\label{item:symbolic-causes-from-effect-18}
$\Symbol{\Vinst{up}} = \Predot{\Veim{effect}}$,
\item
\label{item:symbolic-causes-from-effect-19}
$\Predot{\Veim{effect}} = \Postdot{\Veim{down}}$, and
\item
\label{item:symbolic-causes-from-effect-21}
$\Next{\Vdr{down}} = \Vdr{effect}$
\end{enumerate}
\end{theorem}

\begin{proof}
Without loss of generality,
let \Veim{effect} be
\begin{equation}
\label{eq:symbolic-causes-from-effect-20}
\Veim{effect} =
\big[ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k}
\big].
\end{equation}
so that by
Definition \ref{def:causes},
the definition of causes,
the bottom-up cause is
\begin{equation}
\label{eq:symbolic-causes-from-effect-25}
\Vinst{up} = \Vmkl{j} \Vstr{A} \Vmkr{k},
\end{equation}
and the top-down cause is
\begin{equation}
\label{eq:symbolic-causes-from-effect-30}
\Veim{down} = [ [ \Vsym{down} \derives \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j} ].
\end{equation}

Trivially, we can see that
\eqref{eq:symbolic-causes-from-effect-20},
\eqref{eq:symbolic-causes-from-effect-25},
and
\eqref{eq:symbolic-causes-from-effect-30}
satisfy
requirements \ref{item:symbolic-causes-from-effect-9}
\ref{item:symbolic-causes-from-effect-12},
\ref{item:symbolic-causes-from-effect-15},
\ref{item:symbolic-causes-from-effect-18},
\ref{item:symbolic-causes-from-effect-19}
and
\ref{item:symbolic-causes-from-effect-21}
in the statement of the theorem.
It remains to show that
\eqref{eq:symbolic-causes-from-effect-25},
and
\eqref{eq:symbolic-causes-from-effect-30}
are valid.

Using
\eqref{eq:symbolic-causes-from-effect-20},
and the definition of location markers,
we have
\begin{equation}
\label{eq:symbolic-causes-from-effect-29}
\Vstr{pre} \cat \Vstr{A} \destar \var{w}[\var{i}, (\var{k} \subtract 1)],
\end{equation}
so that there must be some \Vloc{j}, $\var{i} \le \var{j} \le \var{k}$,
such that
\begin{equation}
\label{eq:symbolic-causes-from-effect-32}
\Vsym{down} \derives \Vmkl{i} \Vstr{pre} \Vmkm{j} \Vstr{A} \Vmkm{k} \Vstr{post}.
\end{equation}
Simplifying
\eqref{eq:symbolic-causes-from-effect-32},
we have
\eqref{eq:symbolic-causes-from-effect-25},
our bottom-up cause.
This shows that \Vinst{up} is valid
which was requirement \ref{item:symbolic-causes-from-effect-3}
in the statement of this theorem.

A different simplification of
\eqref{eq:symbolic-causes-from-effect-32}
produces
\begin{equation}
\label{eq:symbolic-causes-from-effect-38}
\Vsym{down} \derives \Vmkl{i} \Vstr{pre} \Vmkm{j} \Vstr{A} \cat \Vstr{post}.
\end{equation}
and
from
\eqref{eq:symbolic-causes-from-effect-38},
by the definition of EIM validity,
we have
\eqref{eq:symbolic-causes-from-effect-30}.
This shows that \Veim{down} is valid
which was
requirement \ref{item:symbolic-causes-from-effect-6},
the only remaining requirement
in the statement of this theorem.
\end{proof}

\begin{theorem}
\ttitle{Top-down cause of confirmed effect}
\label{t:down-cause-from-effect}
Let
\begin{equation*}
\Veim{effect} = [ \Vdr{effect}, \var{i}, \var{k} ]
\end{equation*}
be a valid, confirmed EIM,
then \Veim{down} is a valid, top-down cause,
such that
\begin{align}
\label{eq:down-cause-from-effect-10}
\Veim{down} & = \big[ \Prev{\Vdr{effect}}, \var{i}, \var{k} ] \\
& \qquad \text{if \Veim{effect} is a null-scan EIM,} \notag\\
\label{eq:down-cause-from-effect-13}
\Veim{down} & = \big[ \Prev{\Vdr{effect}}, \var{i}, \Vdecr{k} ] \\
& \qquad \text{if \Veim{effect} is a read EIM, and} \notag\\
\label{eq:down-cause-from-effect-16}
\Veim{down} & = \big[ \Prev{\Vdr{effect}}, \var{i}, \var{j} ] \\
& \qquad \text{where $\var{j} < \var{k}$, if \Veim{effect} is a telluric EIM.} \notag
\end{align}
\end{theorem}

\begin{proof}
The proof follows from
Theorem \ref{t:symbolic-causes-from-effect},
noting in particular the requirement that
\Veim{effect} must have
a top-down cause, \Veim{down},
and \Vinst{up},
and that they must match.
By the definition of matching causes,
\begin{equation}
\label{eq:down-cause-from-effect-20}
\Right{\Veim{down}} = \Left{\Vinst{up}}.
\end{equation}
By the definition of left and right location,
\begin{equation}
\label{eq:down-cause-from-effect-23}
\Right{\Vinst{up}} = \Left{\Vinst{up}} + \Vsize{\Vinst{up}}.
\end{equation}
Using
Theorem \ref{t:symbolic-causes-from-effect},
\begin{equation}
\label{eq:down-cause-from-effect-26}
\Right{\Veim{up}} = \Right{\Veim{effect}} = \Vloc{k}.
\end{equation}

Combining
\eqref{eq:down-cause-from-effect-20},
\eqref{eq:down-cause-from-effect-23} and
\eqref{eq:down-cause-from-effect-26},
we have
\begin{equation}
\label{eq:down-cause-from-effect-29}
\begin{aligned}
\Right{\Veim{down}} & = \Right{\Veim{effect}} \subtract \Vsize{\Vinst{up}} \\
& = \Vloc{k} \subtract \Vsize{\Vinst{up}}.
\end{aligned}
\end{equation}

If \Veim{effect} is a null-scan, then
\Vinst{up} is a nulling symbol instance,
and
$\Vsize{\Vinst{up}} = 0$,
so that we have
\eqref{eq:down-cause-from-effect-10}.

If \Veim{effect} is a null-scan, then
\Vinst{up} is a non-nulling terminal symbol instance,
and
$\Vsize{\Vinst{up}} = 1$,
so that we have
\eqref{eq:down-cause-from-effect-13}.

If \Veim{effect} is telluric, then
\Vinst{up} is a telluric symbol instance,
and
$\Vsize{\Vinst{up}} > 1$,
so that we have
\eqref{eq:down-cause-from-effect-16}.
\end{proof}

\begin{sloppypar}
\begin{FlushLeft}
\begin{theorem}
\ttitle{Effect from symbol instance}
\label{t:effect-from-symbol}
Let
\begin{equation}
\var{A-inst} = \Vmkm{j} \Vsym{A} \Vmkm{k}
\end{equation}
be a valid symbol instance.
If $\Vsym{A} \neq \Vsym{accept}$
then
\begin{enumerate}
\item
\label{req:effect-from-symbol-1}
\var{A-inst} is the bottom-up cause of a valid effect, call
it \Veim{effect}
\item
\label{req:effect-from-symbol-2}
\var{A-inst} has a matching valid top-down cause, call it
\Veim{down},
\item
\label{req:effect-from-symbol-3}
\Vloc{j} is such that for some \Vloc{i}, \Vdr{down},
\[\Veim{down} = [\Vdr{down}, \Vloc{i}, \Vloc{j}] \quad \text{and}\]
\item
\label{req:effect-from-symbol-4}
$\Vsym{A} = \Postdot{\Veim{down}} = \Predot{\Veim{effect}}.$
\end{enumerate}
\end{theorem}

\end{FlushLeft}
\end{sloppypar}

\begin{proof}
Without loss of generality,
we let the symbolic equivalent of the bottom-up cause be
\begin{equation}
\label{eq:effect-from-symbol-6}
\Vmkl{j} \Vsym{A} \Vmkr{k}.
\end{equation}
By assumption for the theorem,
$\Vsym{A} \neq \Vsym{accept}$.
Therefore,
there is at least one derivation step before
\eqref{eq:effect-from-symbol-6},
so that, without loss of generality, we may write
\begin{equation}
\label{eq:effect-from-symbol-9}
\Vsym{down} \destar \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post}.
\end{equation}
If we let \Vloc{i} = \Right{\Vstr{pre}},
and introduce a location marker for it into
\eqref{eq:effect-from-symbol-9},
we have
\begin{equation}
\label{eq:effect-from-symbol-25}
\Vsym{down} \derives \Vmkm{i} \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post}.
\end{equation}
From
\eqref{eq:effect-from-symbol-25},
and the definition of validity for an Earley item,
we know that
\begin{equation}
\label{eq:effect-from-symbol-28}
\Veim{down} =
[ [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j} ] \\
\end{equation}
and
\begin{equation}
\label{eq:effect-from-symbol-29}
\Veim{effect} =
[ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ]
\end{equation}
are valid.

\eqref{eq:effect-from-symbol-29}
and Definition \ref{def:causes},
the definition of causes,
shows requirement
\ref{req:effect-from-symbol-1} of this theorem.
\eqref{eq:effect-from-symbol-28}
and Definition \ref{def:causes},
the definition of causes,
shows requirement
\ref{req:effect-from-symbol-2} of this theorem.
\eqref{eq:effect-from-symbol-28} shows
requirement \ref{req:effect-from-symbol-3}.
And requirement \ref{req:effect-from-symbol-4}
follows from
\eqref{eq:effect-from-symbol-28}
and
\eqref{eq:effect-from-symbol-29}.
\end{proof}

\begin{theorem}
\ttitle{Terminal bottom-up causes are unique}
\label{t:terminal-cause-unique}
Let an instance of the terminal
\Vsym{term} be the bottom-up cause of an effect,
\Veim{effect}.
Then that instance is the only
bottom-up cause of \Veim{effect}.
\end{theorem}

\begin{proof}
Since \Vsym{term} is a terminal
it cannot appear on the LHS of a rule.
Since \Vsym{term} cannot be the LHS of a rule,
by Theorem
\ref{t:eim-equivalent-from-non-terminal},
\Vinst{inst} has no EIM equivalent.
Let
\begin{equation}
\Vinst{inst1} = \Vmkm{l1} \Vsym{term} \Vmkm{r1}
\end{equation}
Assume for a reductio that
\Vinst{inst2} is another
symbolic bottom-up cause of \Veim{effect},
\begin{gather}
\Vinst{inst2} = \Vmkm{l2} \Vsym{sym2} \Vmkm{r2} \\
\label{eq:terminal-cause-unique-5}
\text{where $\var{l1} \neq \var{l2}$ or $\Vsym{term} \neq \Vsym{sym2}$ or $\var{r1} \neq \var{r2}.$}
\end{gather}

But by Theorem \ref{t:symbolic-causes-from-effect},
the bottom-up cause symbol of \Veim{effect} must be
\Predot{\Veim{effect}},
so that
\begin{equation}
\label{eq:terminal-cause-unique-10}
\Vsym{term} = \Predot{\Veim{effect}} = \Vsym{sym2}.
\end{equation}

Also by Theorem \ref{t:symbolic-causes-from-effect},
the right location of its bottom-up cause must be
\Right{\Veim{effect}}, so that
\begin{equation}
\label{eq:terminal-cause-unique-20}
\var{r1} = \Right{\Veim{effect}} = \var{r2}.
\end{equation}

Finally,
if \Vsym{term} is terminal,
it must be telluric in which case its length is 1,
or nulling, in which case its length is 0.
In either case its length is fixed.
Let $\Vsize{\Vsym{term}} = \var{len}$.
Then
\begin{equation}
\label{eq:terminal-cause-unique-30}
\var{l1} \; = \; \left(\Right{\Veim{effect}} \subtract \var{len}\right) \; = \; \var{l2}.
\end{equation}

Gathering
\eqref{eq:terminal-cause-unique-10},
\eqref{eq:terminal-cause-unique-20}
and
\eqref{eq:terminal-cause-unique-30},
we see that
\eqref{eq:terminal-cause-unique-5} is false,
which is contrary to the assumption for the reductio.
So \Vinst{inst1} must be unique.
Our choice of \Vinst{inst1} was without loss of generality,
so this shows the theorem.
\end{proof}

\section{Predictions}

\begin{theorem}
\ttitle{Prediction from top-down cause}
\label{t:prediction-from-cause}
Let
\begin{equation}
\label{eq:prediction-from-cause-3}
\Veim{down} = \big[
  [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j}
\big]
\end{equation}
be a valid EIM,
and let
\begin{equation}
\label{eq:prediction-from-cause-6}
[ \Vsym{A} \de \Vstr{A-rhs} ]
\end{equation}
be a rule in \Cg{}.
Then
\begin{equation}
\label{eq:prediction-from-cause-9}
\Veim{prediction} = \big[ [ \Vsym{A} \de \mydot \Vstr{A-rhs} ], \var{j}, \var{j}
\big]
\end{equation}
is valid.
\end{theorem}

\begin{proof}
By assumption,
\eqref{eq:prediction-from-cause-3}
is valid.
From the definition of validity for an Earley item:
\begin{equation}
\label{eq:prediction-from-cause-20}
\Vsym{down} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \cat \Vstr{post}
\end{equation}
Using
\eqref{eq:prediction-from-cause-6}
and
\eqref{eq:prediction-from-cause-20},
and expanding \Vsym{A} into its
direct descendants,
we have
\begin{equation}
\label{eq:prediction-from-cause-25}
\begin{split}
\Vsym{down} \; & \derives \; \Vmk{i} \Vstr{pre} \Vmk{j} \Vsym{A} \Vstr{post} \\
    & \derives \; \Vmk{i} \Vstr{pre} \Vmk{j} \Vsym{A-rhs} \Vstr{post}.
\end{split}
\end{equation}

Simplifying
\eqref{eq:prediction-from-cause-25},
we have
\begin{equation}
\label{eq:prediction-from-cause-12}
\Vsym{A} \derives [\var{j}] \Vstr{A-rhs}.
\end{equation}
By the definition of validity for an Earley item,
this shows
\eqref{eq:prediction-from-cause-9},
and therefore the theorem.
\end{proof}

\begin{theorem}
\ttitle{Top-down cause from prediction}
\label{t:cause-from-prediction}
Let
\begin{equation}
\label{eq:cause-from-prediction-1}
\Veim{prediction} = \big[ [ \Vsym{A} \de \mydot \Vstr{A-rhs} ], \var{j}, \var{j}
\big]
\end{equation}
be a predicted EIM.
Then there is a rule,
\begin{equation}
\label{eq:cause-from-prediction-2}
  [ \Vsym{down} \de \Vstr{pre} \Vsym{A} \Vstr{post} ] \in \Crules
\end{equation}
and some \Vloc{i},
some \Vstr{pre}, and
some \Vstr{post}
such that
\begin{equation}
\label{eq:cause-from-prediction-3}
\Veim{down} = \big[
  [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j}
\big]
\end{equation}
is valid.
\end{theorem}

\begin{proof}
Since
\eqref{eq:cause-from-prediction-1} is valid,
we have
\begin{equation}
\label{eq:cause-from-prediction-12}
\Vsym{A} \derives [\var{j}] \Vstr{A-rhs}.
\end{equation}
All symbols in Marpa grammars are accessible
and,
because
\eqref{eq:cause-from-prediction-1} is a prediction,
and therefore not the start EIM,
\Vsym{A} is not \Vsym{accept}.
So we can expand
\eqref{eq:cause-from-prediction-12}
to
\begin{equation}
\label{eq:cause-from-prediction-25}
\begin{split}
\Vsym{accept} \; & \destar \; \Vstr{pre2} \Vsym{down} \Vstr{post2} \\
    & \derives \; \Vstr{pre2} \Vstr{pre} \Vmk{j} \Vsym{A} \Vstr{post} \Vstr{post2} \\
    & \derives \; \Vstr{pre2} \Vstr{pre} \Vmk{j} \Vsym{A-rhs} \Vstr{post} \Vstr{post2}.
\end{split}
\end{equation}
which simplifies to
\begin{equation}
\label{eq:cause-from-prediction-30}
\Vsym{down} \derives \Vmk{i} \Vstr{pre} \Vmk{j} \Vsym{A} \Vstr{post},
\end{equation}
into which we have introduced a location
marker for \Vloc{i}.
From \eqref{eq:cause-from-prediction-30},
for the definition of EIM validity,
we have
\eqref{eq:cause-from-prediction-3}.
\eqref{eq:cause-from-prediction-3},
in turn,
shows
\eqref{eq:cause-from-prediction-2}
and the theorem.
\end{proof}

\section{Ethereal closures}

In the following definitions,
$\langle \Veim{down}, \Veim{effect} \rangle$
is a duple,
where \Veim{down} is a top-down cause
of \Veim{effect}:
\begin{align}
\var{predict-op} \; & \defined \left\lbrace \;
  \langle \var{down}, \var{effect} \rangle
  \; \middle| \;
  \text{\var{effect} is a prediction}
\; \right\rbrace ,
\notag \\
\var{null-scan-op} \; & \defined \left\lbrace \;
  \langle \var{down}, \var{effect} \rangle
  \; \middle| \;
  \text{\var{effect} is a null-scan}
\; \right\rbrace ,
\notag \\
\var{read-op} \; & \defined \left\lbrace \;
  \langle \var{down}, \var{effect} \rangle
  \; \middle| \;
  \text{\var{effect} is a read}
\; \right\rbrace ,
\notag \\
\var{reduction-op} \; & \defined \left\lbrace \;
  \langle \var{down}, \var{effect} \rangle
  \; \middle| \;
  \text{\var{effect} is a reduction}
\; \right\rbrace ,
\notag \\
\label{eq:def-epsilon-op}
\var{epsilon-op} \; & \defined \var{predict-op} \cup \var{null-scan-op}
, \; \text{and} \\
\var{ethereal-closure} \; & \defined \var{epsilon-op}^\ast.
\notag
\end{align}

\begin{sloppypar}
\begin{FlushLeft}
\begin{theorem}
\ttitle{Ethereal closure properties}
\label{t:ethereal-closure}
Let \Veim{base} be a valid
EIM such that
\begin{equation}
\Veim{desc} \in \var{ethereal-closure}(\Veim{base}).
\end{equation}
Then
\begin{enumerate}
\item
\label{req:ethereal-closure-3}
$\Valid{\Veim{desc}}$
\item
\label{req:ethereal-closure-5}
$\DR{\Veim{desc}} \in \var{ethereal-dr-closure}(\DR{\Veim{base}})$,
\item
\label{req:ethereal-closure-8}
$\Current{\Veim{desc}} = \Current{\Veim{base}},$
\item
\label{req:ethereal-closure-12}
if \Veim{desc} is a quasi-prediction,
$\Origin{\Veim{desc}} = \Current{\Veim{base}}$,
and
\item
\label{req:ethereal-closure-15}
if \Veim{desc} is quasi-confirmed,
then
$\Origin{\Veim{desc}} = \Origin{\Veim{base}}.$
\end{enumerate}
\end{theorem}
\end{FlushLeft}
\end{sloppypar}

\begin{proof}
Our proof will be by induction on the iterations
of
\var{epsilon-op}.
Let
\begin{equation}
\var{eims}[\var{i}] = \var{epsilon-op}^{\displaystyle \var{i}}(\Veim{base}).
\end{equation}

We take as our induction hypothesis:
\begin{equation}
\label{eq:ethereal-closure-18}
\myparbox{
The EIM's in $\var{eims}[\var{x}]$
obey requirements
\eqref{req:ethereal-closure-3},
\ref{req:ethereal-closure-5},
\ref{req:ethereal-closure-8},
\ref{req:ethereal-closure-12}
and
\ref{req:ethereal-closure-15}
in the statement of the theorem.
}
\end{equation}

When we apply dotted rule notions
and functions to
an EIM, we refer to its dotted rule.
By the definition of a quasi-prediction
and of \myfnname{Dotix},
\begin{equation}
\label{eq:ethereal-closure-20}
\begin{gathered}
\text{if \Veim{qpred} is a quasi-prediction, then} \\
\var{i} < \op{Dotix}{\Veim{qpred}} \implies \RHS{\Veim{dr}, \var{i}} \derives \epsilon \quad \text{and} \\
\text{\Current{\Veim{qpred}} = \Origin{\Veim{qpred}}}.
\end{gathered}
\end{equation}

\textbf{Basis}:
As the basis of the induction, we show
\eqref{eq:ethereal-closure-18}
for $\var{x} = 0$.
By the definition of an ethereal closure,
$\var{eims}[0] = \lbrace \Veim{base} \rbrace.$
By assumption for the theorem, \Veim{base} is valid.
This shows requirement~\ref{req:ethereal-closure-3}.
Trivially, \DR{\Veim{base}} is in its own ethereal closure.
This shows requirement \ref{req:ethereal-closure-5}.
Also trivially, its current location is self-identical,
which shows requirement \ref{req:ethereal-closure-8}.

If \Veim{base} is a quasi-prediction,
then
we have
requirement \ref{req:ethereal-closure-12},
by self-identity and
\eqref{eq:ethereal-closure-20}.
If \Veim{base} is quasi-confirmed, we
have
requirement \ref{req:ethereal-closure-12}
vacuously.

If \Veim{base} is quasi-prediction,
then
we have
requirement \ref{req:ethereal-closure-15}
vacuously.
If \Veim{base} is quasi-confirmed, we
have
requirement \ref{req:ethereal-closure-15}
by self-identity.
This shows all the requirements for the basis of the induction.

\textbf{Step}:
For the step of the induction, we assume
\eqref{eq:ethereal-closure-18} for $\var{x} = \var{i}$,
to show
\eqref{eq:ethereal-closure-18} for $\var{x} = \Vincr{i}$.
We consider, without loss of generality,
just one of the dotted rules in $\var{eims}[\var{i}+1]$.
Call it \Veim{wlog2}.
By assumption for the step,
\Veim{wlog2} is not in $\var{eims}[0]$,
so that it has a top-down
cause in
$\var{eims}[\var{i}]$.
Call this top-down cause, \Veim{wlog1}.

We show requirement \ref{req:ethereal-closure-3}
by cases,
depending on the type of \Veim{wlog2}.
By definition of \var{ethereal-closure} for EIM's,
\Veim{wlog2} is a null-scan or a prediction.
Theorem \ref{t:prediction-from-cause}
shows that the
validity of
top-down cause \Veim{wlog1}
is sufficient to make
\Veim{wlog2} valid,
if \Veim{wlog2} is a prediction.
Theorem \ref{t:null-scan-from-down-cause}
shows that the
validity of
top-down cause \Veim{wlog1}
is sufficient to make
\Veim{wlog2} valid,
if \Veim{wlog2} is a null-scan.
We have both cases
and therefore have shown
requirement \ref{req:ethereal-closure-3}
for the step.

The EIM types are defined in terms of the types of their
dotted rules,
so that
\begin{gather}
\text{if $\langle \Veim{wlog1}, \Veim{wlog2} \rangle \in \var{epsilon-op},$} \\
\label{eq:ethereal-closure-27}
\text{then} \quad \big\langle \DR{\Veim{wlog1}}, \DR{\Veim{wlog2}} \big\rangle \in \var{epsilon-dr-op}.
\end{gather}
By the definition of the ethereal closure for dotted rules,
\begin{gather}
\label{eq:ethereal-closure-30}
\var{ethereal-dr-closure} = \var{epsilon-dr-op}^\ast
\end{gather}
and by assumption for the step
\begin{equation}
\label{eq:ethereal-closure-33}
\DR{\Veim{wlog1}} \in \var{ethereal-dr-closure}(\DR{\Veim{base}})
\end{equation}
Using
\eqref{eq:ethereal-closure-27},
\eqref{eq:ethereal-closure-30}
and
\eqref{eq:ethereal-closure-33},
we have
\begin{equation*}
\DR{\Veim{wlog2}} \in \var{ethereal-dr-closure}(\DR{\Veim{base}}),
\end{equation*}
which is requirement \ref{req:ethereal-closure-5}
for the step.

If \Veim{wlog2} is a prediction,
we know from Theorem \ref{t:prediction-from-cause}
that
\begin{equation}
\label{eq:ethereal-closure-36}
\Current{\Veim{wlog1}} = \Current{\Veim{wlog2}}
\end{equation}

If \Veim{wlog2} is a null-scan,
we know from Theorem \ref{t:right-location-of-top-down-cause}
that
\begin{align}
\Right{\Veim{wlog1}} & = \Right{\Veim{wlog2}}
\intertext{and since the right location of an EIM is also its current location,}
\label{eq:ethereal-closure-39}
\Current{\Veim{wlog1}} & = \Current{\Veim{wlog2}}
\end{align}
Also, for null-scans, we know from
Theorem
\ref{t:effect-from-symbolic-causes}
that
\begin{equation}
\label{eq:ethereal-closure-42}
\Origin{\Veim{wlog1}} = \Origin{\Veim{wlog2}}.
\end{equation}

If \Veim{wlog2} is a quasi-prediction, we know from
\eqref{eq:ethereal-closure-20} that
\begin{equation}
\label{eq:ethereal-closure-45}
\Current{\Veim{wlog2}} = \Origin{\Veim{wlog2}}.
\end{equation}

We have
\begin{equation}
\label{eq:ethereal-closure-48}
\Current{\Veim{wlog1}} = \Current{\Veim{wlog2}}
\end{equation}
from
\eqref{eq:ethereal-closure-36}
for predictions and from
\eqref{eq:ethereal-closure-39}
for null-scans.
Either way, we have
requirement
\ref{req:ethereal-closure-8} for
the step of the induction.

We have
requirement \ref{req:ethereal-closure-12}
from
\eqref{eq:ethereal-closure-20} and
\eqref{eq:ethereal-closure-48}
for quasi-predictions;
and vacuously for quasi-confirmed EIM's.

We have
requirement \ref{req:ethereal-closure-15}
from \eqref{eq:ethereal-closure-42} for
quasi-confirmed EIM's,
and vacuously for quasi-predictions.

This shows all four requirements and,
since the choice of \Veim{wlog2} was
without loss of generality,
we have shown
\eqref{eq:ethereal-closure-18} for $\var{eims}[\var{i}+1]$.
With this,
we have shown the step of the induction,
the induction,
and the theorem.
\end{proof}

\section{Ambiguity}

\begin{theorem}
\ttitle{Multiple top-down causes make a grammar ambiguous}
\label{t:multi-down-cause-ambiguous}
If any Earley item has more than one top-down cause,
the grammar is ambiguous.
\end{theorem}

\begin{proof}
Assume for a reductio, that \Veim{effect} has more than one top-down cause
but that \Cg{} is not ambiguous.
Without loss of generality,
let \Veim{effect} be
\begin{equation}
\label{eq:multi-down-cause-ambiguous-20}
\Veim{effect} =
[ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ].
\end{equation}
By the Definition \ref{def:causes},
the dotted rule and origin of a top-down cause are determined by its effect,
so that the two down-causes can differ only in their current position.
Let the two top-down causes be
\begin{gather*}
\Veim{down1} = [ [ \Vsym{down} \derives \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j1} ] \\
\Veim{down2} = [ [ \Vsym{down} \derives \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j2} ]
\end{gather*}
From these we get the two derivations:
\begin{gather*}
\Vsym{down} \derives \Vmk{i} \Vstr{pre} \Vmk{j1} \Vsym{A} \Vstr{post} \\
\Vsym{down} \derives \Vmk{i} \Vstr{pre} \Vmk{j2} \Vsym{A} \Vstr{post}
\end{gather*}
These will be different factorings of the input if $\var{j1} \neq \var{j2}$.
By assumption for the reductio, \Cg{} is not ambiguous,
and therefore no derivation step can allow two different factorings of the input.
So $\var{j1} = \var{j2}$.
But then $\Veim{down1} = \Veim{down2}$, which is contrary to assumption
for the reductio.
\end{proof}

\begin{theorem}
\ttitle{Multiple bottom-up causes make a grammar ambiguous}
\label{t:multi-up-cause-ambiguous}
If any Earley item has more than one bottom-up cause,
the grammar is ambiguous.
\end{theorem}

\begin{proof}
Assume for a reductio, that \Veim{effect} has more than one bottom-up cause
but that \Cg{} is not ambiguous.
Without loss of generality,
let \Veim{effect} be
\begin{equation}
\label{eq:multi-up-cause-ambiguous-20}
\Veim{effect} =
\big[ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k}
\big]
\end{equation}
and call the two differing up-causes, \Vinst{up1}
and \Vinst{up2}.
By Definition \ref{def:causes},
the symbol and right location of a bottom-up cause are determined by its effect,
so that
\begin{gather}
\label{eq:multi-up-cause-ambiguous-25}
\Right{\var{up1}} = \Right{\var{up2}} \\
\label{eq:multi-up-cause-ambiguous-26}
\Symbol{\var{up1}} = \Symbol{\var{up2}}
\end{gather}
Therefore
\Vinst{up1} and \Vinst{up2}
can differ only in their left location, or in their EIM equivalent.

By Theorem \ref{t:symbolic-causes-from-effect},
up-causes must have matching down causes.
By the definition of matching causes,
if two up-causes differ in their left location,
their matching down causes will differ in their right location.
But by Theorem
\ref{t:multi-down-cause-ambiguous},
if an Earley item has more than one top-down cause,
\Cg{} is ambiguous.
This is contrary to assumption for the reductio.
So we know that
\begin{equation}
\label{eq:multi-up-cause-ambiguous-28}
\Left{\var{up1}} = \Left{\var{up2}}
\end{equation}

Since
$\Symbol{\var{up1}} = \Symbol{\var{up2}}$,
and every symbol is either a terminal or a non-terminal,
two up-causes cannot differ in whether they have an EIM equivalent.
Theorem
\ref{t:eim-equivalent-from-non-terminal}
shows that an parse instance has an EIM equivalent
if and only if its symbol is a non-terminal,
so we know that
if \var{up1} has no EIM equivalent,
then \var{up2} has no EIM equivalent.
But if both
\var{up1} and
\var{up2} have no EIM equivalent,
they consist entirely of their parse symbol instance,
so that by
\eqref{eq:multi-up-cause-ambiguous-25},
\eqref{eq:multi-up-cause-ambiguous-26}
and
\eqref{eq:multi-up-cause-ambiguous-28},
they are identical.

It remains to examine the case where up-causes differ in their EIM equivalent.
By
\eqref{eq:multi-up-cause-ambiguous-25}
and \eqref{eq:multi-up-cause-ambiguous-26}
the symbol and right location of the two up-causes must be the same.
By Definition \ref{def:causes},
an EIM bottom-up cause must be a completion.
By \eqref{eq:multi-up-cause-ambiguous-28}
their left locations must be identical.
So the two up-causes can differ only in the RHS of their dotted rule.

\begin{sloppypar}
Without loss of generalization,
let the two differing up-causes be
\begin{gather*}
\Veim{up1} = \big[ [ \Vsym{up} \derives \Vstr{up-rhs1} \mydot ], \var{j}, \var{k}
\big] \\
\Veim{up2} = \big[ [ \Vsym{up} \derives \Vstr{up-rhs2} \mydot ], \var{j}, \var{k}
\big]
\end{gather*}
From these we get the two derivations:
\begin{gather*}
\Vsym{up1} \derives \Vmk{j} \Vstr{up-rhs1} \Vmk{k} \\
\Vsym{up1} \derives \Vmk{j} \Vstr{up-rhs2} \Vmk{k}
\end{gather*}
These will be different derivations of the input if
$\Vstr{up-rhs1} \neq \Vstr{up-rhs2}.$
By assumption for the reductio, \Cg{} is not ambiguous,
and therefore no derivation step can allow two different sentential forms.
So $\Vstr{up-rhs1} = \Vstr{up-rhs2}.$
But then $\Veim{up1} = \Veim{up2}$, which is contrary to assumption
for the reductio.\qedhere
\end{sloppypar}
\end{proof}

\section{Fleeting and lasting}

Recall that dotted rule notions applied to EIM's
refer to the dotted rule of the EIM.
Therefore, a fleeting EIM is an EIM
with a nulling postdot symbol;
and a lasting EIM is any EIM which is not
a fleeting EIM.

\begin{definition}
\dtitle{Lasting equivalent}
\label{def:lasting-equivalent-eim}
Let
\begin{equation*}
\var{eim1} = [ \Vdr{dr1}, \Vorig{i}, \Vloc{j} ].
\end{equation*}
Let \Vdr{lasting1} be the lasting equivalent of \Vdr{dr1}.
Then the lasting equivalent of \var{eim1}
is
\begin{equation*}
[ \Vdr{lasting1}, \Vorig{i}, \Vloc{j} ].
\end{equation*}
We say that the \dfn{lasting effect} of a cause
is the lasting equivalent of the effect of that cause.
\end{definition}

\begin{theorem}
\ttitle{Valid EIMs have valid lasting equivalents}
\label{t:lasting-valid-if-fleeting-valid}
If an EIM is valid, so is its lasting equivalent.
\end{theorem}

\begin{proof}
Without loss of generality,
let the EIM be
\begin{equation}
\label{eq:lasting-valid-if-fleeting-valid-10}
\Veim{fleet} =
[ [ \Vsym{fleet} \de \Vstr{pre} \mydot \Vstr{nulls} \Vstr{post} ], \var{i}, \var{k} ]
\end{equation}
By \eqref{eq:lasting-valid-if-fleeting-valid-10}
and the definition of EIM validity,
\begin{multline}
\label{eq:lasting-valid-if-fleeting-valid-20}
\Vsym{fleet} \derives \Vmk{i} \Vstr{pre} \Vmk{k} \Vstr{nulls} \Vstr{post} \\
\shoveleft{\qquad \text{where $\Vstr{nulls} \derives \epsilon$.}}
\end{multline}
From
\eqref{eq:lasting-valid-if-fleeting-valid-20}, we know that
\begin{equation*}
\Vsym{fleet} \derives \Vmk{i} \Vstr{pre} \Vmk{k} \Vstr{nulls} \Vmk{k} \Vstr{post}
\end{equation*}
so that
using the definition of EIM validity in the opposite direction,
we know that
\begin{equation}
\label{eq:lasting-valid-if-fleeting-valid-25}
\Veim{lasting} =
[ [ \Vsym{fleet} \de \Vstr{pre} \Vstr{nulls} \mydot \Vstr{post} ], \var{i}, \var{k} ]
\end{equation}
is valid.
By the definition of lasting equivalent EIM's,
Definition \ref{def:lasting-equivalent-eim},
\eqref{eq:lasting-valid-if-fleeting-valid-25} is
the lasting equivalent of
\eqref{eq:lasting-valid-if-fleeting-valid-10}.
\end{proof}

\chapter{Tethers}
\label{ch:tethers}

\begin{theorem}
\ttitle{Right location of top-down cause}
\label{t:right-location-of-top-down-cause}
Let \Veim{effect} be a valid EIM other than
the start EIM.
Then it has a top down cause,
that cause is an EIM, call it \Veim{down},
and
\begin{align}
\label{req:right-location-of-top-down-cause-3}
& \rlap{$\Right{\Veim{down}} = \Right{\Veim{effect}}$} & \\
& \qquad \qquad \text{if \Veim{effect} is a prediction,} & \notag \\
\label{req:right-location-of-top-down-cause-4}
& \rlap{$\Right{\Veim{down}} = \Right{\Veim{effect}}$} & \\
& \qquad \qquad \text{if \Veim{effect} is a null-scan,} & \notag\\
\label{req:right-location-of-top-down-cause-6}
& \rlap{$\Right{\Veim{down}} = \decr{\Right{\Veim{effect}}}$} & \\
& \qquad \qquad \text{if \Veim{effect} is read.} & \notag\\
\label{req:right-location-of-top-down-cause-9}
& \rlap{$\Right{\Veim{down}} < \Right{\Veim{effect}}$} & \\
& \qquad \qquad \text{if \Veim{effect} is a reduction.} & \notag
\end{align}
\end{theorem}

\begin{proof}
If \Veim{effect} is a prediction,
this theorem follows from
Theorem \ref{t:prediction-from-cause}.
This shows requirement
\ref{req:right-location-of-top-down-cause-3}.

For the remaining cases,
we will
use Theorem \ref{t:symbolic-causes-from-effect}
to show that \Veim{down} is the matching
cause of a symbolic instance.
Call that instance, \Vinst{up},
and let its length be
\begin{equation}
\label{eq:right-location-of-top-down-cause-12}
\Vsize{\Vinst{up}} =
(\Right{\Vinst{up}} \subtract \Left{\Vinst{up}}).
\end{equation}
From Theorem \ref{t:symbolic-causes-from-effect},
we know that
\begin{equation}
\label{eq:right-location-of-top-down-cause-15}
\Right{\Vinst{up}} = \Right{\Veim{effect}}
\end{equation}
From the definition of matching causes,
\begin{equation}
\label{eq:right-location-of-top-down-cause-18}
\Right{\Veim{down}} = \Left{\Vinst{up}}.
\end{equation}
Using
\eqref{eq:right-location-of-top-down-cause-12},
\eqref{eq:right-location-of-top-down-cause-15}
and
\eqref{eq:right-location-of-top-down-cause-18}
we see that
\begin{equation}
\label{eq:right-location-of-top-down-cause-21}
\Right{\Veim{effect}} = \Right{\Veim{down}} + \Vsize{\Vinst{up}}.
\end{equation}

If \Veim{effect} is a null scan, \Vinst{up} is
a nulling symbol instance,
$\Vsize{\Vinst{up}} = 0$.
Using
\eqref{eq:right-location-of-top-down-cause-21},
we have
\[
  \Right{\Veim{down}} = \Right{\Veim{effect}},
\]
which is requirement
\ref{req:right-location-of-top-down-cause-4}.

If \Veim{effect} is a read, \Vinst{up} is
a terminal symbol instance,
$\Vsize{\Vinst{up}} = 1$.
Using
\eqref{eq:right-location-of-top-down-cause-21}
\[
  \Right{\Veim{down}} = \Right{\Veim{down}} \subtract 1,
\]
which is requirement
\ref{req:right-location-of-top-down-cause-6}.

If \Veim{effect} is a reduction, \Vinst{up} is
a non-terminal symbol instance,
$\Vsize{\Vinst{up}} \ge 1$.
Using
\eqref{eq:right-location-of-top-down-cause-21}
\[
  \Right{\Veim{down}} < \Right{\Veim{effect}},
\]
which is requirement
\ref{req:right-location-of-top-down-cause-9}.

\end{proof}

\begin{definition}
\label{def:tether}
\qdtitle{Tether}
Let \Veim{base} be an Earley item.
The sequence
\begin{equation}
\el{teth}{0},
\el{teth}{1},
\ldots
\Vel{teth}{top}, \quad \text{where $\var{top} \ge 1$}
\end{equation}
is a
\dfn{tether}
of \Veim{base} if and only if
\begin{align}
\label{eq:def-tether-10}
& \el{teth}{0} = \Veim{eim},
\\
\label{eq:def-tether-20}
& \text{\Vel{teth}{top} is the start EIM, and}
\\
\label{eq:def-tether-30}
& \myparbox{
for all \var{x} such that
$\var{top} > \var{x} \ge 0,$
\el{teth}{\Vincr{x}} is a top-down cause of
\Vel{teth}{x}.
}
\end{align}
We sometimes say that
\Veim{eim} ``has'' the tether \var{teth}.
\end{definition}

Recall from standard parsing theory that two derivations
which share the same parse tree are considered equivalent,
in the sense that they derive the same sentence,
in our case \Cw{}.
For our purposes,
we want to know if derivations
from the same parse tree
are equivalent in a stronger
sense.
The following theorem show that,
for two derivations from the same parse tree,
the sets of EIM validity equivalents are the same.

\begin{theorem}
\label{t:derivation-eim-equivalence}
Let \Veim{eim} be an EIM,
and let \var{d1} and \var{d2} be two derivations
which share the same parse tree.
\Veim{eim} has a validity equivalent in \var{d1}
if and only if it has a validity equivalent in \var{d2}.
\end{theorem}

\begin{proof}
Because \var{d1} and \var{d2} are symmetric, we need
to show only the ``if'' direction.
We assume that
\Veim{eim} has a validity equivalent in \var{d1},
to show that it also has a validity equivalent in \var{d2}.
Let the EIM be
\begin{multline}
\label{eq:derivation-eim-equivalence-10}
   \Veim{eim} =
   \big[ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vstr{suffix} ], \Vloc{i}, \Vloc{j} ] \\
\text{where $\Vstr{prefix} = \Vsym{C1} \Vsym{C2} \ldots \Vsym{Cn}$} \\
\text{and $\Vstr{suffix} = \Vsym{D1} \Vsym{D2} \ldots \Vsym{Dn}$}
\end{multline}
so that the validity equivalent of \Veim{eim} is
\begin{equation}
\label{eq:derivation-eim-equivalence-15}
    \Vsym{A} \derives \Vmk{i} \Vstr{prefix} \Vmk{j} \Vstr{suffix}
\end{equation}
Here
\eqref{eq:derivation-eim-equivalence-10} and
\eqref{eq:derivation-eim-equivalence-15}
are without loss of generality.
The corresponding node and direct descendants in the parse tree
will be
\[
\Tree [.A
  \Vsym{C1} \Vsym{C2} \ldots{} \Vsym{Cn}
  \Vsym{D1} \Vsym{D2} \ldots{} \Vsym{Dn}
]
\]
and since symbol instances in the parse tree are those
of \var{d1},
it remains the case that
\begin{equation}
\label{eq:derivation-eim-equivalence-20}
\Vsym{C1} \Vsym{C2} \ldots{} \Vsym{Cn} \destar \Cw[\var{i},(\Vdecr{j})]
\end{equation}

The order of the derivation in \var{d2} may differ from \var{d1}.
But
\eqref{eq:derivation-eim-equivalence-20} from the parse tree will continue
to hold in \var{d2};
and in \var{d2} there is some point at which \Vsym{A}
expands into its direct descendants.
Therefore
\eqref{eq:derivation-eim-equivalence-15}
holds for some derivation step in \var{d2}.
Since
\eqref{eq:derivation-eim-equivalence-15}
is unchanged,
it remains the validity equivalent of
\eqref{eq:derivation-eim-equivalence-10}.
Because \eqref{eq:derivation-eim-equivalence-10}
was chosen without loss of generality,
this shows the theorem.
\end{proof}

\begin{lemma}
\label{lem:eim-tether-prediction}
Let
$\Veim{prd} = \langle \var{s}, \var{rha1}, \var{rhz1}, 0\rangle$ be a prediction
expressed in terms of \var{d},
a derivation focused within \Veim{prd}.
Then \Veim{prd} has a top-down cause, \Veim{down},
such that,
for some
\var{rha2}, \var{rhz2}, \var{dot2},
we have
\begin{gather}
\label{lem:eim-tether-prediction-5}
\Veim{down} = \langle \Vdecr{s}, \var{rha2}, \var{rhz2}, \var{dot2}\rangle \\
\label{lem:eim-tether-prediction-8}
\text{and \var{d} is focused within \Veim{down}.}
\end{gather}
\end{lemma}

\begin{proof}
Since \Veim{prd} is a prediction, it is not the start EIM,
and therefore its LHS symbol is not \Vsym{accept}.
Without loss of generality, let
\begin{equation}
\label{lem:eim-tether-prediction-15}
\Veim{prd} = \big[ [ \Vsym{prd} \de \mydot \Vstr{prd-rhs} ], \Vloc{j}, \Vloc{j} \big]
\end{equation}
By assumption for the lemma, \var{d} is focused within \Veim{eim},
and it is at derivation step \var{s}.
So,
combining Theorem \ref{t:focusing-props} with
\eqref{lem:eim-tether-prediction-15},
we have
for some \Vloc{i}, \Vstr{pre} and \Vstr{post},
\begin{equation}
\label{lem:eim-tether-prediction-20}
\begin{aligned}
& \Vsym{down} && \text{Step \xxsubtract{\var{s}}{2}} \\
\derives \; & \Vmk{i} \Vstr{pre} \Vmk{j} \Vsym{prd} \Vstr{post} \qquad && \text{Step \Vdecr{s}} \\
\derives \; & \Vmk{i} \Vstr{pre} \Vmk{j} \Vsym{prd-rhs} \Vstr{post} \qquad && \text{Step \var{s}}
\end{aligned}
\end{equation}
and
\begin{equation}
\label{lem:eim-tether-prediction-23}
\text{\var{d} is focused within \Vsym{down}.}
\end{equation}

From
\eqref{lem:eim-tether-prediction-20},
we see that
$[\Vsym{down} \de \Vstr{pre} \Vsym{prd} \Vstr{post}]$ is a rule,
so that
\begin{equation}
\label{lem:eim-tether-prediction-24}
\Vdr{down} = [\Vsym{down} \de \Vstr{pre} \mydot \Vsym{prd} \Vstr{post}]
\end{equation}
is a
dotted rule
and one which is,
by the definition of top-down cause for dotted rules,
a top-down cause of \DR{\Veim{prd}}.
From this observation and
Definition \ref{def:causes},
we see that
\begin{equation}
\label{lem:eim-tether-prediction-25a}
\begin{gathered}
\text{\Veim{down} is a top-down cause of \Veim{prd}} \\
\text{if} \quad \DR{\Veim{down}} = \Vdr{down} \\
\text{and} \quad \Current{\Veim{down}} = \Current{\Veim{prd}}.
\end{gathered}
\end{equation}

From
\eqref{lem:eim-tether-prediction-20}
and the definition of EIM validity,
we see that
\begin{gather}
\label{lem:eim-tether-prediction-26}
\Veim{down} = \big[ [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{prd} \Vstr{post} ], \Vloc{i}, \Vloc{j} \big]
\\
\text{and} \quad \Valid{\Veim{down}}.
\end{gather}

From
\eqref{lem:eim-tether-prediction-24}
and
\eqref{lem:eim-tether-prediction-26},
we have
\begin{equation}
\label{lem:eim-tether-prediction-26c}
\DR{\Veim{down}} = \Vdr{down}
\end{equation}

From
\eqref{lem:eim-tether-prediction-20}
and
\eqref{lem:eim-tether-prediction-26},
we have
\begin{align}
\label{lem:eim-tether-prediction-27a}
\Current{\Veim{down}} & = \Vloc{j} \\
\label{lem:eim-tether-prediction-27b}
& = \myfnname{Left}\big(\Vsym{prd}@(\Vdecr{s})\big) \\
\label{lem:eim-tether-prediction-27c}
& = \Left{\LHS{\Veim{prd}}} \\
\intertext{and because \Veim{prd} is a prediction}
\label{lem:eim-tether-prediction-27d}
\Current{\Veim{down}} & = \Current{\Veim{prd}}.
\end{align}

From
\eqref{lem:eim-tether-prediction-25a},
\eqref{lem:eim-tether-prediction-26c}
and
\eqref{lem:eim-tether-prediction-27d},
we can conclude that
\begin{equation}
\label{lem:eim-tether-prediction-30}
\text{\Veim{down} is a top-down cause of \Veim{prd}.}
\end{equation}

In 4-tuple representation, \Veim{down} is
for some \var{tbd},
\var{rha2}, \var{rhz2}, \var{dot2},
\begin{equation}
\label{lem:eim-tether-prediction-29}
\Veim{down} = \langle \var{tbd}, \var{rha2}, \var{rhz2}, \var{dot2}\rangle.
\end{equation}
We see from
\eqref{lem:eim-tether-prediction-20} that the LHS instance of \Veim{down}
is at Step \xxsubtract{\var{s}}{2},
and its RHS instance is at Step \Vdecr{s}.
The EIM has the same derivation step as its RHS instance,
so that
\eqref{lem:eim-tether-prediction-29}
becomes
\begin{equation}
\label{lem:eim-tether-prediction-33}
\Veim{down} = \langle \Vdecr{s}, \var{rha2}, \var{rhz2}, \var{dot2}\rangle.
\end{equation}
\eqref{lem:eim-tether-prediction-30}
and
\eqref{lem:eim-tether-prediction-33}
show
\eqref{lem:eim-tether-prediction-5}
in the statement of the lemma.

It remains to show
\eqref{lem:eim-tether-prediction-8}
in the statement of the lemma.
From
\eqref{lem:eim-tether-prediction-23},
we know that
\var{d} is focused in \LHS{\Veim{down}}.
By the definition of derivations focused within an EIM,
we have
\eqref{lem:eim-tether-prediction-8}
and the lemma.
\end{proof}

\begin{lemma}
\label{lem:eim-tether-confirmation}
Let
\begin{equation}
\label{lem:eim-tether-confirmation-2}
\Veim{firm} = \langle \var{s}, \var{rha}, \var{rhz}, \var{dx}\rangle
\end{equation}
be a confirmed
EIM
expressed in terms of \var{d},
a derivation focused within \Veim{firm}.
Then \Veim{prd} has a top-down cause,
\begin{gather}
\label{lem:eim-tether-confirmation-5}
\Veim{down} = \langle \var{s}, \var{rha}, \var{rhz}, \Vdecr{dx}\rangle \\
\label{lem:eim-tether-confirmation-8}
\text{and \var{d} is focused within \Veim{down}.}
\end{gather}
\end{lemma}

\begin{proof}
Without loss of generality,
we write
\eqref{lem:eim-tether-confirmation-2}
in 3-tuple form as
\begin{equation}
\label{lem:eim-tether-confirmation-15}
\Veim{firm} = [ [ \Vsym{A} \de \Vstr{before} \cat \Vsym{up} \mydot \Vstr{after} ], \var{i}, \var{k} ].
\end{equation}
its predot symbol instance as
\begin{equation}
\label{lem:eim-tether-confirmation-18}
\Vmkl{j} \Vsym{up} \Vmkr{k}
\end{equation}
and
\Veim{down} as
\begin{equation}
\label{lem:eim-tether-confirmation-21}
\Veim{down} = [ [ \Vsym{A} \de \Vstr{before} \mydot \Vsym{up} \cat \Vstr{after} ], \var{i}, \var{j} ]
\end{equation}
where
by
Definition \ref{def:4-tuple-eim},
the definition of the 4-tuple notation for EIM's,
we have
\begin{gather}
\label{lem:eim-tether-confirmation-22}
\Vsym{A} = \Symbol{\drv{d}{\Vdecr{s}}{\var{rha}}} \\
\notag
\Vstr{before} = \Symbol{\drv{d}{\var{s}}{\, \var{rha} \; \ldots \; (\var{rha}+\Vdecr{dx}) \, }} \\
\notag
\Vsym{up} = \Symbol{\drv{d}{\var{s}}{\var{rha}+\var{dx}}} \quad \text{and} \\
\notag
\Vstr{after} = \Symbol{\drv{d}{\var{s}}{\, (\var{rha}+\var{dx}+1) \; \ldots \; \var{rhz} \, }}
\end{gather}
By Definition \ref{def:causes} (``causes''),
\eqref{lem:eim-tether-confirmation-21} is a top-down cause
of \Veim{firm}.

Let our first draft of \Veim{down} take the fully general form
\begin{equation}
\label{lem:eim-tether-confirmation-27}
\Veim{down} = \langle \var{s2}, \var{rha2}, \var{rhz2}, \var{dx2}\rangle \\
\end{equation}
The symbol instances of the RHS for \Veim{down} are exactly those
of \Veim{firm}:
$\var{rha} = \var{rha2}$,
and
$\var{rhz} = \var{rhz2}$.
Since the symbol instances of the RHS are the same as those of \Veim{firm},
\Veim{down}'s RHS derivation step is also the same,
and therefore its EIM step is the same:
$\var{s} = \var{s2}$.
Finally, it is clear from comparision of
\eqref{lem:eim-tether-confirmation-15}
with
\eqref{lem:eim-tether-confirmation-21},
that
$\var{dx2} = \Vdecr{dx}$.
Collecting these results and substituting them in
\eqref{lem:eim-tether-confirmation-27},
we have
\begin{equation}
\label{lem:eim-tether-confirmation-35}
\Veim{down} = \langle \var{s}, \var{rha}, \var{rhz}, \Vdecr{dx}\rangle,
\end{equation}
which shows
\eqref{lem:eim-tether-confirmation-5}
in the statement of the lemma.

It remains to show
\eqref{lem:eim-tether-confirmation-8}
in the statement of the lemma.
If \var{d} is focused within \Veim{firm} then it is,
by the definition of a focused derivation for EIM's,
focused within its LHS symbol instance.
From
\eqref{lem:eim-tether-confirmation-2},
we see that
the LHS symbol instance of \Veim{firm}
is \drv{d}{\Vdecr{s}}{\var{rha}}
From
\eqref{lem:eim-tether-confirmation-22} we see that
the LHS symbol instance of \Veim{down} is also
\drv{d}{\Vdecr{s}}{\var{rha}}.
So \var{d} is focused within the LHS symbol instance of
\Veim{down},
and therefore
\var{d} is focused within \Veim{down}.
This shows
\eqref{lem:eim-tether-confirmation-8}.
\end{proof}

\begin{theorem}
\label{t:deriv-tether}
\ttitle{EIM tether}
Let
\begin{equation}
\label{t:deriv-tether-3}
\Veim{eim} = \big[ [ \Vsym{A} \derives \Vstr{predot} \mydot \Vstr{postdot} ],
\Vorig{i}, \Vloc{j} \big]
\end{equation}
Then
\begin{equation}
\label{t:deriv-tether-5}
\Vsym{A} \derives [\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot}
\end{equation}
if and only if
\begin{equation}
\label{t:deriv-tether-8}
\text{\Veim{eim} has a tether.}
\end{equation}
\end{theorem}

\begin{proof}
\textbf{The ``if'' direction}:
The ``if'' direction is easy to show.
We assume
\eqref{t:deriv-tether-8}
to show
\eqref{t:deriv-tether-5}.
If \Veim{eim} has a tether,
call it \var{teth},
then,
by the definition of a tether,
\Veim{eim} is \el{teth}{0}.
All elements of \var{teth} are valid
by
the assumption of
\eqref{t:deriv-tether-8}.
\eqref{t:deriv-tether-5}
follows from
\eqref{t:deriv-tether-3}
by EIM validity.
This shows the ``if'' direction.

\textbf{The ``only if'' direction}:
For ``only if'' direction,
we will
assume
\eqref{t:deriv-tether-5}
to show
\eqref{t:deriv-tether-8}.
To do this, we will first construct a
\var{teth} which has \Veim{eim}
as its base,
and then we will show that \var{teth}
satisfies requirements
\eqref{eq:def-tether-10},
\eqref{eq:def-tether-20}
and
\eqref{eq:def-tether-30}
in Definition \ref{def:tether} (``tether'').

From
\eqref{t:deriv-tether-5}
and the definition of EIM validity,
we know that
\eqref{t:deriv-tether-3}
is valid.
We assume
\eqref{t:deriv-tether-5}
to be focused within \Veim{eim}.
We can do this without loss of generality,
because by Theorem
\ref{t:derivation-eim-equivalence},
every derivation has a
derivation focused at \Vloc{i} which validates
the same set of EIM's.

We construct \var{teth} using
the following algorithm:
\begin{algorithmic}[1]
\Procedure{Construct tether}{}
\State $\el{teth}{0} \gets \Veim{eim}$
\label{line:eim-tether-10}
\State $\var{ix} \gets 0$
\While{ \Vel{teth}{ix} is not the start EIM }
\label{line:eim-tether-20}
\If{\Vel{teth}{ix} is a confirmed EIM}
\State Find \Veim{td}, top-down cause of \Vel{teth}{ix} \ldots
\State \ldots{} using Lemma \ref{lem:eim-tether-confirmation}
\Else
\State If here, \Vel{teth}{ix} is a prediction
\State Find \Veim{td}, top-down cause of \Vel{teth}{ix} \ldots
\State \ldots{} using Lemma \ref{lem:eim-tether-prediction}
\EndIf
\State $\el{teth}{\Vincr{ix}} \gets \Veim{td}$
\State $\var{ix} \gets \Vincr{ix}$
\EndWhile
\label{line:eim-tether-90}
\EndProcedure
\end{algorithmic}%
\index{recce-theorems}{Construction: Construct tether}

\textbf{``Only if'', first requirement}:
We have
\eqref{eq:def-tether-10},
trivially from
Line \ref{line:eim-tether-10}
of the above algorithm.

\textbf{``Only if'', second requirement}:
We show
\eqref{eq:def-tether-30}
by induction, where the induction hypothesis
is
\begin{gather}
\label{t:deriv-tether-20}
\text{\var{d} is focused within \Vel{teth}{x}} \\
\label{t:deriv-tether-22}
\text{and} \quad
\left(
\begin{gathered}
\var{x} = 0 \\
\text{or \Vel{teth}{x} is a top-down cause} \\
\text{of \Vel{teth}{\Vdecr{x}}}.
\end{gathered}
\right)
\end{gather}

We assumed that \var{d} is focused within
$\el{teth}{0} = \Veim{eim}$,
and we have
\eqref{t:deriv-tether-22} for $\var{x} = 0$
trivially.
We use this as the basis of the induction.

For the step of the induction we assume
\begin{equation}
\tag{STEP}
\label{t:deriv-tether-30}
\text{\eqref{t:deriv-tether-20} and \eqref{t:deriv-tether-22}
for $\var{x} = \var{i}$.}
\end{equation}
and we seek to show
\begin{equation}
\tag{GOAL}
\label{t:deriv-tether-35}
\text{
\eqref{t:deriv-tether-20} and \eqref{t:deriv-tether-22}
for $\var{x} = \Vincr{i}$.
}
\end{equation}
When \Vel{teth}{i} is a confirmed EIM,
\eqref{t:deriv-tether-35} follows from
\eqref{t:deriv-tether-30}
and Lemma \ref{lem:eim-tether-confirmation}.
When \Vel{teth}{i} is a predicted EIM,
\eqref{t:deriv-tether-35} follows from
\eqref{t:deriv-tether-30}
and Lemma \ref{lem:eim-tether-prediction}.
This shows the step of the induction,
the induction,
and
\eqref{eq:def-tether-30}.

\textbf{``Only if'', third requirement}:
It remains to show
\eqref{eq:def-tether-20}.
From Line
\ref{line:eim-tether-20}
of the algorithm, we see that we stop
only when and if we find a start EIM.
So if we stop,
and \var{top} is the last index of \var{teth},
then \Vel{teth}{top} is the start EIM.
But do we ever stop?

We will count the number of EIM's in the tether.
They fall into three categories:
the start EIM,
predicted EIM's,
and confirmed EIM's,
whose count we will write as,
respectively \var{s-cnt},
\var{p-cnt},
and
\var{c-cnt}.
It will be most convenient to count EIM's,
not as they are added,
but as they are processed in a pass of the
loop from Lines
\ref{line:eim-tether-20}--\ref{line:eim-tether-90}
of our ``Construct tether'' algorithm.
From
Line \ref{line:eim-tether-20},
we see that
only one start EIM
is ever processed:
\begin{equation}
\label{t:deriv-tether-40}
\var{s-cnt} = 1.
\end{equation}
We note from
Lemma \ref{lem:eim-tether-prediction},
that every time we process a prediction in \var{teth},
the derivation step is decremented in the next
and examination of
Lemma \ref{lem:eim-tether-confirmation}
shows that the derivation step is never incremented.
Recall that the length of every derivation is
a finite constant ---
even a derivation of the form $\Vstr{A} \destar \Vstr{B}$
signifies that there exists some finite \var{k}
such that $\Vstr{A} \xderives{\var{k}} \Vstr{B}$.
So we know that
$\var{p-cnt} = \var{k}$ for some finite \var{k},
and therefore
\begin{equation}
\label{t:deriv-tether-43}
\var{p-cnt} = \Oc.
\end{equation}

Finally, we examine the confirmations.
Call the maximum length of a RHS, \var{maxrh}.
\var{maxrh} is a constant which depends on \Cg.
We see from
Lemma \ref{lem:eim-tether-confirmation}
that the dot index is decremented every
time a confirmation is processed.
But Lemma \ref{lem:eim-tether-prediction}
resets the dot index
every time a prediction is processed,
possibly to as high as
\var{maxrh}.
So the number of confirmations may be as great as
\begin{equation}
\label{t:deriv-tether-46}
\var{c-cnt} = \var{maxrh} \times \var{p-cnt}.
\end{equation}
\var{maxrh} is a constant,
so that, using \eqref{t:deriv-tether-43},
\eqref{t:deriv-tether-46}
becomes
\begin{equation}
\label{t:deriv-tether-49}
\var{c-cnt} = \Oc.
\end{equation}
Combining
\eqref{t:deriv-tether-40},
\eqref{t:deriv-tether-43}
and
\eqref{t:deriv-tether-49},
we see that the total number of tether EIM's is
\begin{equation}
\var{s-cnt} + \var{p-cnt} + \var{c-cnt} =
1 + \Oc + \Oc = \Oc.
\end{equation}

We have shown that the construction of the tether does,
indeed, stop.
With this we have shown
\eqref{eq:def-tether-20}
and the theorem.
\end{proof}

\begin{theorem}
\label{t:eim-tether}
\ttitle{EIM tether}
If \Veim{eim} is valid,
it has a tether.
\end{theorem}

\begin{proof}
Without loss of generality, let
\begin{equation}
\label{t:eim-tether-3}
\Veim{eim} = \big[ [ \Vsym{A} \derives \Vstr{predot} \mydot \Vstr{postdot} ],
\Vorig{i}, \Vloc{j} \big]
\end{equation}
Since, by assumption for the theorem, \Veim{eim} is valid,
we have
\begin{equation}
\label{t:eim-tether-5}
\Vsym{A} \derives [\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot}
\end{equation}
This theorem follows from
\eqref{t:eim-tether-5} and
Theorem \ref{t:deriv-tether}
\end{proof}

If \var{hi} and \var{lo} are
tether indexes such that $\var{hi} > \var{lo}$,
we say that \Vel{teth}{lo} is
\xdfn{below}{below (in a tether)}
\Vel{teth}{hi};
and that the direction from
\Vel{teth}{hi} to
\Vel{teth}{lo} is
\xdfn{downward}{downward (in a tether)}.

\begin{theorem}
\label{t:non-decreasing-location}
If we follow a tether downward,
the right locations of its elements are non-decreasing:
\begin{equation}
\var{lo} < \var{hi}
\implies \Right{\Vel{teth}{lo}} \ge \Right{\Vel{teth}{hi}}
\end{equation}
\end{theorem}

\begin{proof}
By examining
the cases of Theorem
\ref{t:right-location-of-top-down-cause},
we see
that the right location of an effect
is never less than the right location of its
top-down cause.
\end{proof}

\begin{theorem}
\ttitle{Location 0 Earley items}
\label{t:location-0-eims}
A EIM is valid at location 0 if and only if it
is in the ethereal closure of the start EIM.
\end{theorem}

\begin{proof}
Let \var{ecs} be the ethereal closure of the
start EIM.
We first show
the ``if'' direction.
We assume that an EIM is in \var{ecs} to show that
it is at location 0.
The start EIM is by definition at location 0.
Any other EIM in \var{ecs} is produced by iterating
\var{epsilon-op}.
As can be seen from
Theorem
\ref{t:right-location-of-top-down-cause},
an \var{epsilon-op} does not increment the right location
of its argument.
Therefore every EIM in the ethereal closure of the start
EIM will be at location 0.

It remains to
show the ``only if'' direction.
We assume that an EIM, call it \Veim{eim},
is at location 0.
We seek
to show that \Veim{eim} is in \var{ecs}.
By Theorem \ref{t:eim-tether},
every EIM has a tether,
so every EIM at location 0 has a tether.
By Theorem
\ref{t:non-decreasing-location}
locations in a tether are non-decreasing
so that every element of the tether of \Veim{eim}
must be at location zero.
We note from
Theorem
\ref{t:right-location-of-top-down-cause},
that the reduction and read EIM's have a location
greater than that of their top-down cause,
and therefore other than 0.
So the tether of \Veim{eim} can contain only null-scans,
predictions, and the start EIM.
By its definition, the tether starts with its top
element, the start EIM.
Therefore \Veim{eim} is the result of the iteration
of epsilon operations,
beginning with the start EIM.
By the definition of ethereal closure, any EIM
that is the result of iterating epsilon operations
on a telluric base is
in the ethereal closure of that telluric base.
So \Veim{eim} is in the ethereal closure of the
start EIM, or \var{ecs}.
\end{proof}

\begin{theorem}
\ttitle{Earley item has telluric base at same location}
\label{t:eim-has-telluric-base}
Let \Veim{desc} be a valid EIM at location
\Vloc{i}.
Then \Veim{desc} has a telluric base,
\Veim{base}, such that
$\Current{\Veim{base}} = \Vloc{i}$.
\end{theorem}

\begin{proof}
We consider first, as a special case,
location 0.
By Theorem \ref{t:location-0-eims},
all EIM's at location 0
are in the ethereal closure of the start EIM,
call it \var{ecs}.
The telluric base of \var{ecs} is the start
EIM, and it is at location 0.

We now consider the case
locations greater than 0.
By Theorem \ref{t:eim-tether}
\Veim{base} has a tether,
call it \var{teth}.
By definition of the tether, its top element has right
location 0.
By assumption for this case,
\Veim{desc} has a right location greater than 0.
Let \Current{\Veim{desc}} be \Vloc{i}.
If we descend
\var{teth} from the its top element,
we will encounter a first element of \var{teth} at \Vloc{i}.
Call this first element, $\Veim{base} = \Vel{teth}{base-ix}$.

\Veim{base} will not be the top element
because the top element is at location 0,
which by assumption for the case is not location \Vloc{i}.
Therefore there will be a tether element above it,
\el{teth}{\Vdecr{base-ix}}.
Call
\el{teth}{\Vdecr{base-ix}}, \Veim{prev}.
\Veim{prev} will not be at \Vloc{i}, otherwise it would
have been the first tether element encountered at \Vloc{i}.
So we have
\begin{equation}
\label{eq:eim-has-telluric-base-20}
\Right{\Veim{prev}} \neq \Right{\Veim{base}}.
\end{equation}
From
\eqref{eq:eim-has-telluric-base-20},
and Theorem
\ref{t:right-location-of-top-down-cause},
we know that \Veim{base} is not a null-scan
or a prediction.
Therefore
\begin{equation}
\label{eq:eim-has-telluric-base-25}
\text{\Veim{base} is a telluric EIM.}
\end{equation}

We choose
\Veim{base} in such a way that
\[
  \Right{\Veim{base}} = \Right{\Veim{desc}},
\]
and \Veim{base} is on \var{teth} above \Veim{desc}.
So there is a series of zero or more top-down causes
from \Veim{base} to \Veim{desc}.
Let the tether index of \Veim{desc} be \var{desc-ix}.
Since right location is non-decreasing,
every EIM with an index \var{ix}
such that
\[
\var{base-ix} > \var{ix} \ge \var{desc-ix}
\]
must not increase the right location
from that of its top-down cause.
Therefore,
\begin{equation}
\label{eq:eim-has-telluric-base-35}
\Current{\Veim{base}} = \Current{\Veim{desc}}.
\end{equation}
Together,
\eqref{eq:eim-has-telluric-base-25}
and
\eqref{eq:eim-has-telluric-base-35}
show the theorem.
\end{proof}

\chapter{Earley tables}
\label{ch:earley-tables}

In the context of a specific \Cg{},
a specific \Cw{},
and a specific Earley implementation,
an Earley parser builds a table of Earley sets,
\Ctables.
Let \alg{Impl} be an Earley implementation.
In contexts where it is not clear
which implementation is being referred to,
we say \Vtables{Impl}
when we are referring to the tables of \alg{Impl}.
For example, the tables for the Marpa
are \Vtables{Marpa}.

Traditionally, the tables of an Earley algorithm
are grouped into sets,
one \dfn{Earley set} for each location of \Cw{}:
\begin{equation*}
\EVtable{\alg{Impl}}{i},
\quad \text{where} \quad
0 \le \Vloc{i} \le \size{\Cw}.
\end{equation*}
Earley sets are of type \type{ES}.
Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
The type designator \type{ES} is often omitted to avoid clutter,
especially in cases where the Earley set is not
named by location.

\EVtable{\alg{Impl}}{i} will be
the Earley set at \Vloc{i}
in the table of Earley sets of
the \alg{Impl} implementation.
For example,
\EVtable{\Marpa}{j} will be Earley set \Vloc{j}
in \Marpa's table of Earley sets.
In contexts where it is clear which recognizer is
intended,
\Vtable{k}, or \Ves{k}, will symbolize Earley set \Vloc{k}
in that recognizer's table of Earley sets.
If \Ees{\var{working}} is an Earley set,
$\size{\Ees{\var{working}}}$%
\index{recce-notation}{\Pipe{}es\Pipe{}@\Vsize{es} (size of an Earley set)}
is the number of Earley items
in \Ees{\var{working}}.

\Rtablesize{\alg{Recce}} is the total number
of Earley items in all Earley sets for \alg{Recce},
\begin{equation*}
\Rtablesize{\alg{Recce}} =
     \sum\limits_{\Vloc{i}=0}^{\size{\Cw}}
	{\bigsize{\EVtable{\alg{Recce}}{i}}}.
\end{equation*}
For example,
\Rtablesize{\Marpa} is the total number
of Earley items in all the Earley sets of
a \Marpa{} parse.

An Earley item may be memoized.
An Earley item is \dfn{memoized}
if and only if it is
not kept in an Earley set,
but is kept in a form from which it can be recovered.
Chapter \ref{ch:leo}
will show one way in which Earley items
can be memoized.
To say that \Veim{x} is memoized,
we also say \Memoized{\Veim{x}}.%
\index{recce-notation}{Memoized(x)@\Memoized{\var{eim}}}

The memozations discussion in this document
will all be optional ---
their omission will not affect correctness.
An EIM wich could be memozied is said to be
\dfn{memoable}.
To say that \Veim{x} is memoable,
we also write \Memoable{\Veim{x}}.%
\index{recce-notation}{Memoable(x)@\Memoable{\var{eim}}}

A set of EIM's (not necessarily an Earley set)
is \dfn{consistent} if and only if all of its
EIM's are valid and unmemoized.
For example,
the Earley set \Ves{i}
is \dfn{consistent} if and only if
every EIM in the Earley set at \Vloc{i}
is valid and unmemoized:
\begin{equation}
\label{eq:def-complete-1}
\Veim{eim} \in \Veimset{x} \implies \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}}
\end{equation}

Let \Veimset{x} be a set of EIM's.
\Veimset{x} is not necessary an Earley set.
We say that
\Veimset{x} is \dfn{complete} for a predicate \var{phi},
if and only if,
for all \Veim{eim},
\begin{equation}
\label{eq:def-complete-2}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \land \var{phi}(\var{eim}) \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation}
If we say that an EIM set, \Veimset{x},
is \dfn{self-complete},
then \var{phi} is membership in \Veimset{x}:
\[ \lambda \Veim{eim} . \var{eim} \in \Veimset{x}, \]
so that
\eqref{eq:def-complete-2} simplifies to
\begin{equation}
\label{eq:def-complete-4}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation}

For example,
the EIM set \Veimset{x} is complete
for the predicate
\begin{equation}
\label{eq:def-complete-6}
\lambda \var{eim} . \Current{\Veim{eim}} = \Vloc{i}
\end{equation}
if and only if,
for all \Veim{eim},
\begin{equation*}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \land \Current{\var{eim}} = \Vloc{i} \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation*}
If $\Veimset{x} = \Ves{i}$, then
saying that \Ves{i} is complete for
\eqref{eq:def-complete-6}
is the same as saying that
\Ves{i} is self-complete.

We say that
\Veimset{x} is \dfn{correct} for a predicate \var{phi},
if and only if it is consistent
and complete for a predicate \var{phi}.
We say that
\Veimset{x} is \dfn{self-correct},
if and only if it is consistent
and self-complete.

We often say that an EIM set is complete or correct,
in a context where no predicate is specified.
In that case,
we mean that the EIM set is self-complete or self-correct.

We call
\begin{equation}
\Veim{accept} = [\Vdr{accept}, 0, \Vsize{\Cw}]
\end{equation}
the \dfn{accept EIM}.
Let \alg{Impl} be an Earley implementation.
We say that \alg{Impl}
\dfn{accepts} an input
if and only if \Veim{accept} is in its Earley tables:
\begin{equation}
\label{eq:def-implementation-accepts}
\myL{\alg{Impl},\Cg}
\defined
\Veim{accept} \in \Vtables{Impl}
\end{equation}
We say that an Earley implementation is \dfn{correct} if and only
if the set of strings it accepts is exactly the set of
strings in the language of its grammar.

\begin{theorem}\label{t:algorithm-correct}
\ttitle{Earley implementation correctness}
If, for a given Earley implementation,
\begin{equation}
\label{eq:algorithm-correct-1}
\text{\Ves{\Vsize{\Cw}} is correct,}
\end{equation}
then
that implementation accepts all and only the
correct inputs:
\begin{equation}
\label{eq:algorithm-correct-2}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}} \; \equiv \; \Cw \in \var{L}(\Cg).
\end{equation}
\end{theorem}

\begin{proof}
We prove the forward direction of the equivalence first.
We assume that
\begin{equation}
\label{eq:algorithm-correct-3}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
By \eqref{eq:algorithm-correct-1},
\Ves{\Vsize{\Cw}} is correct.
Therefore,
by \eqref{eq:algorithm-correct-3},
the definition of validity for an Earley item,
and the definition of the accept rule \eqref{eq:accept-rule-def},
\begin{equation}
\label{eq:algorithm-correct-6}
\begin{split}
& \Vsym{accept} \destar \mk{0} \Vsym{start} \mk{\Vsize{\Cw}}
\end{split}
\end{equation}
By the definition of the location markers,
\eqref{eq:algorithm-correct-6}
means that
\begin{equation}
\label{eq:algorithm-correct-15}
\Vstr{accept} \destar \var{w}[\var{0}, \Vsize{\Cw} \subtract 1] \destar \Cw.
\end{equation}
From
\eqref{eq:algorithm-correct-15},
by the definition of \Cw{},
we have that
\begin{equation}
\label{eq:algorithm-correct-27}
\Cw \in \var{L}(\Cg).
\end{equation}
This shows the forward direction of the equivalence.

To show the reverse direction of the equivalence, we assume
\eqref{eq:algorithm-correct-27}.
From it,
the definition of the \Vsym{accept} symbol,
and the definition of $\var{L}(\Cg)$,
we have
\eqref{eq:algorithm-correct-15}.
By the definition of the \Vsym{accept} symbol,
it is only on the LHS of the accept rule,
so that we have
\begin{equation}
\label{eq:algorithm-correct-40}
\Vsym{accept} \derives [0]\, \Vsym{start} \,[\Vsize{\Cw}] \destar \Cw.
\end{equation}
By assumption for the theorem, the Earley set at
\Vsize{\Cw} is correct and therefore complete.
If the Earley set at \Vsize{\Cw} is complete,
by \eqref{eq:algorithm-correct-40}
and the definition of validity for Earley items,
we have
\begin{equation}
\label{eq:algorithm-correct-46}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
This shows the reverse direction of the equivalence.
We have now shown both directions of the equivalence,
and therefore the theorem.
\end{proof}

\begin{theorem}
\ttitle{Earley set size}
\label{t:es-count}
The size of the Earley set at \Vloc{i} is,
worst case, $\order{\var{i}}$:
\begin{equation*}
\textup{
    $\bigsize{\EVtable{\Marpa}{i}} = \order{\var{i}}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
EIM's have the form $[\Vdr{x}, \Vorig{x}]$.
\Vorig{x} is the origin of the EIM,
which in Marpa cannot be after the current
Earley set  at \Vloc{i},
so that
\begin{equation*}
0 \le \Vorig{x} \le \Vloc{i}.
\end{equation*}
The number of possible values for \Vdr{x}
is the count of dotted rules in \Cg{}.
Call that count, $\size{\Cdr}$.
$\size{\Cdr}$
is a finite constant that depends on the grammar,
\Cg{}.
Since duplicate EIM's are never added to an Earley set,
the maximum size of Earley set \Vloc{i} is therefore
\begin{equation*}
\Vloc{i} \times \size{\Cdr} = \order{\Vloc{i}}.\qedhere
\end{equation*}
\end{proof}

\chapter{Silos}
\label{ch:silos}

\begin{definition}
\dtitle{Silo}
\label{def:silo}
A dotted rule is
\xdfn{silo-eligible}{silo-eligible (dotted rule)}
if and only if it is quasi-complete.
An Earley item
is
\xdfn{silo-eligible}{silo-eligible (EIM)}
if and only
if its dotted rule is silo-eligible.
We say that an EIM, call it \Veim{cuz},
is the \dfn{silo cause} of another EIM,
call it \Veim{effect},
if and only if
\begin{itemize}
\item
\Veim{cuz} and \Veim{effect}
are silo-eligible,
\item
$\Right{\Veim{cuz}} = \Right{\Veim{effect}}$,
and
\item
\Veim{cuz} is a top-down or bottom-up cause
of \Veim{effect}.
\end{itemize}
If \Veim{cuz} is the
\dfn{silo cause}
of \Veim{effect},
we say that \Veim{effect} is the
\dfn{silo effect}
of \Veim{cuz}.

A \dfn{silo},
call it \var{s1},
is a sequence of EIM's,
where
\Vel{s1}{\var{i}} is the silo cause of
\el{s1}{\Vincr{i}}.
\end{definition}

This definition implies that any subsequence of a silo
is also a silo.
Also, if any two overlapping silos are overlaid,
and the result will be a silo.

We call an element of a silo,
a
\xdfn{layer}{layer (of silo)}.
We write \Vsize{\var{s1}}%
\index{recce-notation}{\Pipe{}silo\Pipe{}@\Vsize{silo} (size of a silo)}
for the number
of elements in silo \var{s1}.
Our indexing convention is to use
consecutive non-negative integers for silo layers,
started at zero.
\el{s1}{0}
is called the
\xdfn{bottom}{bottom (of silo)}
of \var{s1},
and
$\var{s1}\big[ \, \Vlastix{s1} \, \big]$
is the
\xdfn{top}{top (of silo)}
of \var{s1}.

\begin{theorem}
\label{t:silo-location}
Let \Vel{silo}{i}
and
\Vel{silo}{j}
be two layers of a silo.
Then
\begin{equation}
\label{eq:silo-location-4}
\Right{\Vel{silo}{i}} =
\Right{\Vel{silo}{j}}.
\end{equation}
\end{theorem}

\begin{proof}
We first will show that
\begin{multline}
\label{eq:silo-location-7}
\forall \var{x} :
0 \le \var{x}
\le \Vel{silo}{top} \implies \\
\Right{\Vel{silo}{0}} =
\Right{\Vel{silo}{x}} \\
\qquad \shoveleft{\text{where $\var{top} =
\xxsubtract{\Vsize{\var{silo}}}{1}$}}.
\end{multline}
The proof
of \eqref{eq:silo-location-7}
is by induction.
We take as the induction hypothesis,
\begin{equation}
\tag{IND}
\label{eq:silo-location-10}
\Right{\el{silo}{0}} =
\Right{\Vel{silo}{x}}
\end{equation}
We have
\eqref{eq:silo-location-10}
trivially if $\var{x} = 0$,
and we use this as the basis of
the induction.

For the step, we assume
\eqref{eq:silo-location-10}
for $\var{x} = \var{i}$
to show
\eqref{eq:silo-location-10}
for $\var{x} = \Vincr{i}$.
We know that
\Vel{silo}{i}
is the silo cause of
\Vel{silo}{\Vincr{i}}.
By the definition of a silo cause,
then
\begin{equation}
\label{eq:silo-location-13}
\Right{\Vel{silo}{i}} =
\Right{\el{silo}{\Vincr{i}}}
\end{equation}
The assumption for the step is that
\begin{equation}
\label{eq:silo-location-16}
\Right{\el{silo}{0}} =
\Right{\el{silo}{\var{i}}}
\end{equation}
and from
\eqref{eq:silo-location-13}
and \eqref{eq:silo-location-16}
we have
\begin{equation}
\label{eq:silo-location-17}
\Right{\el{silo}{0}} =
\Right{\el{silo}{\Vincr{i}}},
\end{equation}
which shows the step, the induction and
\eqref{eq:silo-location-7}.

It remains to show
\eqref{eq:silo-location-4}.
\begin{align}
\label{eq:silo-location-19}
&
\Right{\el{silo}{0}} =
\Right{\Vel{silo}{i}}
&& \eqref{eq:silo-location-7}
\\
\label{eq:silo-location-22}
&
\Right{\el{silo}{0}} =
\Right{\Vel{silo}{j}}
&& \eqref{eq:silo-location-7}
\\
\label{eq:silo-location-25}
&
\Right{\Vel{silo}{i}} =
\Right{\Vel{silo}{j}}
&& \eqref{eq:silo-location-19},
\eqref{eq:silo-location-22}
\end{align}
\eqref{eq:silo-location-25} is
\eqref{eq:silo-location-4},
which shows the theorem.
\end{proof}

We say that \Right{\Vel{sil}{0}}
is the
\xdfn{location}{location (of a silo)}.
From Theorem 
\ref{t:silo-location}
we know that the location of the silo will
be the right locaiton of each of its layers.
We also write the
location of silo \var{sil}
as \Right{\var{sil}}.%
\index{recce-notation}{Right(silo)@\Right{\var{silo}}}

\begin{theorem}
\label{t:silo-causes}
\ttitle{Silo causes}
Every silo layer
\begin{itemize}
\item
is a read EIM,
and has no bottom-up or
top-down causes;
\item
is a null-scan EIM and
has at least one top-down silo cause
and no bottom-up silo causes; or
\item
is a reduction EIM and
has at least one bottom-up silo cause
and no top-down silo causes.
\end{itemize}
\end{theorem}

\begin{proof}
\textbf{Read EIM}:
Let \Veim{rd} be a quasi-complete read EIM.
By the definition of causes,
we know that
the bottom-up cause of \Veim{rd} has
no EIM equivalent and therefore is not
a silo cause.
Let \Veim{down} be the top-down cause of a
read EIM.
From Theorem
\ref{t:down-cause-from-effect},
we know that
\begin{gather}
\notag
\Right{\Veim{down}} = \decr{\Right{\Veim{rd}}} \\
\label{eq:silo-causes-20}
\therefore \; \Right{\Veim{down}} \neq \decr{\Right{\Veim{rd}}}
\end{gather}
\eqref{eq:silo-causes-20} shows that \Veim{down}
cannot be a silo cause of \Veim{rd}.

\textbf{Null-scan EIM}:
Let \Veim{ns} be a quasi-complete null-scan EIM.
By the definition of causes,
we know that
the bottom-up cause of \Veim{ns} has
no EIM equivalent and therefore is not
a silo cause.
Let \Veim{down} be the top-down cause of a
read EIM.
From Theorem
\ref{t:down-cause-from-effect},
we know that
\begin{equation}
\label{eq:silo-causes-30}
\myparbox{$\Right{\Veim{down}} = \Right{\Veim{ns}}.$}
\end{equation}
The only symbol between the dot index of \Veim{down}
and the dot index of \Veim{ns} is,
by the definition of a null-scan EIM,
a nulling symbol,
so that if the dot suffix of \Veim{ns} contains
only nulling symbols,
the dot suffix of \Veim{ns} must also contain
only nulling symbols.
Therefore
\begin{equation}
\label{eq:silo-causes-33}
\myparbox{\Veim{down} is quasi-complete.}
\end{equation}
From
\eqref{eq:silo-causes-30}
and
\eqref{eq:silo-causes-33},
we see that
\Veim{down} is a silo cause
of \Veim{ns}.

\textbf{Reduction EIM}:
Let \Veim{duct} be a quasi-complete reduction EIM,
let its top-down cause be \Veim{down}
and let the EIM equivalent of
its bottom-up cause be \Veim{up}.
By the definition of causes,
we know that
\begin{gather}
\label{eq:silo-causes-36a}
\myparbox{
the bottom-up cause of \Veim{duct} is
a complete EIM,
and is therefore a quasi-complete EIM;
} \\
\label{eq:silo-causes-36c}
\myparbox{
$\Right{\Veim{up}} = \Right{\Veim{duct}}.$
}
\end{gather}
From
\eqref{eq:silo-causes-36a}
and
\eqref{eq:silo-causes-36c},
we see that
\Veim{up}
is
a silo cause of \Veim{duct}.

From Theorem
\ref{t:down-cause-from-effect},
we know that
\begin{gather}
\notag
\Right{\Veim{down}} < \Right{\Veim{rd}} \\
\label{eq:silo-causes-40}
\therefore \; \Right{\Veim{down}} \neq \Right{\Veim{rd}}
\end{gather}
\eqref{eq:silo-causes-40} shows that \Veim{down}
cannot be a silo cause of \Veim{rd}.

\textbf{Prediction EIM}:
By Theorem \ref{t:quasi-drs-disjoint},
we see that a prediction is never quasi-complete,
and therefore can never be a silo layer.

\textbf{Start EIM}:
By its definition, the start EIM has a telluric
symbol in its dot suffix,
and therefore is not quasi-complete.
For this reason,
the start EIM is never a silo layer.
\end{proof}

\begin{lemma}
\ttitle{Read EIM silo layer properties}
\label{t:silo-read-eim-layer}
Let a silo layer, call it \Vel{silo}{i}, be a read EIM.
Then
\begin{gather}
\label{eq:silo-read-eim-layer-1}
\var{i} = 0,
\\
\label{eq:silo-read-eim-layer-2}
\nexists \;
\Vel{silo}{j} :
\var{j} < \var{i}, \quad \text{and}
\\
\label{eq:silo-read-eim-layer-3}
\Vel{silo}{i} = \Vel{silo}{j} \implies \var{i} = \var{j}.
\end{gather}
\end{lemma}

\begin{proof}
Let \Vel{silo}{x} be a read EIM.
By Theorem
\ref{t:silo-causes},
it has no causes,
and therefore by the definition of
a silo,
there is no silo layer
\Vel{silo}{\Vdecr{x}}.
Our indexing convention is that 0 is the
numerically the lowest index of the silo.
This shows
\eqref{eq:silo-read-eim-layer-1}.

Recall that
silo layer indexes are
consecutive integers starting at zero.
\eqref{eq:silo-read-eim-layer-2}
follows from this observation
and \eqref{eq:silo-read-eim-layer-1}.

It remains to show
\eqref{eq:silo-read-eim-layer-3}
in the statement of the theorem.
Suppose for a reductio
that there are two silo layers,
\Vel{silo}{x}
and
\Vel{silo}{y},
such that
\begin{equation}
\label{eq:silo-read-eim-layer-20}
\tag{RAA}
\begin{gathered}
\Vel{silo}{x} = \Vel{silo}{y} \; \land \; \var{x} \neq \var{y}, \\
\text{where \Vel{silo}{x} is a read EIM.}
\end{gathered}
\end{equation}
By
\eqref{eq:silo-read-eim-layer-20},
the assumption for the reductio,
$\Vel{silo}{x} = \Vel{silo}{y}$,
so that
\begin{equation}
\label{eq:silo-read-eim-layer-25}
\text{\Vel{silo}{y} is a read EIM.}
\end{equation}

Above, we showed
\eqref{eq:silo-read-eim-layer-1},
so that it is available to show
\eqref{eq:silo-read-eim-layer-3}.
\begin{align}
\label{eq:silo-read-eim-layer-28}
& \var{x} = 0 &&
\eqref{eq:silo-read-eim-layer-1},
\eqref{eq:silo-read-eim-layer-20}
\\
\label{eq:silo-read-eim-layer-31}
& \var{y} = 0 &&
\eqref{eq:silo-read-eim-layer-1},
\eqref{eq:silo-read-eim-layer-25}
\\
\label{eq:silo-read-eim-layer-34}
& \var{x} = \var{y} &&
\eqref{eq:silo-read-eim-layer-28},
\eqref{eq:silo-read-eim-layer-31}
\end{align}
\eqref{eq:silo-read-eim-layer-34}
contradicts
\eqref{eq:silo-read-eim-layer-20}
and shows the reductio.
This in turn shows
\eqref{eq:silo-read-eim-layer-3},
which was the last requirement for the theorem.
\end{proof}

\begin{theorem}
\ttitle{Silo layer telluric prefix}
\label{t:silo-telluric-prefix}
Every layer of a silo at \Vloc{k} has a telluric symbol before
\Vloc{k}.
\end{theorem}

\begin{proof}
A silo layer must be quasi-complete,
so that there must be a telluric symbol before \Vloc{k}.
\end{proof}

\begin{theorem}
\ttitle{No location 0 silo}
\label{t:no-silo-at-0}
There are no silos at location 0.
\end{theorem}

\begin{proof}
From theorem \ref{t:silo-telluric-prefix},
there must be a telluric symbol before \Vloc{k}.
Therefore, without loss of generality, a silo layer is
\begin{equation}
\label{eq:no-silo-at-0-10}
\big[ [ \Vsym{A} \de \Vstr{fore} \Vsym{tell} \mydot \Vstr{suffix} ], \Vloc{i}, \Vloc{k} \big]
\end{equation}
for some \Vloc{i},
and some rule
\[
  [ \Vsym{A} \de \Vstr{fore} \Vsym{tell} \Vstr{suffix} ] \in \Crules
\]
where \Vsym{tell} is telluric.
By the definition of EIM validity, we know that
\begin{align}
& \Vmk{i} \Vstr{fore} \Vsym{tell} \Vmk{k} &&
  \text{\eqref{eq:no-silo-at-0-10}}
\notag
\\
\label{eq:no-silo-at-0-16}
\therefore \quad & \var{k} \ge \var{i} + \Vsize{\Vsym{tell}} &&
\\
\label{eq:no-silo-at-0-17}
  & \Vsize{\Vsym{tell}} \ge 1 &&
\text{\Vsym{tell} is telluric}
\intertext{and because parse locations are non-negative integers,}
& \var{i} \ge 0  &&
\notag
\\
\therefore \quad & \var{k} \ge 0 + \Vsize{\Vsym{tell}} &&
\text{\eqref{eq:no-silo-at-0-16}}
\notag
\\
\therefore \quad & \var{k} \ge 0 + 1 &&
\text{\eqref{eq:no-silo-at-0-17}}
\notag
\\
\label{eq:no-silo-at-0-27}
\therefore \quad & \Vloc{k} \ge 1 &&
\end{align}
\eqref{eq:no-silo-at-0-10} was stated without loss of generality,
so that
\eqref{eq:no-silo-at-0-27} shows the theorem.
\end{proof}

\begin{theorem}
\ttitle{Silo layer predot}
\label{t:silo-predot}
Let \Vloc{k} be the location of a silo.
Every layer of the silo has a symbol before \Vloc{k}.
\end{theorem}

\begin{proof}
This theorem follows
directly from Theorem
\ref{t:silo-telluric-prefix}.
\end{proof}

\begin{lemma}

\label{lem:silo-reflection-nt}
Let \el{silo}{\Vincr{i}},
\Vel{silo}{i},
be two silo layers.
\begin{multline}
\label{eq:lem-silo-reflection-nt-3}
\text{If \Predot{\el{silo}{\Vincr{i}}} is a non-terminal,}
\\
\text{then
$\op{Valid-eq}{\el{silo}{\Vincr{i}}} \xderives{1} \op{Valid-eq}{\el{silo}{\var{i}}}.$}
\end{multline}
\end{lemma}

\begin{proof}
Without loss of generality, we let
\begin{equation}
\label{eq:lem-silo-reflection-nt-10}
\el{silo}{\Vincr{i}}
= \bigl[ [\Vsym{A} \de \Vstr{pre-B} \Vsym{B} \mydot \Vstr{post-B}], \var{a}, \var{c} \bigr].
\end{equation}
By assumption for
\eqref{eq:lem-silo-reflection-nt-3},
we have that
\begin{equation}
\label{eq:lem-silo-reflection-nt-11}
\text{\Vsym{B} is a non-terminal.}
\end{equation}

Since \Vsym{B} is a non-terminal,
we know from Theorem \ref{t:silo-causes},
that \Vel{silo}{i}, its silo cause,
is its bottom-up cause.
From Theorem \ref{t:symbolic-causes-from-effect}, we know
that the bottom-up cause of
\eqref{eq:lem-silo-reflection-nt-10}
is $\Vmk{b} \Vsym{B} \Vmk{c}$ for some \Vloc{b}.
Since \Vsym{B} is a non-terminal, by
the definition of validity for a non-terminal symbol instance,
we know that
it has an equivalent EIM which is,
without loss of generality,
\begin{equation}
\label{eq:lem-silo-reflection-nt-12}
\Vel{silo}{i} = \bigl[[\Vsym{B} \de \Vstr{B-rhs} \mydot ], \var{b}, \var{c} \bigr].
\end{equation}
The validity equivalent of
\eqref{eq:lem-silo-reflection-nt-10}
is
\begin{equation}
\label{eq:lem-silo-reflection-nt-14}
\Vsym{A} \derives \Vmk{a} \Vstr{pre-B} \Vsym{B} \Vmk{c} \Vstr{post-B}.
\end{equation}
and
the validity equivalent of
\eqref{eq:lem-silo-reflection-nt-12}
is
\begin{equation}
\label{eq:lem-silo-reflection-nt-16}
\Vsym{B} \derives \Vmk{b} \Vstr{B-rhs} \Vmk{c}.
\end{equation}
Combining
\eqref{eq:lem-silo-reflection-nt-14}
and
\eqref{eq:lem-silo-reflection-nt-16},
we have
\begin{equation}
\label{eq:lem-silo-reflection-nt-20}
\begin{aligned}
& \Vsym{A}
  && \\
\derives \; & \Vmk{a} \Vstr{pre-B} \Vsym{B} \Vmk{c} \Vstr{post-B}
  && \text{Step \var{s}} \\
\derives \; & \ldots \Vmk{b} \Vstr{B-rhs} \Vmk{c} \ldots
  && \text{Step \Vincr{s}},
\end{aligned}
\end{equation}
where we have added step numbers for convenience,
and without loss of generalization.
Recall that our convention is to say that the derivation step of an EIM
layer is the second step of its derivation move,
so that the derivation step for
\eqref{eq:lem-silo-reflection-nt-10} is at
Step \var{s},
and the derivation step for
\eqref{eq:lem-silo-reflection-nt-12} is at
Step \Vincr{s}.
Clearly,
Step \var{s} derives Step \Vincr{s} in one step,
which shows \eqref{eq:lem-silo-reflection-nt-3}.
\end{proof}

\begin{lemma}
\label{lem:silo-reflection-nulling1}
Let \el{silo}{\Vincr{i}},
\Vel{silo}{i},
be two silo layers.
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-6}
\begin{aligned}
\text{If $\op{Valid-eq}{\el{silo}{\Vincr{i}}} \xderives{0} \op{Valid-eq}{\el{silo}{\var{i}}}$},
\\
\qquad \text{then \Predot{\el{silo}{\Vincr{i}}} is a nulling terminal.}
\end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
Without loss of generality, we let
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-10}
\el{silo}{\Vincr{i}}
= \bigl[ [\Vsym{A} \de \Vstr{pre-B} \Vsym{B} \mydot \Vstr{post-B}], \var{a}, \var{c} \bigr].
\end{equation}
The validity equivalent of
\eqref{eq:lem-silo-reflection-nulling1-10}
is
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-10a}
\Vsym{A} \derives \Vmk{a} \Vstr{pre-B} \Vsym{B} \Vmk{c} \Vstr{post-B}.
\end{equation}

We assume
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-11}
\text{\Vsym{B} is a non-terminal.}
\end{equation}
for a reductio.
If \Vsym{B} is a non-terminal, by
the definition of validity for a non-terminal symbol instance,
we know that its parse instance has an EIM equivalent,
so that the bottom-up cause
of \eqref{eq:lem-silo-reflection-nulling1-10}
is, without loss of generality,
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-12}
\Vel{silo}{i} = \bigl[[\Vsym{B} \de \Vstr{B-rhs} \mydot ], \var{b}, \var{c} \bigr].
\end{equation}
The validity equivalent
of \eqref{eq:lem-silo-reflection-nulling1-12}
is
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-16}
\Vsym{B} \derives \Vmk{b} \Vstr{B-rhs} \Vmk{c},
\end{equation}
which implies that
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-38}
\text{$[ \Vsym{B} \de \Vstr{B-rhs} ]$ is a rule of \Cg.}
\end{equation}

We now combine
\eqref{eq:lem-silo-reflection-nulling1-10a}
and
\eqref{eq:lem-silo-reflection-nulling1-16}.
In case nulling symbols are significant,
we avoid
the use of location markers or
simplifications.
\begin{align}
\notag
& \Vsym{A} \\
\label{eq:lem-silo-reflection-nulling1-42}
\derives \; & \Vstr{pre-B} \Vsym{B} \Vstr{post-B} \\
\label{eq:lem-silo-reflection-nulling1-44}
\xderives{0} \; & \Vstr{pre-B} \Vstr{B-rhs} \Vstr{post-B}
\end{align}
Notice that the derivation from
\eqref{eq:lem-silo-reflection-nulling1-42}
to
\eqref{eq:lem-silo-reflection-nulling1-44}
takes place in zero steps,
as required by assumption for
\eqref{eq:lem-silo-reflection-nulling1-6}.
Following the implications,
\begin{align}
&
\begin{multlined}
\Vstr{pre-B} \Vsym{B} \Vstr{post-B} \\
\xderives{0} \Vstr{pre-B} \Vstr{B-rhs} \Vstr{post-B}
\end{multlined}
&&
\eqref{eq:lem-silo-reflection-nulling1-42},
\eqref{eq:lem-silo-reflection-nulling1-44}
\\
\therefore \quad &
\begin{multlined}
\Vstr{pre-B} \Vsym{B} \Vstr{post-B} \\
= \Vstr{pre-B} \Vstr{B-rhs} \Vstr{post-B}
\end{multlined}
&&
\\
\label{eq:lem-silo-reflection-nulling1-49}
\therefore \quad &
\Vsym{B} = \Vstr{B-rhs}
&&
\\
\label{eq:lem-silo-reflection-nulling1-50}
\therefore \quad &
\text{$[ \Vsym{B} \de \Vsym{B} ]$ is a rule of \Cg.}
&&
\eqref{eq:lem-silo-reflection-nulling1-38},
\eqref{eq:lem-silo-reflection-nulling1-49}
\end{align}
But the rule of
\eqref{eq:lem-silo-reflection-nulling1-50}
is a cycle and cycles are not allowed in Marpa grammars.
This concludes the reductio.
Our assumption for the reductio was
\eqref{eq:lem-silo-reflection-nulling1-11},
so we conclude that
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-52}
\text{\Vsym{B} is a terminal.}
\end{equation}

We have shown that \Vsym{B} is a terminal,
but it
remains to show that \Vsym{B} is a nulling terminal.
We assume for a reductio that \Vsym{B} is a telluric
terminal.
From
\eqref{eq:lem-silo-reflection-nulling1-10},
we see, if \Vsym{B}
is a telluric terminal,
\el{silo}{\Vincr{i}} is a read EIM.
But by Theorem
\ref{t:silo-causes},
we know that a read EIM has no causes.
If \el{silo}{\Vincr{i}} has no causes,
by the definition of a silo,
there is no
silo element \Vel{silo}{i}.
But, by assumption
for the lemma,
there is a
silo element \Vel{silo}{i}.
This contradiction shows the reductio,
and we conclude that
\begin{equation}
\label{eq:lem-silo-reflection-nulling1-53}
\text{\Vsym{B} is not a telluric terminal.}
\end{equation}

From Theorem
\ref{eq:lem-silo-reflection-nulling1-10},
we know that
$\Vsym{B} = \Predot{\el{silo}{\Vincr{i}}}$.
Using
\eqref{eq:lem-silo-reflection-nulling1-52}
and
\eqref{eq:lem-silo-reflection-nulling1-53}
we conclude that
\begin{equation}
\text{\Predot{\el{silo}{\Vincr{i}}} is a nulling terminal,}
\end{equation}
which shows the theorem.
\end{proof}

\begin{lemma}
\label{lem:silo-reflection-nulling-induction}
\begin{multline}
\label{eq:lem-silo-reflection-nulling-induction-10}
\text{If $\op{Valid-eq}{\Vel{silo}{hi}} \xderives{0} \op{Valid-eq}{\Vel{silo}{lo}}$,} \\
\text{then for all \var{a},} \quad
\var{hi} \ge \var{a} > \var{lo} \implies \Predot{\Vel{silo}{a}} = \epsilon.
\end{multline}
\end{lemma}

\begin{proof}
We proceed in two cases, depending on the relative
values of \var{lo} and \var{lo}.
We take as our first case,
$\var{lo} \ge \var{hi}$.
For this case
\eqref{eq:lem-silo-reflection-nulling-induction-10}
follows vacuously.

The second case remains:
$\var{lo} < \var{hi}$.
For it
we proceed by induction,
taking as the hypothesis:
\begin{multline}
\label{eq:lem-silo-reflection-nulling-induction-20}
\tag{IND}
\text{If $\op{Valid-eq}{\Vel{silo}{hi}} \xderives{0} \op{Valid-eq}{\Vel{silo}{x}}$,} \\
\text{then for all \var{a},} \quad
\var{hi} \ge \var{a} > \var{x} \implies \Predot{\Vel{silo}{a}} = \epsilon.
\end{multline}
\eqref{eq:lem-silo-reflection-nulling-induction-20}
for $\var{x} = \Vdecr{hi}$
follows from
Lemma \ref{lem:silo-reflection-nulling1},
and we take this as the basis of the induction.

For the step,
we assume
\eqref{eq:lem-silo-reflection-nulling-induction-20}
for $\var{x} = \var{i}$
to show
\eqref{eq:lem-silo-reflection-nulling-induction-20}
for $\var{x} = \Vdecr{i}$.
The assumption for the step is
\begin{multline}
\label{eq:lem-silo-reflection-nulling-induction-30}
\text{If} \quad
\op{Valid-eq}{\Vel{silo}{hi}} \xderives{0} \op{Valid-eq}{\Vel{silo}{i}}
\\
\text{then} \quad
\forall \var{a} : \var{hi} \ge \var{a} > \var{i}
\implies
\text{\Predot{\Vel{silo}{a}} is nulling.}
\end{multline}
Lemma \ref{lem:silo-reflection-nulling1}
shows us that
\begin{multline}
\label{eq:lem-silo-reflection-nulling-induction-33}
\text{if} \quad
\op{Valid-eq}{\Vel{silo}{i}}
\xderives{0}
\op{Valid-eq}{\el{silo}{\Vdecr{i}}}
\\
\shoveleft{
\qquad \text{then \Predot{\Vel{silo}{i}}
is nulling.}
}
\end{multline}
Combining
\eqref{eq:lem-silo-reflection-nulling-induction-30}
and
\eqref{eq:lem-silo-reflection-nulling-induction-33},
we have
\begin{multline}
\label{eq:lem-silo-reflection-nulling-induction-40}
\text{if} \quad
\op{Valid-eq}{\Vel{silo}{hi}} \xderives{0} \op{Valid-eq}{\el{silo}{\Vdecr{i}}}
\\
\shoveleft{
\text{then} \quad
\forall \var{a} : \var{hi} \ge \var{a} > \Vdecr{i}
\implies
\text{\Predot{\Vel{silo}{a}} is nulling.}
}
\end{multline}
\eqref{eq:lem-silo-reflection-nulling-induction-40}
is
\eqref{eq:lem-silo-reflection-nulling-induction-20}
for $\var{x} = \Vdecr{i}$,
which is what we needed to show for the step.
With this we have the induction,
and
the induction shows
\eqref{eq:lem-silo-reflection-nulling-induction-10}
for $\var{lo} < \var{hi}$.
This was our second case, and with it we
have the theorem.
\end{proof}

\begin{lemma}
\label{lem:silo-reflection-nulling}
Let \el{silo}{\Vincr{i}},
\Vel{silo}{i},
be two silo layers.
\begin{multline*}
\Predot{\el{silo}{\Vincr{i}}} = \epsilon \\
\implies
\op{Valid-eq}{\el{silo}{\Vincr{i}}} \xderives{0} \op{Valid-eq}{\el{silo}{\var{i}}}.
\end{multline*}
\end{lemma}

\begin{proof}
Without loss of generality, we let
\begin{equation}
\label{eq:lem-silo-reflection-nulling-10}
\el{silo}{\Vincr{i}} = \bigl[ [\Vsym{A} \de \Vstr{pre-B} \Vsym{B} \mydot \Vstr{post-B}], \var{a}, \var{c} \bigr],
\end{equation}
where, by assumption for the theorem,
\Vsym{B} is nulling.

Since \Vsym{B} is nulling,
we know from Theorem \ref{t:silo-causes},
that \Vel{silo}{i}, its silo cause,
is its top-down cause.
From
Definition \ref{def:causes},
the definition of causes,
we know that
the top-down cause of
\eqref{eq:lem-silo-reflection-nulling-10}
is
\begin{equation}
\label{eq:lem-silo-reflection-nulling-12}
\el{silo}{\var{i}} = \bigl[ [\Vsym{A} \de \Vstr{pre-B} \mydot \Vsym{B} \Vstr{post-B}], \var{a}, \var{c} \bigr],
\end{equation}
The validity equivalent of
\eqref{eq:lem-silo-reflection-nulling-10}
is
\begin{equation}
\label{eq:lem-silo-reflection-nulling-14}
\op{Valid-eq}{\el{silo}{\Vincr{i}}} =
\Vsym{A} \derives \Vmk{a} \Vstr{pre-B} \Vsym{B} \Vmk{c} \Vstr{post-B}.
\end{equation}
and
the validity equivalent of
\eqref{eq:lem-silo-reflection-nulling-12}
is
\begin{equation}
\label{eq:lem-silo-reflection-nulling-16}
\op{Valid-eq}{\el{silo}{\var{i}}} =
\Vsym{A} \derives \Vmk{a} \Vstr{pre-B} \Vmk{c} \Vsym{B} \Vstr{post-B}.
\end{equation}
But since $\Vsym{B} \derives \epsilon$, we know that both
\eqref{eq:lem-silo-reflection-nulling-14}
and \eqref{eq:lem-silo-reflection-nulling-16}
are equivalent to
\begin{equation}
\label{eq:lem-silo-reflection-nulling-19}
\Vsym{A} \derives \Vmk{a} \Vstr{pre-B} \Vmk{c} \Vsym{B} \Vmk{c} \Vstr{post-B}.
\end{equation}
so that
\begin{equation}
\op{Valid-eq}{\el{silo}{\Vincr{i}}} \xderives{0}
\op{Valid-eq}{\el{silo}{\var{i}}}.
\qedhere
\end{equation}
\end{proof}

We next show that, for every silo,
there is a derivation which
\mbox{``reflects''} % do not hyphenate
it.

\begin{theorem}
\ttitle{Silo reflection}
\label{t:silo-reflection}
Let \el{silo}{\var{hi}} and
\Vel{silo}{lo}
be two silo layers,
such that $\var{lo} \le \var{hi}$.
Then
\begin{equation}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\Vel{silo}{lo}}.
\end{equation}
\end{theorem}

\begin{proof}
The proof is by induction, where the induction
hypothesis is
\begin{equation}
\tag{IND}
\label{eq:silo-reflection-10}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\Vel{silo}{x}}.
\end{equation}
The derivation equivalent of any silo layer derives itself,
so we have
\eqref{eq:silo-reflection-10} for
$\var{x} = \var{hi}$ trivially.
We take this as the basis of our induction.

For the step of the induction we assume
\eqref{eq:silo-reflection-10}
for $\var{x} = \var{i}$ to show
\eqref{eq:silo-reflection-10}
for $\var{x} = \Vdecr{i}$.
The assumption for the step is
\begin{equation}
\tag{STEP}
\label{eq:silo-reflection-20}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\Vel{silo}{i}}.
\end{equation}
By Theorem
\ref{t:silo-predot},
we know that every layer of \var{silo} has a predot symbol.
We show the step by cases, according to whether the predot symbol
of \Vel{silo}{i} is
a telluric terminal,
a nulling terminal,
or a non-terminal.

\textbf{Telluric terminal}:
We first consider the case
where
\begin{equation*}
\text{\Predot{\Vel{silo}{i}} is a telluric terminal}.
\end{equation*}
By Theorem \ref{t:silo-causes},
a silo layer with a telluric terminal predot symbol
has no silo causes.
Therefore there is no \el{silo}{\Vdecr{i}}.
This gives us
\eqref{eq:silo-reflection-10} for \Vdecr{i}
vacuously for the case of a telluric terminal.

\textbf{Nulling terminal}:
We next consider the case where
\begin{equation}
\label{eq:silo-reflection-17}
\Predot{\Vel{silo}{i}} = \epsilon.
\end{equation}
From
\eqref{eq:silo-reflection-17}
and Lemma \ref{lem:silo-reflection-nulling}, we know that
\begin{equation}
\label{eq:silo-reflection-22}
\op{Valid-eq}{\Vel{silo}{i}}
\xderives{0}
\op{Valid-eq}{\el{silo}{\Vdecr{i}}}.
\end{equation}
Combining
\eqref{eq:silo-reflection-20}
and
\eqref{eq:silo-reflection-22},
we have
\begin{equation}
\label{eq:silo-reflection-24}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\el{silo}{\Vdecr{i}}}
\end{equation}
\eqref{eq:silo-reflection-24} is
\eqref{eq:silo-reflection-10} for \Vincr{i},
giving us the case of a nulling terminal.

\textbf{Non-terminal}:
We now consider the case where
\begin{equation}
\label{eq:silo-reflection-37}
\text{\Predot{\Vel{silo}{i}} is a non-terminal}.
\end{equation}
From
\eqref{eq:silo-reflection-37}
and Lemma \ref{lem:silo-reflection-nt}, we know that
\begin{equation}
\label{eq:silo-reflection-42}
\op{Valid-eq}{\Vel{silo}{i}}
\xderives{1}
\op{Valid-eq}{\el{silo}{\Vdecr{i}}}.
\end{equation}
Combining
\eqref{eq:silo-reflection-20}
and
\eqref{eq:silo-reflection-42},
we have
\begin{equation}
\label{eq:silo-reflection-44}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\el{silo}{\Vdecr{i}}}
\end{equation}
\eqref{eq:silo-reflection-44} is
\eqref{eq:silo-reflection-10} for \Vincr{i},
giving us the case of a non-terminal.
This gives us the last of our three cases,
the step of the induction,
and the theorem.
\end{proof}

\begin{theorem}
\ttitle{Silo layer is unique}
\label{t:silo-layer-unique}
No layer of a silo occurs twice.
That is, if \var{silo} is a silo,
\begin{equation}
\forall \; \var{i}, \var{j}, \Vel{silo}{i} = \Vel{silo}{j} \implies \var{i} = \var{j}
\end{equation}
\end{theorem}

\begin{proof}
Let \var{silo},
be a silo,
and \var{hi}, \var{lo}
be two layers in it.
Without loss of generalization,
let $\var{hi} \ge \var{lo}$.
We assume for a reductio that
\begin{equation}
\tag{RAA}
\label{eq:silo-layer-unique-10}
\text{$\Vel{silo}{hi} = \Vel{silo}{lo}$ and $\var{hi} \neq \var{lo}$}
\end{equation}
From Theorem
\ref{t:silo-reflection},
we know that
\begin{equation}
\label{eq:silo-layer-unique-20}
\op{Valid-eq}{\Vel{silo}{hi}} \destar
\op{Valid-eq}{\Vel{silo}{lo}}.
\end{equation}

Even when two silo layers are different,
their validity equivalents may
may derive each other
in zero steps ---
in other words, they may be equal to each other.
We call a derivation in zero steps, ``trivial''.
We now proceed by cases: trivial derivations,
and non-trivial derivations.

\textbf{Trivial derivation}:
We assume for an inner reductio that
\begin{equation}
\tag{RAA1}
\label{eq:silo-layer-unique-30}
\op{Valid-eq}{\Vel{silo}{hi}} \xderives{0}
\op{Valid-eq}{\Vel{silo}{lo}}
\end{equation}

From
\eqref{eq:silo-layer-unique-30}
and
Lemma
\ref{lem:silo-reflection-nulling-induction},
we have
\begin{align}
\label{eq:silo-layer-unique-40}
& \Predot{\Vel{silo}{hi}} = \epsilon
&&
\\
\label{eq:silo-layer-unique-43}
& \begin{multlined}
\text{silo cause of \Vel{silo}{hi}} \\
\text{is top-down cause}
\end{multlined}
&& \text{Th \ref{t:silo-causes}},
\eqref{eq:silo-layer-unique-40}
\\
\label{eq:silo-layer-unique-46}
&
\begin{multlined}
\Vel{silo}{hi} = \\
  \big[ [ \Vrule{r}, \var{ix} ], \Vorig{hi}, \Vloc{here} ]
\end{multlined}
&&
\text{WLOG}
\intertext{Note
  that in \eqref{eq:silo-layer-unique-46}, we are using our alternate notation
  for dotted rules, where dotted rule is a duple of rule and RHS index.
  From \eqref{eq:silo-layer-unique-40}, \eqref{eq:silo-layer-unique-46}
  and the definition of top-down
  cause for a null-scan, we have
}
\label{eq:silo-layer-unique-49}
& \begin{multlined}
\text{top-down cause of \Vel{silo}{hi} is} \\
\big[ [ \Vrule{r}, \Vdecr{ix} ], \Vorig{hi}, \Vloc{here} \big]
\end{multlined}
&&
\\
\label{eq:silo-layer-unique-52}
& \begin{multlined}
\el{silo}{\Vdecr{hi}} = \\
\big[ [ \Vrule{r}, \Vdecr{ix} ], \Vorig{hi}, \Vloc{here} \big]
\end{multlined}
&& \eqref{eq:silo-layer-unique-43}, \eqref{eq:silo-layer-unique-49}
\end{align}
For this proof, let the function \myfn{TDC}{\Veim{eim}} be the
top-down cause of \Veim{eim} for a null-scan,
if \Predot{\Veim{eim}} is nulling,
and undefined, otherwise.
From this definition of \myfnname{TDC} and
\eqref{eq:silo-layer-unique-52}, we have
\begin{multline}
\label{eq:silo-layer-unique-55}
\Predot{\Vel{silo}{hi}} = \epsilon  \\
\implies
\myfn{TDC}{\Vel{silo}{hi}} =
\big[ [ \Vrule{r}, \Vdecr{ix} ], \Vorig{hi}, \Vloc{here} \big]
\end{multline}

By iteration of
\eqref{eq:silo-layer-unique-55},
we have
\begin{multline}
\label{eq:silo-layer-unique-58}
\text{if} \quad
\forall \var{a} :
\var{hi} \ge \var{a} > \var{lo} \implies
\Predot{\Vel{silo}{a}} = \epsilon, \\
\text{then} \quad
\Vel{silo}{hi} =
\myfnname{TDC}^{\displaystyle (\xxsubtract{\var{hi}}{\var{lo}})}(\Vel{silo}{hi}) = \\
\big[ [ \Vrule{r},
\xxsubtract{\var{ix}}{
  (\xxsubtract{\var{hi}}{\var{lo}})
}
], \Vorig{hi}, \Vloc{here} \big]
\end{multline}

From Lemma
\ref{lem:silo-reflection-nulling-induction}
and
\eqref{eq:silo-layer-unique-30}
we have
\begin{equation}
\label{eq:silo-layer-unique-61}
\forall \var{a} :
\var{hi} \ge \var{a} > \var{lo} \implies
\Predot{\Vel{silo}{a}} = \epsilon,
\end{equation}
so that we can rewrite
\eqref{eq:silo-layer-unique-58}
as
\begin{multline}
\label{eq:silo-layer-unique-64}
\Vel{silo}{lo} =
\myfnname{TDC}^{\displaystyle (\xxsubtract{\var{hi}}{\var{lo}})}(\Vel{silo}{hi}) = \\
\big[ [ \Vrule{r},
\xxsubtract{\var{ix}}{
  (\xxsubtract{\var{hi}}{\var{lo}})
}
], \Vorig{hi}, \Vloc{here} \big].
\end{multline}

From
\eqref{eq:silo-layer-unique-46}
and
\eqref{eq:silo-layer-unique-64},
we have
\begin{equation}
\label{eq:silo-layer-unique-70}
\Vel{silo}{hi} = \Vel{silo}{lo} \implies \var{hi} = \var{lo}.
\end{equation}

\eqref{eq:silo-layer-unique-70}
contradicts
\eqref{eq:silo-layer-unique-10},
our assumption for the outer reductio.
This shows the inner reductio
and the case of trivial derivations.

\textbf{Non-trivial derivations}:
We next consider the case of non-trivial derivations:
\begin{equation}
\label{eq:silo-layer-unique-76}
\op{Valid-eq}{\Vel{silo}{hi}} \deplus
\op{Valid-eq}{\Vel{silo}{lo}}
\end{equation}

From \eqref{eq:silo-layer-unique-10},
we have that
$\Vel{silo}{hi} = \Vel{silo}{lo}.$
Without loss of generality, let
both
\Vel{silo}{hi} and \Vel{silo}{lo}
be
\begin{equation}
\label{eq:silo-layer-unique-85}
\big[ [ \Vsym{A} \de \Vstr{pre} \mydot \Vstr{post} ], \var{i}, \var{k} \big].
\end{equation}
so that their validity equivalents are
\begin{equation}
\label{eq:silo-layer-unique-88}
\Vsym{A} \derives \Vmk{i} \Vstr{pre} \Vmk{k} \Vstr{post}
\end{equation}

The dotted rule in a silo layer must be quasi-complete,
so that $\Vstr{post} = \epsilon$.
Therefore we can use
\eqref{eq:silo-layer-unique-88}
to write
\eqref{eq:silo-layer-unique-76}
as
\begin{equation}
\begin{aligned}
\label{eq:silo-layer-unique-93}
\Vmk{i} \Vsym{A} \Vmk{k} \derives & \; \Vmk{i} \Vsym{pre} \Vmk{k} \\
  \destar & \; \Vmk{i} \Vsym{A} \Vmk{k} \\
  \derives & \; \Vmk{i} \Vsym{pre} \Vmk{k} \\
\end{aligned}
\end{equation}
From
\eqref{eq:silo-layer-unique-93}
we have
\begin{gather}
\label{eq:silo-layer-unique-95a}
\Vmk{i} \Vsym{A} \Vmk{k} \deplus \Vmk{i} \Vsym{A} \Vmk{k} \\
\label{eq:silo-layer-unique-95b}
\text{and} \quad
 \Vmk{i} \Vsym{pre} \Vmk{k} \deplus \Vmk{i} \Vsym{pre} \Vmk{k}
\end{gather}
Using Theorem
\ref{t:location-marker-cycle},
we see that both
\eqref{eq:silo-layer-unique-95a}
and
\eqref{eq:silo-layer-unique-95b}
are cycles,
which are not
allowed in a Marpa grammar.
This contradicts
\eqref{eq:silo-layer-unique-10},
and shows the reductio for non-trivial
derivations.

We have already shown
the reductio for trivial
derivations.
We now have
both cases,
the reductio and theorem.
\end{proof}

\begin{theorem}
\ttitle{Silos are finite}
\label{t:silo-finite}
Every silo is of finite height.
\end{theorem}

\begin{proof}
By Theorem
\ref{t:es-count}, the number of Earley items
with a single right location is finite.
By Theorem
\ref{t:silo-location},
all the layers of a silo share the same right location.
So either a silo has finite length, or the same layer
occurs more than once in a silo.
But, by Theorem
\ref{t:silo-layer-unique},
no silo layer occurs
more than once in a silo.
\end{proof}

\begin{theorem}
\label{t:quasi-confirmed-eim-has-silo}
Every quasi-confirmed EIM has
a silo of finite length,
whose bottom is a read EIM.
\end{theorem}

\begin{proof}
Let,
without loss of generality,
\Veim{quasi} be a quasi-confirmed EIM.
By the definition of silo,
the sequence consisting only of 
\Veim{quasi} is a silo.
It remains to show that 
\Veim{quasi} has a silo of finite length
whose bottom is a read EIM.


By Theorem
\ref{t:silo-finite}
a silo must have a finite length.
Since every silo with \Veim{quasi} at the top
has a finite length,
there are one or more silos which have a maximum
length.
Assume, for a reductio,
that \Veim{bot} is the bottom of one of these maximum
length silos, and that it is not
a read EIM.
Since it is not a read EIM,
by Theorem \ref{t:silo-causes},
\Veim{bot} has a silo cause.
Therefore
a silo layer could be added below \Veim{bot}.
But if a silo layer could be added below \Veim{bot},
then \Veim{bot} is not at the bottom of one of the silos
of maximum length,
which is contrary to assumption for the reductio.
We conclude that \Veim{bot} is a read EIM.
\end{proof}


\begin{theorem}
\ttitle{Quasi-completion silo}
\label{t:quasi-completion-silo}
Let \Veim{qc} be a valid quasi-completion.
Then
\begin{align}
\label{eq:quasi-completion-silo-5}
& \text{\Veim{qc} is the bottom of a silo,
  call it \var{slo}
}
\\
\label{eq:quasi-completion-silo-9}
& \text{\var{slo} contains at most \Vsize{\Vrule{r}}
  elements
}
\\
\label{eq:quasi-completion-silo-10}
& \text{$\var{slo}\big[ \Vlastix{ns} \big]$
  is a completion
}
\\
\label{eq:quasi-completion-silo-12}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} < \Vlastix{slo} \\
& \quad \implies \text{\Vel{slo}{a} is an incomplete EIM}
\end{aligned}
\\
\label{eq:quasi-completion-silo-13}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{slo} \\
& \quad \implies \text{\Vel{slo}{a} is a quasi-complete EIM}
\end{aligned}
\\
\label{eq:quasi-completion-silo-14}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{slo}
  \implies \Valid{\Vel{slo}{a}}
\end{aligned}
\\
\label{eq:quasi-completion-silo-16}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{slo} \\
& \qquad \implies \op{Right}{\el{slo}{0}} = \op{Right}{\Vel{slo}{a}}
\end{aligned}
\\
\label{eq:quasi-completion-silo-18}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \Vlastix{slo} \\
& \qquad \implies \op{Rule}{\el{slo}{0}} = \op{Rule}{\Vel{slo}{a}}
\end{aligned}
\\
\label{eq:quasi-completion-silo-20}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \decr{\Vlastix{slo}} \\
& \qquad \implies \text{\Vel{ns}{a} is the top-down cause of
	\el{ns}{\Vincr{a}}
}
\end{aligned}
\\
\label{eq:quasi-completion-silo-21}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \decr{\Vlastix{slo}} \\
& \qquad \implies \text{\Vel{ns}{a} is the silo cause of
	\el{ns}{\Vincr{a}}
}
\end{aligned}
\\
\label{eq:quasi-completion-silo-22}
& \begin{aligned}
& \forall \var{a} : 0 \le \var{a} \le \decr{\Vlastix{slo}} \\
& \qquad \implies \text{\el{slo}{\Vincr{a}}
	is the unique effect of \Vel{slo}{a}
}
\end{aligned}
\end{align}
\end{theorem}

\begin{proof}
Requirement of the theorem
\eqref{eq:quasi-completion-silo-5}
follows from
\begin{itemize}
\item
the assumption for the theorem that \el{slo}{0} is quasi-complete;
\item
\eqref{eq:iterative-completion-from-null-scan-cause-13},
\eqref{eq:iterative-completion-from-null-scan-cause-16}
and
\eqref{eq:iterative-completion-from-null-scan-cause-20}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
and
\item
the definition of a silo;
\end{itemize}

For the remaining requirements,
\begin{itemize}
\item
Requirement
\eqref{eq:quasi-completion-silo-9}
of this theorem follows from
\eqref{eq:iterative-completion-from-null-scan-cause-9}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause}.
\item
Requirement
\eqref{eq:quasi-completion-silo-10}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-10}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-12}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-12}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-13}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-13}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-14}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-14}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-16}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-16}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-18}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-18}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-20}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-20}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause};
\item
Requirement
\eqref{eq:quasi-completion-silo-21} follows from
\eqref{eq:quasi-completion-silo-13},
\eqref{eq:quasi-completion-silo-16}
and
\eqref{eq:quasi-completion-silo-20};
\item
Requirement
\eqref{eq:quasi-completion-silo-22}
follows from
\eqref{eq:iterative-completion-from-null-scan-cause-22}
of Theorem
\ref{t:iterative-completion-from-null-scan-cause}.
\qedhere
\end{itemize}
\end{proof}

\chapter{Leo memos}
\label{ch:leo}

\section{The history of a conjecture}

Jay Earley~\cite[p. 60]{Earley1968}%
\index{recce-general}{Earley, Jay}
conjectured that \Earley{} could be
modified to be \On{} for
all deterministic context-free grammars (DCFG's).
(The DCFG's are the union of the
\LRk{} grammars for all \var{k}.)
He gave no details of the method he had in mind.
Earley left the field shortly thereafter ---
by 1973 he had earned a second Ph.D. and
was a practicing psychotherapist in California.

In the late 1980's, Joop Leo%
\index{recce-general}{Leo, Joop}
also conjectured that
Earley's algorithm could be modified to be linear for all
DCFG's.
Leo was initially unaware of Earley's earlier conjecture ---
he discovered it in the course of writing up his
discovery.
Leo published his method in~\cite{Leo1991}.

The problem both invesigators noticed was that,
while \Earley{} is \On{}
for left recursion,
and in fact very efficient,
it is $\order{\var{n}^2}$ for right recursion.
This is because all the EIM's necessary for a right
recursion are created at every parse location
where the right recursion might end.

For simplicity,
in the discussion of this section,
we will assume that the grammar is unambiguous,
or at least that it is not ambiguous in a way that affects
the right recursion we are discussing.
We will call each potential end location of a right recursion,
an \dfn{EORR},
and will will call
the EIM's needed to represent the right recursion
at an EORR, an \dfn{EORR set}.

In \Earley,
if the length of the right
recursion at an EORR is \var{n}, then the EORR set contains \var{n} EIM's.
The total number of EIM's in the EORR sets
needed for a right recursion of length \var{n} is
\[
  \sum_{\var{i} = 1}^\var{n} \var{i} = \order{\var{n}^2}.
\]
Of these EIM's, only \var{n} will actually be needed ---
the rest are useless.

Leo's idea was to memoize the right recursions.
With Leo memoization, each EORR set is represented by
a pair of EIM's and a memo.
There are \Oc{} Leo memos per Earley set,
so the time and space complexity
of an EORR set is \Oc{}.
The time and space complexity of all the EORR
sets in
a right recursion of length \var{n} will
be
\[
  \sum_{\var{i} = 1}^\var{n} 1 = \On.
\]

If, at evaluation time,
it is desirable to expand the Leo memoizations,
only the EORR set actually used in the parse
needs to be expanded.
All of the EIM's actually used in a right recursion
will be in a single EORR set.
The number of memoized
EIM's that need to be expanded
will be \On{},
where \var{n} is the length of the recursion.
As a result,
even if the time and space
required to expand Leo memoization
during evaluation
is taken into account,
the time and space complexity of
a right recursion become
$\On{} = \On{} + \On{}$.

Joop Leo%
\index{recce-general}{Leo, Joop}
showed that,
with his modification, Earley's algorithm
is \On{} for all LR-regular grammars.
LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.
Earley did not claim \On{} for LR-regular
because LR-regular was not introduced
to the literature until 1973~\cite{Culik1973}.
Even in 1991, LR-regular was not well-known,
which is why Leo only claims
the weaker \LRk{} bound in
his title.

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this document Leo's ``transitive items''
will be called Leo memos.

Implementation of Leo memoization
will be considered in full detail,
and its correctness proved,
in Section \ref{ch:recce}.
In this chapter, we develop the
concepts behind Leo memoization.
We define, at an abstract level,
the data structures
that Leo memoization requires,
and we demonstrate their basic properties.

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this document Leo's ``transitive items''
will be called Leo memos.

\section{Definition}

In this document Leo's ``transitive items''
will be called Leo memos.
Leo memos have type \type{LEO}.
In each Earley set, there is at most one Leo memo per symbol.
A Leo memo is the 4-tuple
\begin{equation}
\label{eq:def-leo-memo-10}
\Vleo{leo} = [ \Vsym{transition}, \Vdr{top}, \Vorig{top}, \Vloc{memloc} ].
\end{equation}
For \Vleo{leo} as in
\eqref{eq:def-leo-memo-10},
we write
\begin{alignat*}{1}
\Symbol{\Vleo{leo}} & = \Vsym{transition}, \\
\DR{\Vleo{leo}} & = \Vdr{top}, \\
\Origin{\Vleo{leo}} & = \Vorig{top} \\
\text{and \Current{\Vleo{leo}}} & = \Vloc{memloc}.
\end{alignat*}

In \eqref{eq:def-leo-memo-10}
\Vsym{transition} is the
\dfn{Leo transition symbol}
or
\xdfn{transition symbol}{transition symbol (of a Leo memo)}.
\Vleo{leo} is considered to be located at \Vloc{memloc},
and \Vloc{memloc} is called the
\xdfn{current location}{current location (of a Leo memo)}
of \Vleo{leo}.
We will sometimes speak of \Vleo{leo} as being in the
Earley set at \Vloc{memloc},
although \Vleo{leo} is \textbf{not}
an Earley item,
and therefore, strictly speaking,
not in any Earley set.

Intuitively, \Vleo{leo},
the Leo memo of
\eqref{eq:def-leo-memo-10}
states an intent to memoize
certain layers of silos occuring after
\Current{\Vleo{leo}}.
The elements
\Symbol{\Vleo{leo}},
\DR{\Vleo{leo}},
and \Origin{\Vleo{leo}}
are
used to define which silo layers will be memoized.

\DR{\Vleo{leo}} must be a completion.
Since a completion is uniquely determined by a rule,
that element of the Leo memos could be represented by
a rule, instead of a dotted rule.
We will find working with the dotted rule
more convenient.

\section{Leo memo validity}

We now look at the conditions under which Leo memos become valid.
Recall that
we say that \Vdr{q} is a \dfn{quasi-penult}
if and only if it is
\begin{equation*}
\begin{split}
& \Vdr{q} = [ \Vsym{A} \de \Vstr{before} \mydot \Vsym{B} \cat \Vstr{after} ] \\
& \qquad \text{for some $[ \Vsym{A} \de \Vstr{before} \cat \Vsym{B} \cat \Vstr{after} ] \in \Crules$} \\
& \qquad \qquad \text{such that $\Vstr{after} \derives \epsilon$ and $\Vstr{B} \nderives \epsilon$}.
\end{split}
\end{equation*}
An EIM is \dfn{postdot-unique} if its postdot symbol
is unique in its Earley set.
That is,
\Veim{uniq} is \dfn{postdot-unique}
if and only if
for all \Veim{eim} in the parse,
\begin{multline}
\label{eq:def-postdot-unique}
\Postdot{\var{eim}} = \Postdot{\var{uniq}} \\
\implies  \var{eim} = \var{uniq}
\; \vee \;
 \Current{\var{eim}} \neq \Current{\var{uniq}}.
\end{multline}

An EIM can be the basis of a Leo memo
only if it is
Leo-eligible.
A rule is
\xdfn{Leo-eligible}{Leo-eligible (rule)}
if and only if it is right
recursive.
A dotted rule is
\xdfn{Leo-eligible}{Leo-eligible (dotted rule)}
if and only if
its rule is Leo-eligible,
and it is a quasi-penult.
An EIM is
\xdfn{Leo-eligible}{Leo-eligible (EIM)}
if and only if
its dotted rule is Leo-eligible,
and it is postdot-unique.

In Leo's original algorithm,
and in early versions of the Marpa algorithm,
all rules were treated
as Leo-eligible,
not just those rules which are
right-recursive.
Experience with Marpa showed that,
while the costs of Leo memoization are
quite manageable,
they do exist,
so that it makes sense to
target Leo memoization carefully.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff from Leo memoization
is therefore very limited.
By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where EORR sets can grow arbitrarily
large.

\begin{definition}
\label{def:basis-of-leo-item}
\dtitle{Basis of a Leo item}
Let, without loss of generality,
a Leo-eligible EIM be
\begin{equation}
\label{eq:def-basis-of-leo-item-5}
\Veim{basis} = \left[
  \begin{gathered}
     [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{trans} \Vstr{nul2} ], \\
     \Vloc{i}, \qquad \Vloc{j}
  \end{gathered}
\right]
\end{equation}
where $\Vstr{nul2} = \epsilon$.
For any Leo item of the form
\begin{equation}
\label{eq:def-basis-of-leo-item-10}
  \Vleo{eff} = [ \Vsym{trans}, \Vdr{tbd}, \Vorig{tbd}, \Vloc{j} ]
\end{equation}
for some \Vdr{tdb} and \Vorig{tdb},
we say that \Veim{basis} is a
\xdfn{basis}{basis (of a Leo item)}
or
\xdfn{bottom-up cause}{bottom-up cause (of a Leo item)}
of \Vleo{eff}
and that \Vleo{eff} is the
\xdfn{effect}{effect (Leo item)}
of \Veim{basis}.
\end{definition}

\begin{definition}
\label{def:down-cause-of-leo-item}
\dtitle{Top-down cause of an Leo item}
Let
\begin{equation*}
\Veim{basis} = [ \Vdr{basis}, \Vloc{i}, \Vloc{j} ]
\end{equation*}
be the basis of a Leo item, call it \Vleo{eff}.
If there is a Leo item of the form
\begin{equation*}
\Vleo{down} = [ \LHS{\Vdr{basis}}, \Vdr{td-top}, \Vorig{td-top}, \Vloc{i} ],
\end{equation*}
for some \Vdr{td-top} and \Vorig{td-top},
then
we say that
\Vleo{down} is a
\xdfn{top-down cause}{top-down cause (of a Leo item)}
of \Vleo{eff};
and that \Vleo{eff} is the
\xdfn{effect}{effect (Leo item)}
of \Vleo{down}.
If there is no Leo item of the form of
\Vleo{down},
then we say that \Vleo{eff} has no
top-down cause.
\end{definition}

% TODO: Prove top-down cause is unique?

\begin{definition}
\label{def:validity-of-leo-item}
\dtitle{Validity of an Leo item}
Let \Vleo{eff} be a Leo item.
\Vleo{eff} is
\xdfn{valid}{valid (Leo item)}
if and only if
it falls into one to
two cases.

\textbf{Case 1}:
In the first case,
\Vleo{eff} has a
a valid basis, call it \Veim{basis},
top-down cause,
call it \Vleo{down}.
In this case,
\Vleo{eff} is valid if and only if
\begin{equation*}
\Vleo{eff} = \left[
  \begin{gathered}
  \Postdot{\Veim{basis}}, \DR{\Vleo{down}}, \\
  \Origin{\Vleo{down}}, \Current{\Veim{basis}}
  \end{gathered}
\right].
\end{equation*}

\textbf{Case 2}:
In the second case,
\Vleo{eff} has a
a valid basis, call it \Veim{basis},
and has no top-down cause.
In this case,
\Vleo{eff} is valid if and only if
\begin{equation*}
\Vleo{eff} = \left[
  \begin{gathered}
  \Postdot{\Veim{basis}},
  \Next{\DR{\Veim{basis}}}, \\
  \Origin{\Veim{basis}}, \Current{\Veim{basis}}
  \end{gathered}
\right].
\end{equation*}

Note that, in either case,
\Vleo{eff} has a valid basis.
\end{definition}

In this document, we will want to allow for implementations which
selectively omit valid Leo memos.
We therefore will distinquish between
valid Leo memos
and
\xdfn{instantiated}{instantiated (Leo memo)}
Leo memos.
All instantiated Leo memos are valid.
A valid Leo memo may or may not be instantiated,
depending on the implementation.

\begin{definition}
\dtitle{Matching Leo item}
A Leo memo, call it \Vleo{leo},
and an EIM, call it \Veim{eim},
are said to
\xdfn{match}{match (of Leo memo and EIM)}
if and only if
\begin{gather*}
\text{\Veim{eim} is a completion,} \\
\Symbol{\Vleo{leo}} = \LHS{\Veim{eim}} \quad \text{and} \\
\Current{\Vleo{leo}} = \Left{\Veim{eim}}.
\end{gather*}
We also say that \Vleo{leo} is the
\xdfn{matching}{matching (Leo item) [of an EIM]}
Leo memo of \Veim{eim}
and that \Veim{eim} is the
\xdfn{matching}{matching (EIM) [of an Leo memo]}
EIM of \Vleo{leo}.
\end{definition}

\begin{theorem}
\label{t:leo-transition-symbol-non-terminal}
The transition symbol of a valid Leo memo
must be a non-terminal other than the accept symbol.
\end{theorem}

\begin{proof}
To be the basis of a Leo memo,
a rule must be right-recursive.
By the definition of validity for a Leo memo,
the rule of the basis must be of the form
\begin{equation}
\Vrule{r} = [ \Vsym{A} \de \Vstr{prefix} \Vsym{trans} \Vstr{nul} ],
\end{equation}
where $\Vstr{nul} = \epsilon$,
and
where
\begin{gather}
\label{eq:leo-transition-symbol-non-terminal-20}
\text{\Vrule{r} is Leo-eligible,} \\
\label{eq:leo-transition-symbol-non-terminal-23}
\text{and \Vsym{trans} is the transition symbol of the Leo memo.}
\end{gather}
By 
\eqref{eq:leo-transition-symbol-non-terminal-20},
\Vrule{r} is Leo-eligible,
and therefore \Vrule{r} must be right-recursive.
By the definition of right-recursive,
\Vrule{r} is right-recursive only if
\begin{equation}
\label{eq:leo-transition-symbol-non-terminal-26}
\text{\Vsym{trans} is right-recursive}.
\end{equation}
To be right-recursive, \Vsym{trans}
must be a non-terminal.
By \eqref{eq:leo-transition-symbol-non-terminal-23},
\Vsym{trans} is the transition symbol.

It remains
to show that $\Vsym{trans} \neq \Vsym{accept}$.
By its definition, \Vsym{accept} occurs only on the
LHS of the accept rule,
and therefore \Vsym{accept} is not right recursive.
Using this fact and
\eqref{eq:leo-transition-symbol-non-terminal-26},
we see that
$\Vsym{trans} \neq \Vsym{accept}$.
\end{proof}

\section{Leo silos}

Where \Vleo{leo} is a Leo memo,
$\Vloc{memloc} = \Current{\Vleo{leo}}$
and
$\Symbol{\Vleo{leo}} = \Vsym{t}$,
we say that the symbol instance
\begin{equation*}
  \Vinst{lb} = \Vmk{memloc} \Vsym{t} \Vmk{i}
\end{equation*}%
\index{recce-notation}{Leo-bot(x)@\Vop{Leo-bot}{x}}
is a
\xdfn{bottom}{bottom (of a Leo silo)}
EIM matching \Vleo{leo} at \Vloc{i}.

Where \Vleo{leo} is a Leo memo,
\Vdr{top} = \DR{\Vleo{leo}}
and
\Vorig{top} = \Origin{\Vleo{leo}},
we say that
\begin{equation}
\Vop{Leo-top}{\Vleo{leo}, \Vloc{i}} = \big[ \Vdr{top}, \Vorig{top}, \Vloc{i} \big].
\end{equation}%
\index{recce-notation}{Leo-top(x)@\Vop{Leo-top}{x}}
is the
\xdfn{top}{top (of a Leo silo)}
EIM
matching \Vleo{leo}
at \Vloc{i}.

\begin{lemma}
\label{lem:leo-memo-effect}
Let \Vleo{leo} be a valid Leo memo
and \Veim{up} be a valid, complete, EIM
such that \Vleo{leo} matches \Veim{up}.
Then
\begin{gather}
\label{eq:lem-leo-memo-effect-10}
\myparbox{\Veim{up} has a valid effect, call
it \Veim{eff},
} \\
\label{eq:lem-leo-memo-effect-10a}
\myparbox{\Veim{eff} is a silo effect
of \Veim{up},
} \\
\label{eq:lem-leo-memo-effect-12}
\myparbox{\Veim{eff} is right recursive, and}
\\
\label{eq:lem-leo-memo-effect-13}
\myparbox{\Veim{eff} is the only effect
of \op{Sym-Eq}{\Veim{up}},
} \\
\label{eq:lem-leo-memo-effect-14}
\myparbox{either \Veim{eff} is the top of the Leo silo,
or it has a matching Leo memo.}
\end{gather}
\end{lemma}

\begin{proof}
Since \Veim{leo} is valid, by the definition of
Leo memo validity,
it has a valid basis, call it \Veim{basis}.
Without loss of generality, let
\begin{equation}
\label{eq:lem-leo-memo-effect-20}
\Veim{basis} = \left[
  \begin{gathered}
     [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{trans} \Vstr{nul2} ], \\
     \Vloc{i}, \qquad \Vloc{j}
  \end{gathered}
\right]
\end{equation}
where $\Vstr{nul2} = \epsilon$,
and $\Symbol{\Vleo{leo}} = \Vsym{trans}$.
Since \Veim{up},
by assumption for the theorem matches
\Vleo{leo},
we have
\begin{equation}
\label{eq:lem-leo-memo-effect-23}
\Symbol{\Veim{up}} = \Vsym{trans}.
\end{equation}
Using theorem
\ref{t:effect-from-symbolic-causes},
from \eqref{eq:lem-leo-memo-effect-20}
and
\eqref{eq:lem-leo-memo-effect-23}
we see that
\begin{gather}
\label{eq:lem-leo-memo-effect-30}
\Veim{eff} = \left[
  \begin{gathered}
     [ \Vsym{A} \de \Vstr{prefix} \Vsym{trans} \mydot \Vstr{nul2} ], \\
     \Vloc{i}, \qquad \Right{\Veim{up}}
  \end{gathered}
\right] \quad \text{is valid, and}
\\
\label{eq:lem-leo-memo-effect-30a}
\text{\Veim{up} is the bottom-up cause of \Veim{eff}.}
\end{gather}
\eqref{eq:lem-leo-memo-effect-30} shows
requirement
\eqref{eq:lem-leo-memo-effect-10}
of the theorem.

From 
\eqref{eq:lem-leo-memo-effect-30},
we see that
\begin{gather}
\label{eq:lem-leo-memo-effect-32a}
\Right{\Veim{eff}} = \Right{\Veim{ep}}, \quad \text{and} \\
\label{eq:lem-leo-memo-effect-32b}
\text{\Veim{eff} is quasi-complete.}
\end{gather}
\Veim{eff} is the matching EIM of a Leo memo,
so that, by the definition of an EIM matching a Leo
memo, \Veim{eff} must be complete, and therefore
\begin{equation}
\label{eq:lem-leo-memo-effect-32c}
\text{\Veim{eff} is quasi-complete.}
\end{equation}
From \eqref{eq:lem-leo-memo-effect-30a},
\eqref{eq:lem-leo-memo-effect-32a},
\eqref{eq:lem-leo-memo-effect-32b},
and
\eqref{eq:lem-leo-memo-effect-32c}
we have
requirement
\eqref{eq:lem-leo-memo-effect-10a} of the
theorem.

By the definition of the basis of a Leo memo,
\Veim{basis} must be right-recursive.
\Veim{basis} and \Veim{eff} share the same rule,
so \Veim{eff} is right recursive,
which shows
Requirement
\eqref{eq:lem-leo-memo-effect-12}
of the theorem.

By the definition of the basis of a Leo memo,
\Veim{basis} must be postdot-unique ---
that is there can be no distinct EIM, call it \Veim{basis2},
such that
\begin{gather*}
\Current{\Veim{basis}} = \Current{\Veim{basis2}} \quad \text{and} \\
\Postdot{\Veim{basis}} = \Postdot{\Veim{basis2}}.
\end{gather*}
So, by the definition of matching causes, \Veim{basis}
is the only top-down cause matching \Veim{up}.
Since an effect is completely determined by the symbolic equivalent
of its bottom-up cause and its top-down cause, this
gives us 
Requirement
\eqref{eq:lem-leo-memo-effect-13}
of this theorem.

To show
Requirement
\eqref{eq:lem-leo-memo-effect-14}
of the theorem,
we proceed by cases, based on whether
or not \Vleo{leo} has a top-down cause.
If \Vleo{leo} has no top-down cause,
then, by the definition of Leo memo validity
and the definition of 
\myfnname{Leo-top},
we know that
\begin{multline}
\label{eq:lem-leo-memo-effect-35}
\Vop{Leo-top}{\Vleo{leo}, \Right{\Veim{up}}} = \\
	\big[ \Next{\DR{\Veim{basis}}}, \Origin{\Veim{basis}}, \Right{\Veim{up}} \big].
\end{multline}
Using
\eqref{eq:lem-leo-memo-effect-20}
and
comparing
\eqref{eq:lem-leo-memo-effect-30}
\eqref{eq:lem-leo-memo-effect-35}
we see that
\begin{equation}
\Veim{eff} = \Vop{Leo-top}{\Vleo{leo}, \Right{\Veim{up}}}.
\end{equation}
This shows our first case.

In our second case,
\Vleo{leo} has a top-down cause,
call it \Vleo{cuz}.
From the definition of a Leo memo's top-down
cause,
we know that
\begin{equation}
\label{eq:lem-leo-memo-effect-40}
\begin{gathered}
\Symbol{\Vleo{cuz}} = \LHS{\Veim{basis}}
\text{and} \quad \\
\Current{\Vleo{cuz}} = \Origin{\Veim{basis}}.
\end{gathered}
\end{equation}
From 
\eqref{eq:lem-leo-memo-effect-20}
and
\eqref{eq:lem-leo-memo-effect-30}
we know that
\begin{equation}
\label{eq:lem-leo-memo-effect-43}
\begin{gathered}
\LHS{\Veim{basis}} = \LHS{\Veim{eff}}
\text{and} \quad \\
\Origin{\Veim{basis}}  = \Origin{\Veim{eff}}.
\end{gathered}
\end{equation}
From
\eqref{eq:lem-leo-memo-effect-40}
and
\eqref{eq:lem-leo-memo-effect-43}
and the definition of ``matching''
for Leo memos and EIM's,
we see that \Vleo{cuz}
matches \Veim{eff}.
This shows our second case,
Requirement
\eqref{eq:lem-leo-memo-effect-14}
of the theorem,
and the theorem.
\end{proof}

\begin{lemma}
\label{lem:leo-memo-of-incompletion}
Let \Veim{cuz} be an EIM which is quasi-complete,
but incomplete;
let
\Vleo{l}
be a Leo memo matching \Veim{cuz};
and
let \Veim{eff} be an effect of
\Veim{cuz}.
Then \Vleo{l} matches \Veim{eff}.
\end{lemma}

\begin{proof}
Since, by assumption for the theorem,
\Veim{cuz} is incomplete, it has a postdot symbol.
Since, by assumption for the theorem,
\Veim{cuz} is quasi-complete, the postdot symbol must be nulling.
By Theorem
\ref{t:null-scan-from-down-cause},
\begin{equation}
\label{eq:lem-leo-memo-of-incompletion-10}
\begin{aligned}
\Left{\Veim{cuz}} & = \Left{\Veim{eff}} \\
\land \quad \LHS{\Veim{cuz}} & = \LHS{\Veim{eff}}.
\end{aligned}
\end{equation}
By assumption for the theorem
and the definition of a matching Leo memo
\begin{equation}
\label{eq:lem-leo-memo-of-incompletion-20}
\begin{aligned}
\Left{\Veim{cuz}} & = \Current{\Vleo{l}} \\
\land \quad \LHS{\Veim{cuz}} & = \Symbol{\Vleo{l}}.
\end{aligned}
\end{equation}
Combining
\eqref{eq:lem-leo-memo-of-incompletion-10}
and
\eqref{eq:lem-leo-memo-of-incompletion-20},
\begin{equation}
\label{eq:lem-leo-memo-of-incompletion-30}
\begin{aligned}
\Left{\Veim{eff}} & = \Current{\Vleo{l}} \\
\land \quad \LHS{\Veim{eff}} & = \Symbol{\Vleo{l}}.
\end{aligned}
\end{equation}
By the definition of a matching Leo memo,
\eqref{eq:lem-leo-memo-of-incompletion-30}
tells us the \Vleo{l} matches \Veim{cuz}
\end{proof}

\begin{theorem}
\ttitle{Leo silo validity}
\label{t:leo-silo-validity}
Let
\begin{equation*}
\Vleo{leo} = [ \Vsym{transition}, \Vdr{top}, \Vorig{top}, \Vloc{i} ]
\end{equation*}
be an instantiated Leo memo and \Vinst{lb} a valid matching bottom.
Then for every EIM equivalent of \Vinst{lb}, call it \Veim{lb},
\begin{gather}
\label{t:leo-silo-validity-2}
\myparbox{there is a silo,
call it \var{slo}, whose bottom is \Veim{lb};}
\\
\label{t:leo-silo-validity-4}
\myparbox{the top of \var{slo} is 
\op{Leo-top}{\Vleo{leo}, \Right{\Veim{lb}}};}
\\
\label{t:leo-silo-validity-6}
\myparbox{every layer of \var{slo} above the bottom is right-recursive; and}
\\
\label{t:leo-silo-validity-8}
\myparbox{every layer of \var{slo} below the top has exactly one effect.}
\end{gather}
\end{theorem}

\begin{proof}
Theorems
\ref{t:eim-equivalent-from-non-terminal}
and
\ref{t:leo-transition-symbol-non-terminal}
guarantee that
\Vinst{lb} has at least one valid EIM equivalent,
call it \Veim{lb},
such that
\begin{equation}
\label{t:leo-silo-validity-9}
\Valid{\Veim{lb}}
\end{equation}

As an EIM equivalent,
\Veim{lb} is complete,
and therefore quasi-complete,
so that,
by the definition of a silo,
it is in a silo,
if only the trivial silo containing just itself.
This shows
\eqref{t:leo-silo-validity-2}.

Let
\var{topix} and \Veim{top} be
such that
\begin{equation}
\Veim{top} = \Vel{slo}{topix} = \op{Leo-top}{\Vleo{leo}, \Right{\Veim{lb}}}
\end{equation}
We need to ensure that our notation
does not assume that \Veim{top} is a layer of
\var{slo},
since we have yet to prove this.
If \Veim{top} is not a layer of \var{slo}
we say \var{topix} is undefined
and that, for every integer \var{i},
$\var{i} < \var{topix}$.

We next proceed by finite induction.
We take as the induction hypothesis
\begin{align}
\label{t:leo-silo-validity-25}
\tag{IND1}
0 \le \var{x} \le \var{topix}
& \implies \Valid{\Vel{slo}{x}}
\\
\label{t:leo-silo-validity-28}
\tag{IND2}
0 < \var{x} \le \var{topix}
& \implies \text{\Vel{slo}{x} is right recursive}
\\
\label{t:leo-silo-validity-31}
\tag{IND3}
0 < \var{x} \le \var{topix}
& \implies
\begin{multlined}
\text{\Vel{slo}{x} is the unique effect} \\
\text{of \el{slo}{\Vdecr{x}}, and}
\end{multlined}
\\
\label{t:leo-silo-validity-32}
\tag{IND4}
0 \le \var{x} < \var{topix}
& \implies
\text{\Vel{slo}{x} has a matching Leo memo}
\end{align}
For $\var{x} = 0$,
we have
\eqref{t:leo-silo-validity-25} from
\eqref{t:leo-silo-validity-9},
and we have
\eqref{t:leo-silo-validity-28}
and
\eqref{t:leo-silo-validity-31}
vacuously.
By assumption for the theorem,
\Vleo{leo} is a Leo memo matching
$\el{slo}{0} = \Veim{lb}$,
which gives us
\eqref{t:leo-silo-validity-32}.
We take $\var{x} = 0$
as the basis of our induction.

For the step of the induction, we assume
\eqref{t:leo-silo-validity-25},
\eqref{t:leo-silo-validity-28},
\eqref{t:leo-silo-validity-31}
and
\eqref{t:leo-silo-validity-32}
for \var{x} = \var{i},
to show
\begin{equation}
\label{t:leo-silo-validity-34}
\tag{STEP}
\text{\eqref{t:leo-silo-validity-25},
\eqref{t:leo-silo-validity-28},
\eqref{t:leo-silo-validity-31}
and
\eqref{t:leo-silo-validity-32}
for $\var{x} = \Vincr{i}$.}
\end{equation}
Within the step, we proceed by cases.

\textbf{Step case 1}: For the case where $\var{i} > \var{topix}$,
we have
\eqref{t:leo-silo-validity-34}
vacuously.

\textbf{Step case 2}: We next consider
the case where $\var{i} \le \var{topix}$
and \Vel{slo}{i} is incomplete.
By assumption for the step 
\Vel{slo}{i} is a valid silo layer,
and therefore a quasi-completion.
Since
\Vel{slo}{i} is a valid quasi-completion,
we can use
\eqref{eq:quasi-completion-silo-14}
of
Theorem 
\ref{t:quasi-completion-silo}
tells us that
\el{slo}{\Vincr{i}} is valid,
which shows
\eqref{t:leo-silo-validity-25}.

\eqref{eq:quasi-completion-silo-18}
of
Theorem 
\ref{t:quasi-completion-silo} tells us that
the rule of \Vel{silo}{i}
is the rule of 
the rule of \el{silo}{\Vincr{i}}.
Since the 
the rule of \Vel{silo}{i}
is right recursive by assumption for the step,
we have
\eqref{t:leo-silo-validity-28}.

By assumption for the case,
\Vel{slo}{i} is not complete,
so that
\eqref{eq:quasi-completion-silo-22}
of
Theorem 
\ref{t:quasi-completion-silo} tells us that
the unique effect of \Vel{silo}{i}
is \el{silo}{\Vincr{i}}.
This shows
\eqref{t:leo-silo-validity-31}
for this case.

Again,
by assumption for the case
\Vel{slo}{i} is not complete.
Since it is,
by assumption for the step,
a silo layer,
it is quasi-complete.
Also by assumption for the step,
\Vel{slo}{i} has a matching Leo memo.
Using these facts and Lemma
\ref{lem:leo-memo-of-incompletion},
we have
\eqref{t:leo-silo-validity-32}
This shows the step, for this case.

\textbf{Step case 3}: Finally,
we consider
the case where $\var{i} \le \var{topix}$
and \Vel{slo}{i} is complete.
Since \Vel{slo}{i} is complete,
Lemma \ref{lem:leo-memo-effect} applies,
and we have
\eqref{t:leo-silo-validity-25},
\eqref{t:leo-silo-validity-28},
\eqref{t:leo-silo-validity-31}
and
\eqref{t:leo-silo-validity-32}
of the induction hypothesis for this case
from
\eqref{eq:lem-leo-memo-effect-10},
\eqref{eq:lem-leo-memo-effect-12},
\eqref{eq:lem-leo-memo-effect-13}
and
\eqref{eq:lem-leo-memo-effect-14}
of
Lemma \ref{lem:leo-memo-effect}.
This shows shows the final case of the induction
step, and therefore the step and the induction.

The induction shows Requirements
\eqref{t:leo-silo-validity-6}
and
\eqref{t:leo-silo-validity-8}
of the theorem
and
\begin{equation}
\label{t:leo-silo-validity-36}
\forall \var{x} : 0 \le \var{x} < \var{topix}
\implies
\text{\Vel{slo}{x} has a matching Leo memo}
\end{equation}

It remains to show that there is
a \var{slo} such that one of its layers is
\begin{equation}
\label{t:leo-silo-validity-36a}
\op{Leo-top}{\Vleo{leo}, \Right{\Veim{lb}}}.
\end{equation}
which is Requirement
\eqref{t:leo-silo-validity-4}
of the theorem.
We assume, for a reductio, that there is no such silo.

For the purpose of this proof, we will call a silo ``maximal''
if it is not a proper subsequence of any larger silo.
Every silo has at least one maximal silo,
though in the trivial case
a silo is its own maximal silo.
Let a maximal silo of \var{slo} be \var{mslo}.
By Theorem
\ref{t:silo-finite}, any silo containing
\Veim{lb} must be finite,
and so that \var{mslo} must have a highest layer.
Let the index of that highest layer be \var{maxix}.
By assumption for the reductio,
\var{mslo} 
does not contain
\eqref{t:leo-silo-validity-36a}
so that, in particular,
\begin{equation}
\label{t:leo-silo-validity-37}
\tag{RAA}
\Vel{mslo}{maxix} \neq
\op{Leo-top}{\Vleo{leo}, \Right{\Veim{lb}}}.
\end{equation}

From
\eqref{t:leo-silo-validity-36},
we see that \Vel{slo}{maxix} has
a matching Leo memo.
We consider two cases, based on whether
\Vel{mslo}{maxix} is complete or incomplete.

\textbf{Case 1}:
Assume for the case that
\Vel{mslo}{maxix} is incomplete.
If
\Vel{mslo}{maxix} is a 
quasi-complete silo layer,
by
Theorem
\ref{t:quasi-completion-silo},
it is the bottom of some silo,
call it \var{slo1}, such that 
\begin{equation}
\label{t:leo-silo-validity-40}
\text{\el{slo1}{\Vlastix{slo1}} is a completion}
\end{equation}
Since 
$\Vel{mslo}{maxix} = \el{slo1}{0}$ is the bottom
of \var{slo1},
we have
\begin{equation}
\label{t:leo-silo-validity-42}
\Vel{mslo}{maxix} \le \el{slo1}{\Vlastix{slo1}}.
\end{equation}
\var{slo1} and \var{mslo} overlap in
$\Vel{mslo}{maxix} = \el{slo1}{0}$,
so we can compose them into a new silo, \var{slo2},
such that
\begin{equation}
\label{t:leo-silo-validity-43}
\begin{gathered}
\Vel{mslo}{maxix} = \el{slo1}{0} = \el{slo2}{\Vlastix{slo1}}
	\quad \text{and} \\
\el{slo1}{\Vlastix{slo1}} = \el{slo2}{\Vlastix{slo2}}.
\end{gathered}
\end{equation}
By assumption for the case \Vel{mslo}{maxix} is incomplete,
so that
from \eqref{t:leo-silo-validity-40}
we know that
\begin{equation}
\label{t:leo-silo-validity-46}
\Vel{mslo}{maxix} \neq \el{slo1}{\Vlastix{slo1}}.
\end{equation}
From
\eqref{t:leo-silo-validity-40}
and
\eqref{t:leo-silo-validity-46}
we see that
\begin{equation}
\label{t:leo-silo-validity-49}
\Vel{mslo}{maxix} < \el{slo1}{\Vlastix{slo1}}.
\end{equation}
But, by assumption for the reductio,
\Vel{mslo}{maxix} has no silo layer above it.
This contradiction shows this Case 1 of the reductio.

\textbf{Case 2}:
Assume for the case that
\Vel{mslo}{maxix} is complete.
Since
\Vel{mslo}{maxix} has a matching Leo memo,
by
Lemma \ref{lem:leo-memo-effect},
it has a silo effect.
So there is another silo, call it \var{mslo2},
such that
there is a silo layer
\el{mslo2}{\Vincr{maxix}}.
This means that \var{mslo} is a proper
subsequence of \var{mslo2}.
But by assumption, \var{mslo2} was what we call
for this proof,
``maximal'',
and therefore not a proper subsequence of any
other silo.
This contradiction shows this Case 1 of the reductio
and the reductio.

From the reductio, we see that \var{slo}
must contain 
\eqref{t:leo-silo-validity-36a},
the top item for \Vleo{leo}.
This shows Requirement
\eqref{t:leo-silo-validity-4}
and the theorem.
\end{proof}

\begin{definition}
\dtitle{Leo silo}
Let \Vleo{l} be a Leo memo
\dfn{Leo silo}
is a silo whose bottom is the matching bottom
of a Leo memo,
and whose top is
\[
\op{Leo-top}{\Vleo{leo}, \Vloc{i}},
\]
where
\Vloc{i} is the location
of the silo.
\end{definition}

\begin{definition}
If a layer of a Leo silo is not its
top or its bottom layer
we say that it is an
\xdfn{inner}{inner (layer of a Leo silo)}
layer.
It is the
inner layers of a silo
that are
\dfn{Leo memoable}.
That is, if \var{slo} is a Leo silo,
Let the Leo item be
\begin{equation}
  \forall \; \var{a} : \Vfirstix{slo} < \var{a} < \Vlastix{slo}
  \implies \Memoable{\Vel{slo}{a}}.
\end{equation}
Unless stated otherwise,
when we call an EIM
\dfn{memoable},
we mean that it is Leo memoable.
\end{definition}

\begin{theorem}
\ttitle{EIM's in Leo silos}
\label{t:eims-in-leo-silos}
Every EIM in a Leo silo is a null-scan,
a read or
a reduction.
Also, if a read EIM is in a Leo silo,
it must be the bottom of the Leo silo.
\end{theorem}

\begin{proof}
By the definition of a Leo silo,
every EIM in a Leo silo is a silo layer.
From Theorem
\ref{t:silo-read-eim-layer},
we know that, if a read EIM is in
a silo, it is the bottom of the silo.
A Leo silo, by its definition,
is a section of a silo,
so that if a Leo silo
contains a read EIM,
it is the bottom of the Leo silo.
\end{proof}

\section{Recreating a Leo silo}

While Marpa is a parser implementation,
including evalution,
this document does not usually deal with
evaluation issues.
We will make an exception for
Leo memoization,
however.
The following algorithm summarizes
Theorem
\ref{t:leo-silo-validity},
which shows its correctness.

\begin{algorithm}[th]
\caption{Reconstruct memoized EIM's}
\label{alg:reconstruct-memoized-eims}
\begin{algorithmic}[1]
\Procedure{Reconstruct memoized EIM's}{\Vleo{leo}}
\State $\Veim{top} \gets \Vop{Leo-top}{leo}$
\State $\Vinst{scuz} \gets \op{EIM-Eq}{\Vop{Leo-bot}{leo}}$
\State $\Vloc{here} \gets \Right{\Vleo{leo}}$
\While{ true }
% \label{line:reconstruct-memoized-eims-20}
\State \Veim{new-scuz} $\gets$ silo effect of \Veim{scuz}
\State
\Comment silo effect is unique per Theorem \ref{t:leo-silo-validity}
\If{\Veim{new-scuz} = \Veim{top}}
\State return
\EndIf
\State Add \Veim{new-scuz} to \Vel{table}{here}
\State $\qquad$ but only if it has never been added before
\State \Veim{scuz} = \Veim{new-scuz}
\EndWhile
% \label{line:reconstruct-memoized-eims-90}
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-theorems}{Construction: Reconstruct memoized EIM's}

\begin{theorem}
\label{t:leo-quasi-complete}
Only quasi-complete EIM's are
Leo-memoized.
\end{theorem}

\begin{proof}
An EIM is memoized only if it is memoable.
All Leo memoable items are,
by the definition of Leo memoable,
inner layers of
Leo silos.
All silo layers are,
by the definition of a silo,
quasi-complete.
\end{proof}

\begin{theorem}
\label{t:read-eim-not-leo}
Read EIM's and the EIM's in the
ethereal closure of a read EIM
are never Leo-memoized.
\end{theorem}

\begin{proof}
Assume for a reductio,
that an EIM in the ethereal closure of a read EIM
is memoized.
Call a read EIM, \Veim{scanned}.
By Theorem
\ref{t:eims-in-leo-silos}
\Veim{scanned},
if it is in a Leo silo
is the bottom of the silo.
The bottom of a silo is not memoable,
and therefore a read EIM is never memoized.

By the definition of a read EIM,
the predot symbol of \Veim{scanned}
is a terminal.
Since \Veim{scanned} is memoized,
by Theorem \ref{t:leo-quasi-complete},
it must be quasi-complete, so the predot
symbol of \Veim{scanned} must also be its penult.
Therefore the quasi-penult of \Veim{scanned} is
a terminal.

Only EIM's with right recursive rules are memoized.
From the definition of right recursion,
a right recursive rule must have a right recursive
symbol as its quasi-penult.
But a terminal cannot be recursive symbol.

Therefore \Veim{scanned} cannot have a right recursive rule,
and therefore \Veim{scanned} is not memoized,
which is contrary to the assumption for the reductio.
This shows the reduction and the theorem.
\end{proof}

\begin{theorem}
\label{t:earley-set-0-is-Leo-free}
No EIM in Earley set 0 is memoized.
\end{theorem}

\begin{proof}
By definition,
a quasi-complete EIM has at least one
telluric predot symbol,
so its current location cannot be location 0.
Therefore no quasi-complete EIM occurs in
Earley set 0.
But,
by Theorem \ref{t:leo-quasi-complete},
only quasi-complete EIM's are memoized.
Therefore no EIM in Earley set 0
is memoized.
\end{proof}

\begin{theorem}
\label{t:ethereal-closure-is-Leo-disjoint}
An EIM in the ethereal closure of \Veim{eim1}
is Leo-memoized if and only if
\Veim{eim1} is Leo-memoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

A \dfn{non-trivial Leo top item} is
is an unmemoized item with a memoized bottom-up
cause.

\begin{theorem}
\label{t:memoized-effects}
If an effect,
other than
a non-trivial Leo top item,
is unmemoized,
all of its causes are unmemoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:leo-memo-from-top}
Let \Veim{top} be a valid non-trivial Leo top item.
Then a bottom-up cause which matches its Leo memo
will be valid and unmemoized.
\end{theorem}

\begin{proof}
Let \Vleo{leo} be a Leo item and
\Veim{up} a valid EIM that 
matches it.
By Theorem \ref{t:leo-silo-validity}
there is a valid silo with \Veim{up}
as its bottom.
A bottom EIM is not memoable.
If an EIM is not memoable in any of
its occurrences, then it is not memoized.
\end{proof}

\begin{theorem}
\label{t:accept-eim-not-memoized}
The accept EIM is never memoized.
\end{theorem}

\begin{proof}
By definition of the accept EIM,
its LHS never occurs on a RHS.
The accept EIM is therefore not right recursive.
All memoable EIM's are inner layers of Leo silos,
and all inner layers of Leo silos are right
recursive.
Therefore the accept EIM is never memoized.
\end{proof}

\begin{theorem}
\label{t:no-memoized-predictions}
No prediction is memoized.
\end{theorem}

\begin{proof}
This theorem follows from
Theorem \ref{t:leo-quasi-complete}
and Theorem \ref{t:quasi-drs-disjoint}.
\end{proof}

\chapter{Altitude}
\label{ch:altitude}

TODO: Delete this chapter?

\begin{definition}
\dtitle{Altitude}
\label{def:altitude}
We write \Alt{\Vinst{inst}} for the
\xdfn{altitude}{altitude (of an EIM)}
of \Vinst{inst}.

An EIM, \Veim{up}, is an
\dfn{altitude cause}
of a reduction EIM, \Veim{red},
if and only  if \Veim{up} is a bottom-up cause of
\Veim{red}.
An EIM, \Veim{down}, is an
\dfn{altitude cause}
of an ethereal EIM, \Veim{eth},
if and only  if \Veim{down} is a top-down cause of
\Veim{eth}.

\begin{sloppypar}
\begin{FlushLeft}
\begin{enumerate}
\item
\label{case:def-altitude-start}
The altitude of the start rule is 1.
\item
\label{case:def-altitude-read}
The altitude of a read EIM is 1.
\item
\label{case:def-altitude-bump}
The altitude of an unmemoized reduction EIM, \Veim{unmemo}
is the least \Alt{\Veim{cuz}}+1
such that \Veim{cuz} is an altitude cause
of \Veim{unmemo}.
\item
\label{case:def-altitude-other}
Let \Veim{other} be an
unmemoized EIM,
a predicted EIM,
or a null-scan EIM.
The altitude of
\Veim{other}
is the least \Alt{\Veim{cuz}}
such that
\Veim{cuz} is an altitude cause of \Veim{other}.
\end{enumerate}

\end{FlushLeft}
\end{sloppypar}

\end{definition}

We number altitudes starting with 1, so that
we may reserve altitude 0
for non-EIM causes, such as terminal instances.
Notice that,
in some of the
cases
of Definition
\ref{def:altitude},
we state that the altitude is the least
of one or more choices.
EIM's can have multiple top-down causes,
and multiple bottom-up causes.
Where this results in a choice of altitudes,
the alititude is always the minimum of the choices.

\begin{theorem}
\ttitle{Silo cause is altitude cause}
\label{t:silo-cause-is-altitude-cause}
Every silo cause is an altitude cause.
\end{theorem}

\begin{proof}
Silo elements, by the definition
of a silo, are quasi-complete,
and therefore must be read EIM's,
reduction EIM's
or null-scan EIM's.
From
Theorem \ref{t:silo-causes},
we see that only reduction EIM'S
and null-scan EIM's have silo causes.
This shows the theorem vacuously for
the start EIM, predicted EIM's
and read EIM's.

In the cause of a reduction EIM,
its silo causes are its bottom-up causes.
From Definition 
\ref{def:altitude} (``altitude cause''),
we see that a reduction EIM's altitude causes
are its bottom-up causes.
This shows theorem for reduction EIM's.

In the cause of a null-scan EIM,
its silo causes are its top-down causes.
From Definition 
\ref{def:altitude} (``altitude cause''),
we see that a reduction EIM's altitude causes
are its top-down causes.
This shows theorem for its last case,
that of null-scan EIM's.
\end{proof}

\begin{theorem}
\label{t:altitude-from-telluric-base}
The altitude of
a valid EIM is that of the telluric base
of its ethereal closure.
\end{theorem}

\begin{proof}
By Theorem \ref{t:eim-has-telluric-base}
we know that a valid EIM has a valid telluric base.
Call the telluric base of the ethereal closure, \Veim{base}.

An ethereal closure is the transitive closure of the
the \var{predict-op} and \var{null-scan-op}
relations.
Our proof will be by induction on the number of
iterations of
\var{epsilon-op}.
Recall from
\eqref{eq:def-epsilon-op}
that
\begin{equation}
\var{epsilon-op} \defined \var{predict-op} \cup \var{null-scan-op}.
\end{equation}
By Theorem
\ref{t:ethereal-closure},
we know that
\Valid{\var{epsilon-op}(\var{desc})}.
For the purpose of this proof, we define
\begin{equation}
\label{eq:altitude-from-telluric-base-20}
\Vel{eims}{x} = \left\lbrace \var{eim} \;
\middle| \;
\begin{multlined}
\Veim{eim} = \\
    {\myfnname{epsilon-op}^{\displaystyle (\var{x})}}(\Veim{base})
\end{multlined}
\right\rbrace
\end{equation}
By Definition \ref{def:altitude} for
the altitude of
predictions and null-scans,
we know that
\begin{equation}
\label{eq:altitude-from-telluric-base-5}
\Alt{\var{epsilon-op}(\var{desc})} = \Alt{\var{desc}}.
\end{equation}

Our induction hypothesis is
\begin{equation}
\label{eq:altitude-from-telluric-base-10}
\tag{IND}
\forall \, \Veim{eim}, \var{eim} \in \Vel{eims}{x} \implies \Alt{\var{eim}} = \Alt{\Veim{base}}
\end{equation}
As the basis of the induction, we note that the altitude of \Veim{base} is self-identical
to show
\eqref{eq:altitude-from-telluric-base-10} for $\var{x} = 0$.

For the step of the induction, we assume
\eqref{eq:altitude-from-telluric-base-10} for \var{i}
to show
\eqref{eq:altitude-from-telluric-base-10} for $\var{x}+1$.
From
\eqref{eq:altitude-from-telluric-base-20},
we know that
\begin{multline}
\label{eq:altitude-from-telluric-base-30}
\Veim{desc2} \in \el{eims}{\var{i}+1} \\
\implies
\left(
\exists \, \Veim{desc1}
\; \middle| \;
\begin{multlined}
\var{desc1} \in \Vel{eims}{i}  \\
\text{and $\var{desc2} = \var{epsilon-op}(\var{desc1})$}
\end{multlined}
\right).
\end{multline}
We see from the defintion of altitude that it is always the least
of the possible alternative, and that it is non-decreasing,
so that
\begin{equation}
\label{eq:altitude-from-telluric-base-32}
\Vel{eims}{i} \le \el{eims}{\Vincr{i}}.
\end{equation}
By
\eqref{eq:altitude-from-telluric-base-5},
\eqref{eq:altitude-from-telluric-base-30},
and
\eqref{eq:altitude-from-telluric-base-32},
\begin{equation}
\label{eq:altitude-from-telluric-base-33}
\Veim{desc2} \in \el{eims}{\var{i}+1} 
\implies
\Alt{\var{desc2}} = \Alt{\var{desc1}}
\end{equation}

Since \var{desc2} was chosen without last of generality,
the step for the induction,
\eqref{eq:altitude-from-telluric-base-10} for $\var{i}+1$, follows from
\eqref{eq:altitude-from-telluric-base-33}.
This shows the induction and the theorem.
\end{proof}

\begin{theorem}
\ttitle{Ethereal altitude}
\label{t:ethereal-altitude}
All the EIM's in an ethereal closure have the same
altitude.
\end{theorem}

\begin{proof}
This theorem follows directly from
Theorem \ref{t:altitude-from-telluric-base}.
\end{proof}

\begin{theorem}
\ttitle{Quasi-complete altitude}
\label{t:quasi-complete-altitude}
Altitude is finite and defined for every
quasi-complete EIM.
\end{theorem}

\begin{proof}
Let \Veim{quasi} be a quasi-complete EIM
and let \var{silo} be a silo of height \var{h},
such that
\begin{gather}
\Vel{silo}{\Vdecr{h}} = \Veim{quasi} \\
\text{and} \quad \el{silo}{0} = \Veim{bot},
\end{gather}
where \Veim{bot} is a read EIM.
By Theorem 
\ref{t:quasi-confirmed-eim-has-silo},
there is at least one such silo.

We show the theorem by induction,
where the induction hypothesis is
\begin{equation}
\tag{HYP}
\label{eq:quasi-complete-altitude-10}
\text{\Alt{\Vel{silo}{x}} is defined and finite.}
\end{equation}
Since
\el{silo}{0} is a read EIM,
we know that the altitude of \el{silo}{0}
is 1,
directly from the definition of altitude.
This gives us
\eqref{eq:quasi-complete-altitude-10} for
$\var{x} = 0$,
and we take this as the basis of the induction.

For the step of the induction, we assume
\eqref{eq:quasi-complete-altitude-10} for
$\var{x} = \var{i}$,
to show
\eqref{eq:quasi-complete-altitude-10} for
$\var{x} = \Vincr{i}$.
By Theorem
\ref{t:silo-read-eim-layer},
we know that
\el{silo}{\Vincr{i}} is not a read EIM,
and therefore by Theorem
\ref{t:silo-cause-is-altitude-cause},
we know that the altitude cause of
\el{silo}{\Vincr{i}} is
\Vel{silo}{i}.
Since
\el{silo}{\Vincr{i}} is a silo layer
but not a read EIM,
it is a null-scan EIM or a reduction EIM.
From the definition of altitude, we see that
the altitude of
a null-scan EIM or a reduction EIM
is defined and finite, if the altitude of
its altitude cause is defined and finite,
that is, if
\begin{equation}
\label{eq:quasi-complete-altitude-20}
\text{\Alt{\Vel{silo}{i}} is defined and finite.}
\end{equation}
\eqref{eq:quasi-complete-altitude-20}
was our assumption for the step,
and this gives us the step, the induction
and the theorem.
\end{proof}

\begin{theorem}
\label{t:altitude-is-finite}
If \Veim{eim} is a valid EIM,
its altitude is defined and finite.
\end{theorem}

\begin{proof}
We first show that altitude is defined
and finite for telluric EIM's.
We proceed by cases, depending on the
type of telluric \Veim{tell}.
If \Veim{tell} is the start EIM
or a read EIM, we know its altitude is
1 directly
from the definition of altitude,
which gives us the theorem for those
two cases.

The remaining case is where
\Veim{tell} is a reduction.
Using
Theorem \ref{t:symbolic-causes-from-effect}, we know
that its bottom-up cause, \Vinst{up},
is a symbol instance
and that
\begin{equation}
\label{eq:altitude-is-finite-10}
\Symbol{\Vinst{up}} = \Predot{\Veim{tell}}.
\end{equation}
Since \Veim{up} is a reduction,
\Predot{\Veim{tell}} is a non-terminal,
so that
from \eqref{eq:altitude-is-finite-10},
we know that
\Symbol{\Vinst{up}} is a non-terminal.
Since \Symbol{\Vinst{up}} is a non-terminal,
by Theorem
\ref{t:eim-equivalent-from-non-terminal},
we know that
\Vinst{up} has a complete EIM equivalent.

Call the EIM equivalent of \Vinst{up},
\Veim{up}.
Since \Veim{up} is complete,
it is quasi-complete so that,
by Theorem
\ref{t:quasi-complete-altitude},
the altitude of \Veim{up} is defined
and finite.
By the definition of an altitude cause,
the altitude cause of a reduction is its
bottom-up cause, in this case, \Veim{up}.
By the definition of altitude, if the
altitude cause of a reduction is defined
and finite,
then the reduction has a defined and
finite altitude.
This shows the last
case of telluric EIM's ---
that of reduction EIM's.

We have shown that
\begin{equation}
\label{eq:altitude-is-finite-20}
\myparbox{
altitude is defined and
finite for all telluric EIM's.
}
\end{equation}
We now are in a position to show that
altitude is defined and finite for an
arbitrary EIM.
By Theorem
\ref{t:altitude-from-telluric-base},
the altitude of an EIM is
defined and finite if the altitude of
its telluric base is.
By Theorem \ref{t:eim-has-telluric-base},
every EIM is in the ethereal closure
of some telluric base.
The telluric base of an ethereal closure
must be a telluric EIM.
\eqref{eq:altitude-is-finite-20} shows that
the altitude of a telluric EIM
is defined and finite.
This shows that the altitude of an
arbitary EIM is defined and finite,
which was the theorem.
\end{proof}

\begin{theorem}
\label{t:leo-silo-altitude}
The altitude of an unmemoized quasi-confirmed
EIM is one plus that of the
most recent unmemoized bottom-up cause in its silo.
\end{theorem}

\begin{proof}
TODO
\end{proof}

\chapter{The Marpa Recognizer}
\label{ch:recce}
\label{ch:pseudocode}

\section{Complexity}

Alongside the pseudocode of this section
are observations about its space and time complexity.
In what follows,
we will charge all time and space resources
to Earley items,
or to attempts to add Earley items.
We will show that,
to each Earley item actually added,
or to each attempt to add a duplicate Earley item,
we can charge amortized \Oc{} time and space.

At points, it will not be immediately
convenient to speak of
charging a resource
to an Earley item
or to an attempt to add a duplicate
Earley item.
In those circumstances,
we speak of charging time and space
\begin{itemize}
\item to the parse; or
\item to the Earley set; or
\item to the current procedure's caller.
\end{itemize}

We can charge time and space to the parse itself,
as long as the total time and space charged is \Oc.
Afterwards, this resource can be re-charged to
the initial Earley item, which is present in all parses.
Soft and hard failures of the recognizer use
worst-case \Oc{} resource,
and are charged to the parse.

We can charge resources to the Earley set,
as long as the time or space is \Oc.
Afterwards,
the resource charged to the Earley set can be
re-charged to an arbitrary member of the Earley set,
for example, the first.
If an Earley set is empty,
the parse must fail,
and the time can be charged to the parse.

In a procedure,
resource can be ``caller-included''.
Caller-included resource is not accounted for in
the current procedure,
but passed upward to the procedure's caller,
to be accounted for there.
A procedure to which caller-included resource is passed will
sometimes pass the resource upward to its own caller,
although of course the top-level procedure does not do this.

For each procedure, we will state whether
the time and space we are charging is inclusive or exclusive.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource charges passed up from called procedures.
Inclusive time or space includes
resource passed upward to the
current procedure from called procedures.

Earley sets may be represented by \Ves{i},
where \var{i} is the Earley set's location \Vloc{i}.
The two notations should be regarded as interchangeable.
The actual implementation of either
should be the equivalent of a pointer to
a data structure containing,
at a minium,
the Earley items,
a memoization of the Earley set's location as an integer,
and a per-set-list.
Per-set-lists will be described in Section \ref{s:per-set-lists}.

\begin{algorithm}[th]
\caption{Marpa Top-level}
\label{alg:top}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initialization}{}
\label{line:top-20}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\label{line:top-30}
\State \Call{Read pass}{$\var{i}, \var{w}[\Vdecr{i}]$}
\label{line:top-33}
\If{$\size{\Ves{i}} = 0$}
\State reject \Cw{} and return
\EndIf
\State \Call{Reduction pass}{\var{i}}
\label{line:top-40}
\EndFor
\If{$[\Vdr{accept}, 0] \in \Etable{\Vsize{w}}$}
\State accept \Cw{} and return
\EndIf
\State reject \Cw{}
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Marpa Top-level}

\section{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets.
Inclusive time and space for the final loop to
check for \Vdr{accept} is charged to
the Earley items at location \size{\Cw}.
Overhead is charged to the parse.
All these resource charges are obviously \Oc.

\section{Ruby Slippers parsing}
This top-level code represents a significant change
from previous versions of Earley's algorithm.
\call{Read pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when the scanning of tokens that start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.


\begin{algorithm}[th]
\caption{Initialization}
\label{alg:initial}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add EIM set}{$\dr{start}, 0, 0$}
\label{line:initial-10}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Initialization}
\label{p:initial-op}

\subsection{Initialization complexity}
\label{p:initial-op-complexity}
Inclusive time and space is \Oc{}
and is charged to the parse.

\subsection{Initialization correctness}
\label{p:initial-op-correct}

\begin{theorem}
\label{t:initial-op-correct}
Initialization is correct.
\end{theorem}

\begin{proof}
From Theorem
\ref{t:earley-set-0-is-Leo-free},
we know that Leo memoization has
no effect on the set of
EIM's in Earley set 0.

Since the bottom-up causes of both
reads and reductions
has an input length of greater than 0,
these EIM's cannot appear in Earley set 0.
All EIM's in Earley set 0 are therefore
\begin{itemize}
\item the start Earley item,
\item predictions, or
\item null-scans.
\end{itemize}

The set of start Earley items that we will
add to Earley set 0 is the singleton set
\begin{multline}\label{eq:initial-op-correct}
\left\lbrace \bigl[ [ \Vsym{accept} \de \mydot \Vsym{start} ], 0, 0 \bigr] \right\rbrace \\
\text{where $[ \Vsym{accept} \de \mydot \Vstr{start} ]$ is the start rule.}
\end{multline}
We first show that this set is correct.
It is consistent by theorem \ref{t:start-eim-is-valid}.
It is complete because the start rule is by definition unique.
Therefore the set of start EIM's
added by
Algorithm \ref{alg:initial} to Earley set 0
is correct.

TODO: Prove these next assertions.

The predictions and null-scans
must have a top-down cause in the same
Earley set.
Together, the predictions and null-scans
are exactly the EIM's with null transitions.
So null transition EIM's
must have some other EIM in Earley set 0
as either a direct or an indirect cause.
The start Earley item is
the only remaining possibility,
and it is in Earley set 0.
Therefore the start Earley item is the direct
or indirect cause of all other EIM's in Earley set 0.

From these considerations, we see that Earley 0
consists of the start Earley item and the transitive
closure of null transition from it.
By Theorem \ref{t:ethereal-closure-op-correct},
this is exactly the set of EIM's added to Earley set 0
in line
\ref{line:initial-10}
of Algorithm \ref{alg:initial}.
\end{proof}

\begin{algorithm}[th]
\caption{Read pass}
\label{alg:read-pass}
\begin{algorithmic}[1]
\Procedure{Read pass}{$\Vloc{i},\Vsym{up}$}
\State Note: Each pass through this loop is an EIM attempt
\For{each $\Veim{down} \in \var{transitions}((\var{i} \subtract 1),\Vsym{up})$}
\label{line:read-pass-18}
\State $[\Vdr{down}, \Vloc{origin}] \gets \Veim{down}$
\label{line:read-pass-20}
\State $\Vdr{effect} \gets \GOTO(\Vdr{down}, \Vsym{up})$
\State \Call{Add EIM set}{$\Vdr{effect}, \Vloc{origin}, \Vloc{i}$}
\label{line:read-pass-40}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Read pass}

\section{Read pass}
\label{p:read-op}

\subsection{Read pass complexity}
\label{p:read-op-complexity}

\var{transitions} is a set of tables, one per Earley set.
The tables in the set are indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
since the number of Earley sets grows with
the length of the parse,
it cannot be assumed that Earley sets can be indexed by location
in \Oc{} time.
For the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be in \Oc{} time,
\Vloc{l} must represent a link directly to the Earley set.
In the case of scanning,
the lookup is always in the previous Earley set,
which can easily be tracked in \Oc{} space
and retrieved in \Oc{} time.
Inclusive time and space can be charged to the
\Veim{down}.
Overhead is charged to the Earley set at \Vloc{i}.

\subsection{Read pass correctness}
\label{p:read-op-correct}

\begin{theorem}
\label{t:read-op-correct}
Call the
EIM's at \Vloc{i}
whose bottom-up cause is a terminal
symbol instance,
the ``EIM's read at \Vloc{i}``.
Let the Earley tables for all locations
\Vloc{h},
$\var{h} < \Vloc{i}$ be correct.
Then
Algorithm \ref{alg:read-pass}
at \Vloc{i}
adds all and only
the ethereal closure of
the EIM's read at \Vloc{i}.
\end{theorem}

\begin{proof}
We first assure ourselves that Leo
memoization has no effect on the read pass.
The bottom-up causes
in Algorithm
\ref{alg:read-pass}
are terminal symbol instances.
Leo memoizations are of EIM's ---
terminal instances are not Leo-memoized.

The top-down causes
in Algorithm
\ref{alg:read-pass}
must be quasi-incomplete because it is the result
of the \var{transitions} function,
which only memoizes EIM's with a telluric postdot
symbol ---
see line
\ref{line:memoize-transitions-20}
in Algorithm
\ref{alg:memoize-transitions}.
A quasi-complete EIM cannot have a
telluric postdot symbol,
so therefore
\Veim{down} at line
\ref{line:read-pass-20}
must be quasi-incomplete.
By Theorem
\ref{t:leo-quasi-complete},
no quasi-incomplete EIM
is Leo-memoized.
Therefore none of the EIM's used
in Algorithm
\ref{alg:memoize-transitions}
will be overlooked because
of Leo memoization.

By Theorem \ref{t:read-eim-not-leo},
read EIM's are never Leo-memoized.
We have now shown that none of the parse instances
referenced in Algorithm \ref{alg:read-pass},
are Leo-memoized.
We will therefore ignore Leo memoization in the
rest of this proof.

By the definition of \Cw{},
there is only one
terminal symbol instance
with right location \Vloc{i}.
Without loss of generality, let this instance be
\begin{equation}
\label{eq:read-op-correct-40}
\mkl{\var{i} \subtract 1} \,\, \Vsym{up} \Vmkr{i}
\end{equation}
The EIM's read at \Vloc{i}.
are the EIM's whose bottom-up cause
is \eqref{eq:read-op-correct-40}.

The definition of matching top-down cause
requires that \Veim{down} have a postdot symbol
of \Vsym{up} and a right location of
${\var{i} \subtract 1}$.
By theorem
\ref{t:memoize-transitions-correct},
the \var{transitions} function returns all of these for
use as the value of \Veim{down}
in the loop at line
\ref{line:read-pass-18}.

From this, we observe that
all and only
the matching pairs of causes
for the EIM's read at \Vloc{i}
are used to add EIM's
in the loop at line
\ref{line:read-pass-40}.
Call this the ``cause-correctness'' observation.

From the cause-correctness observation and
Theorem
\ref{t:effect-from-symbolic-causes}
we know that the EIM's we attempt to add
at line
\ref{line:read-pass-40}
are consistent.
From the cause-correctness observation and
Theorem
\ref{t:symbolic-causes-from-effect}
we know that the EIM's we attempt to add
at line
\ref{line:read-pass-40}
are complete.

Since the EIM's we attempt to add
at line
\ref{line:read-pass-40}
are consistent and complete,
we know that,
at line
\ref{line:read-pass-40},
we attempted to add EIM's from
a correct set of causes
for the EIM's read at \Vloc{i}.
By Theorem
\ref{t:ethereal-closure-op-correct}
we know that the
Algorithm
\ref{alg:eim-set}
used
at line
\ref{line:read-pass-40}
adds the ethereal closure of
the EIM that is its argument.
Therefore, we did add
all of, and only, the EIM's
in the ethereal closure of the
EIM's read at \Vloc{i},
as required for the theorem.
\end{proof}

\section{Reduction pass complexity}
\label{p:reduction-op-complexity}

\begin{algorithm}[th]
\caption{Reduction pass}
\label{alg:reduction-pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\State Note: \Vtable{i} may include EIM's added by
\State \hspace{2.5em} by \Call{Reduce one up-cause}{} and
\State \hspace{2.5em} the loop must traverse these
\label{line:reduction-pass-18}
\For{each completed Earley item $\Veim{up} \in \Vtable{i}$}
\label{line:reduction-pass-20}
\State $[\Vdr{up}, \Vloc{origin}, \Vloc{dummy}] \gets \Veim{up}$
\State \Comment It is always the case that $\Vloc{dummy} = \Vloc{i}$
\State \Call{Reduce one up-cause}{\Vloc{i}, \Vloc{origin}, \LHS{\Vdr{lhs}}}
\EndFor
\label{line:reduction-pass-50}
\State \Call{Memoize transitions}{\Vloc{i}}
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Reduction pass}

The loop over \Vtable{i} must also include
any items added
by Algorithm \ref{alg:reduction-pass}.
This can be done by implementing \Vtable{i} as an ordered
set and adding new items at the end.

Exclusive time is clearly \Oc{} per
\Veim{work},
and is charged to the \Veim{work}.
Additionally,
some of the time required
by Algorithm \ref{alg:reduction-pass}
is caller-included,
and therefore charged to this procedure.
Inclusive time from
by Algorithm \ref{alg:reduction-pass}
is \Oc{} per call,
as will be seen in section \ref{p:reduce-one-up-cause},
and is charged to the \Veim{work}
that is current
during that call to
by Algorithm \ref{alg:reduction-pass}.
Overhead may be charged to the Earley set at \Vloc{i}.

\section{Reduction pass correctness}
\label{p:reduction-op-correct}

This section is devoted to showing that
Algorithm \ref{alg:reduction-pass} is correct.
In proving this, we will repeatedly employ two
``environmental''
assumptions.
Later we will show that both of those
assumptions are guaranteed
by the caller of
Algorithm \ref{alg:reduction-pass}.
For now, we will assume them explicitly
when needed.

The first ``environmental'' assumption is
\begin{equation}
\label{eq:reduction-op-correct-5}
\tag{ENV1}
\begin{aligned}
& \text{for all locations \Vloc{h},
and all symbols in \Cg{}, \Vsym{sym},
} \\
& \text{if $\var{h} < \Vloc{current}$,} \\
& \text{then-\Ves{h} is correct} \\
& \qquad \qquad
  \text{and $\var{transitions}(\var{h}, \Vsym{sym})$ is correct.}
\end{aligned}
\end{equation}
and the second ``environmental'' assumption is
\begin{equation}
\label{eq:reduction-op-correct-8}
\tag{ENV2}
\begin{aligned}
& \text{the ethereal closure of the set} \\
& \qquad \qquad \text{of EIM's read at \Vloc{current} is correct.}
\end{aligned}
\end{equation}

\begin{observation}
\label{obs:effect-eim}
For the purposes of this section,
we will call an EIM
added by
Algorithm \ref{alg:reduction-pass}
a ``reduction pass effect''.
From examining the pseudocode,
we observe that reduction pass effects
are added by
Algorithm \ref{alg:reduction-pass}
are added indirectly,
by the calls of \call{Add EIM set}{}
\begin{itemize}
\item
in Algorithm \ref{alg:earley-reduction-op}
at line \ref{line:earley-reduction-op-20}; and
\item
in Algorithm \ref{alg:leo-reduction-op}
at line \ref{line:leo-reduction-op-20}.
\end{itemize}
\end{observation}

\begin{lemma}
\label{lem:reduction-effect-validity}
\ltitle{Reduction Effect validity}
If the causes are valid, every attempt
by Algorithm \ref{alg:leo-reduction-op} to
add a reduction pass effect
adds
the ethereal closure of the
reduction pass effect.
\end{lemma}

\begin{proof}
From Theorem
\ref{t:effect-from-symbolic-causes},
we know that the telluric base EIM's we attempt to add
at line
\ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}
are valid if their causes are valid.
From Theorem
\ref{t:leo-silo-validity},
we know that the telluric base EIM's we attempt to add
at line
\ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}
are valid if their causes are valid.
Combining both cases, we know that
every attempt to
add a telluric base EIM by Algorithm \ref{alg:reduction-pass}
is valid if its causes are valid.

By Theorem
\ref{t:ethereal-closure-op-correct}
we know that the
Algorithm
\ref{alg:eim-set}
used
at line
\ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}
and
at line
\ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}
adds the ethereal closure of the telluric base EIM's
added at those lines.
\end{proof}

\begin{theorem}
\label{t:reduction-op-correct}
Let \Veimset{reduced-base} be
the set of
unmemoized EIM's at \Vloc{current}
which are reductions ---
that is,
EIM's whose bottom-up cause is an EIM.
Let \Veimset{reduced-closure} be
the ethereal closure of \Veimset{reduced}.
If \eqref{eq:reduction-op-correct-5}
and \eqref{eq:reduction-op-correct-8},
then after Algorithm \ref{alg:reduction-pass}
runs,
the set of EIM's at \Vloc{current}
is correct for membership in \Veimset{reduced-closure}.
\end{theorem}

\begin{proof}


\textbf{Top-down correctness}:
Consider an arbitrary top-down cause of
a reduced EIM at \Vloc{current}.
Call this EIM, \Veim{down}.
Because \Veim{down} is the top-down cause
of a reduced EIM
Theorem
\ref{t:right-location-of-top-down-cause}
tells us that
\begin{equation}
\label{eq:reduction-op-correct-18}
\Right{\Veim{down}} < \Vloc{current}.
\end{equation}
From
\eqref{eq:reduction-op-correct-5},
an assumption for the theorem, and
\eqref{eq:reduction-op-correct-18}
we see that \Veim{down} is correct.
Since the choice of \Veim{down} as a top-down cause at
\Vloc{current} was without loss of generality,
we can state that
\begin{equation}
\label{eq:reduction-op-correct-21}
\myparbox{
All of the top-down causes used
by Algorithm \ref{alg:leo-reduction-op} are
correct.
}
\end{equation}

\textbf{Iteration sets}:
For the purposes of this proof we divide the EIM's
added by
Algorithm \ref{alg:reduction-pass} into a sequence
of sets,
\[\var{iter}[0], \var{iter}[1], \ldots \var{iter}[\var{n}].\]
$\var{iter}[0]$ is the set of EIM's
at line
\ref{line:reduction-pass-18} of Algorithm \ref{alg:reduction-pass},
before its main loop
from line
\ref{line:reduction-pass-20}
to line \ref{line:reduction-pass-50}.
$\var{iter}[\var{i}]$ is the set of EIM's
added during the \var{i}'th iteration of the main loop
of Algorithm \ref{alg:reduction-pass}.
Iterations are numbered starting with 1,
so that
$\var{iter}[1]$ is the set of EIM's
added during the first iteration of the main loop.

\textbf{Consistency and altitude}:
To show that
Algorithm \ref{alg:reduction-pass} adds only
valid EIM's,
we proceed by induction on the sets in \var{iter}.
Let the induction hypothesis be that
\begin{equation}
\label{eq:reduction-op-correct-21b}
\myparbox{the EIM's in \var{iter}[\var{i}] are valid, unmemoized
and at altitude \var{i}.}
\end{equation}
At line
\ref{line:reduction-pass-18} of Algorithm \ref{alg:reduction-pass},
\Ves{i} contains only read EIM's and their
ethereal closure.
From \eqref{eq:reduction-op-correct-8},
an assumption for the theorem,
we know that
the EIM's in \var{iter}[0] are valid and unmemoized.
By Definition \ref{def:altitude},
read EIM's have altitude 1.
Using Theorem \ref{t:ethereal-altitude},
we see that
all EIM's in the ethereal closure of read EIM's
have altitude 1.
So all EIM's in \var{iter}[0] have altitude 1.
This shows \eqref{eq:reduction-op-correct-21b} for $\var{i} = 0$,
which is the basis of our induction.

For the step, we assume
\eqref{eq:reduction-op-correct-21b}
and we seek to show that
\begin{equation}
\label{eq:reduction-op-correct-21c}
\myparbox{the EIM's in \var{iter}[\var{i}+1] are valid, unmemoized
and at altitude \var{i}+1.}
\end{equation}
For the EIM's in
$\var{iter}[\var{i}+1]$,
the top-down causes are valid by
\eqref{eq:reduction-op-correct-21}
and the bottom-up causes are valid by the assumption for the
induction step so that,
Lemma \ref{lem:reduction-effect-validity}
we see that
\begin{equation}
\label{eq:reduction-op-correct-23}
\myparbox{
all EIM's in
$\var{iter}[\var{i}+1]$ are valid.
}
\end{equation}

We next show that
all EIM's in
$\var{iter}[\var{i}+1]$
are unmemoized.
Again using
\eqref{eq:reduction-op-correct-21},
the induction step,
and Lemma \ref{lem:reduction-effect-validity},
we see that all of the causes
of EIM's in
$\var{iter}[\var{i}]$ are unmemoized.
We need to show that all of the EIM's in
$\var{iter}[\var{i}+1]$ are unmemoized.
By Theorem \ref{t:memoized-effects},
every memoized
effect either has a memoized cause,
or else has a Leo memo.
So if $\Veim{x} \in \var{iter}[\var{i}+1]$ is
memoized,
\Veim{x} must have a Leo memo.
From line
\ref{line:reduce-one-up-cause-25}
of Algorithm
\ref{alg:reduce-one-up-cause}
and from Theorem
\ref{t:leo-silo-validity},
we see that if \Veim{x} has a Leo memo,
line \ref{line:reduce-one-up-cause-30}
of Algorithm
\ref{alg:reduce-one-up-cause},
will add a valid, ummemoized EIM.
So
\begin{equation}
\label{eq:reduction-op-correct-24}
\myparbox{
every EIM $\Veim{x} \in \var{iter}[\var{i}+1]$ is
unmemoized.
}
\end{equation}

To determine their altitude, we look at
the EIM's added
to $\var{iter}[\var{i}+1]$ by cases.
There are two cases:
\begin{enumerate}
\item
\label{case:reduction-op-correct-24}
Case \ref{case:reduction-op-correct-24}:
Those added at
line \ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}; and
\item
\label{case:reduction-op-correct-25}
Case \ref{case:reduction-op-correct-25}:
those added
at line \ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}.
\end{enumerate}

For Case \ref{case:reduction-op-correct-24},
the EIM's added are
the ethereal closure of
the reduced EIM's whose bottom-up cause
is in $\var{iter}[\var{i}]$.
By Definition \ref{def:altitude}
and Theorem \ref{t:ethereal-altitude},
the EIM's of
Case \ref{case:reduction-op-correct-24}
have altitude $\var{i}+1$.
For Case \ref{case:reduction-op-correct-25},
the EIM's added are
the ethereal closure of
the Leo reduced EIM's whose bottom-up cause
is in $\var{iter}[\var{i}]$.
By Theorems
\ref{t:leo-silo-validity}
and \ref{t:ethereal-altitude},
the EIM's of
Case \ref{case:reduction-op-correct-24}
have altitude $\var{i}+1$.
In both cases, the EIM's added
have altitude $\var{i}+1$,
and therefore
\begin{equation}
\label{eq:reduction-op-correct-29}
\myparbox{
all EIM's in
$\var{iter}[\var{i}+1]$
have altitude $\var{i}+1$.
}
\end{equation}

Using
\eqref{eq:reduction-op-correct-23},
\eqref{eq:reduction-op-correct-24}
and
\eqref{eq:reduction-op-correct-29},
we have
\eqref{eq:reduction-op-correct-21c},
the step of the induction,
and the induction.
From this we conclude that
\begin{equation}
\label{eq:reduction-op-correct-30}
\forall \;
  \var{i}, \text{$\var{iter}[\var{i}]$ is consistent.}
\end{equation}
and
\begin{equation}
\label{eq:reduction-op-correct-31}
\forall \;
  \var{i}, \Alt{\var{iter}[\var{i}} = \var{i}.
\end{equation}

\textbf{Completeness of the iteration sets}:
To show that
after
Algorithm \ref{alg:reduction-pass} every set of
\var{iter} is complete,
we proceed by induction on the sets in \var{iter}.
Let the induction hypothesis be that
\begin{equation}
\label{eq:reduction-op-correct-35}
\myparbox{the EIM's in \var{iter}[\var{i}] are complete.}
\end{equation}
From \eqref{eq:reduction-op-correct-8},
an assumption for the theorem,
we have
\eqref{eq:reduction-op-correct-21b} for $\var{i} = 0$,
and this is the basis of our induction.

For the step, we assume
\eqref{eq:reduction-op-correct-35}
and we seek to show
\begin{equation}
\label{eq:reduction-op-correct-40}
\myparbox{the EIM's in \var{iter}[\var{i}+1] are complete.}
\end{equation}
For the EIM's in
$\var{iter}[\var{i}+1]$,
the top-down causes are complete by
\eqref{eq:reduction-op-correct-21}
and the bottom-up causes are complete by the assumption for the
induction step.
So we have complete sets of unmemoized, valid causes for
both bottom-up causes and top-down causes.

We now show that
Algorithm \ref{alg:reduction-pass}
pairs every bottom-up cause with
all of its matching top-down causes.
From line
\ref{line:reduction-pass-20}
of Algorithm
\ref{alg:reduction-pass}
we see that all valid unmemoized bottom-up causes
are used in an outer loop.
From Theorem
\ref{t:memoize-transitions-correct}
and
line
\ref{line:reduce-one-up-cause-20}
of Algorithm
\ref{alg:reduce-one-up-cause},
we see that each
of these unmemoized bottom-up causes
is paired with the matching set of valid top-down causes.

It remains to show that
no unmemoized, valid EIM's are omitted
from $\var{iter}[\var{i}+1]$
because their causes
are memoized.
By Theorem
\ref{t:memoized-effects},
all valid unmemoized items have valid unmemoized causes except
for non-trivial Leo top items.
Without loss of generality,
let a non-trivial Leo top item be
\Veim{top}.
In the case of \Veim{top},
by Theorem \ref{t:leo-memo-from-top},
there will be an valid unmemoized bottom-up cause
which matches the Leo memo,
so that
line \ref{line:reduce-one-up-cause-30}
of Algorithm
\ref{alg:reduce-one-up-cause}
will add the ethereal closure of \Veim{top}.
Therefore all unmemoized, valid EIM's are added
to
$\var{iter}[\var{i}+1]$.

This shows
\eqref{eq:reduction-op-correct-40},
the step of the induction,
and the induction.
From this we conclude that
\begin{equation}
\label{eq:reduction-op-correct-50}
\forall \;
  \var{i}, \text{$\var{iter}[\var{i}+1]$ is complete.}
\end{equation}

\textbf{Completeness}:
We have shown completeness for all of the Earley sets in
\eqref{eq:reduction-op-correct-50}.
The theorem requires that we show
completeness for \Ves{current} for
membership in \var{reduced-closure}
From
Theorems \ref{eq:reduction-op-correct-30},
and \ref{eq:reduction-op-correct-50},
we know that every iteration set is correct for
membership in \var{reduced-closure} ---
contains all and only the valid EIM's.
By Theorem
\ref{t:altitude-is-finite}
we know that every valid EIM has a defined,
finite altitude,
and by
Theorem \ref{eq:reduction-op-correct-31},
we know every valid EIM is in the
iterations set whose index is the same
as the EIM's altitude.

It remains to show that the iteration sets
produced by
by Algorithm
\ref{alg:reduction-pass} capture all of the
reductions necessary.
The loop starting a line
\ref{line:reduction-pass-20}
of Algorithm
\ref{alg:reduction-pass} stops at the first
empty iteration set.
This is adequate if all iteration sets after
the first empty iteration set are also empty:
\begin{equation}
\label{eq:reduction-op-correct-60}
\var{iter}[\var{i}] = \emptyset \land \var{i} \le \var{j}
    \implies \var{iter}[\var{j}] = \emptyset.
\end{equation}
Showing \eqref{eq:reduction-op-correct-60} is the same
as showing that
\begin{equation}
\label{eq:reduction-op-correct-63}
\nexists \, \var{x},
    \var{iter}[\var{x}] = \emptyset \land
    \var{iter}[\var{x}+1] \neq \emptyset
\end{equation}
Suppose, for a reductio, that there was a
\var{x} that did not satisfy
\eqref{eq:reduction-op-correct-63}.
All EIM's in
$\var{iter}[\var{x}+1]$ will have altitude
$\var{x}+1$
and will be
\begin{enumerate}
\item
\label{case:reduction-op-correct-66}
Case \ref{case:reduction-op-correct-66}:
in the ethereal closure added
by Algorithm \ref{alg:earley-reduction-op}
at line \ref{line:earley-reduction-op-20}; or
\item
\label{case:reduction-op-correct-70}
Case \ref{case:reduction-op-correct-70}:
in the ethereal closure added
by Algorithm \ref{alg:leo-reduction-op}
at line \ref{line:leo-reduction-op-20}.
\end{enumerate}
In Case \ref{case:reduction-op-correct-66},
by Definitiion \ref{def:altitude},
they will require a bottom-up cause with
altitude \var{x}.
In Case \ref{case:reduction-op-correct-70},
also by Definitiion \ref{def:altitude},
they will again require a bottom-up cause with
altitude \var{x}.
But $\var{iter}[\var{x}] = \emptyset$ by assumption
for the reductio.
So there is no bottom-up cause that can create
the EIM's in either
Case \ref{case:reduction-op-correct-66}
or Case \ref{case:reduction-op-correct-70}.
This shows the reductio,
\eqref{eq:reduction-op-correct-63},
and therefore
\eqref{eq:reduction-op-correct-60}.

\begin{sloppypar}
\textbf{Correctness}:
Let
\[ \var{reduction-pass-eims} = \bigcup_\var{i} \var{iter}[\var{i}]. \]
From the considerations in the part on \textbf{Completeness}
we conclude that
\var{reduction-pass-eims}
is complete for \Veimset{reduced-closure}.
From
\eqref{eq:reduction-op-correct-30},
we know that
\var{reduction-pass-eims} is consistent.
Therefore,
\var{reduction-pass-eims}
is correct for \Veimset{reduced-closure}.

\end{sloppypar}
\end{proof}

\begin{algorithm}[th]
\caption{Memoize transitions}
\label{alg:memoize-transitions}
\begin{algorithmic}[1]
\Procedure{Memoize transitions}{\Vloc{i}}
\For{every \Vsym{postdot}, a telluric postdot symbol of $\Ves{i}$}
\label{line:memoize-transitions-20}
\State Note: \Vsym{postdot} is ``Leo-eligible" if it is
\State \hspace\algorithmicindent  Leo unique and its rule is right recursive
\If{\Vsym{postdot} is Leo-eligible}
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to a LEO
\Else
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to the set of EIM's at \Vloc{i} that have
\State \hspace\algorithmicindent \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Memoize transitions}

\section{Memoize transitions}

The \var{transitions} table for \Ves{i}
is built once all EIM's have been
added to \Ves{i}.
We first look at the resource,
excluding the processing of Leo items.
The non-Leo processing can be done in
a single pass over \Ves{i},
in \Oc{} time per EIM.
Inclusive time and space are charged to the
Earley items being examined.
Overhead is charged to \Ves{i}.

We now look at the resource used in the Leo processing.
A transition symbol \Vsym{transition}
is Leo-eligible if it is Leo unique
and its rule is right recursive.
(If \Vsym{transition} is Leo unique in \Ves{i}, it will be the
postdot symbol of only one rule in \Ves{i}.)
All but one of the determinations needed to decide
if \Vsym{transition} is Leo-eligible can be precomputed
from the grammar,
and the resource to do this is charged to the parse.
The precomputation, for example,
for every rule, determines if it is right recursive.

One part of the test for
Leo eligibility cannot be done as a precomputation.
This is the determination whether there is only one dotted
rule in \Ves{i} whose postdot symbol is
\Vsym{transition}.
This can be done
in a single pass over the EIM's of \Ves{i}
that notes the postdot symbols as they are encountered
and whether any is enountered twice.
The time and space,
including that for the creation of a LIM if necessary,
will be \Oc{} time per EIM examined,
and can be charged to EIM being examined.

\begin{theorem}
\label{t:memoize-transitions-correct}
Algorithm \ref{alg:memoize-transitions}
is correct.
\end{theorem}

\begin{proof}
TODO: Make sure this accounts for the correctness of Leo memos.
TODO: Make sure this accounts for the completeness of all top-down causes.
\end{proof}

\begin{algorithm}[th]
\caption{Reduce one up-cause}
\label{alg:reduce-one-up-cause}
\begin{algorithmic}[1]
\Procedure{Reduce one up-cause}{\Veim{up}}
\State Note: Each pass through this loop is an EIM attempt
\State $\Vloc{orig} \gets \Origin{\Veim{up}}$
\State $\Vsym{lhs} \gets \LHS{\Veim{up}}$
\State $\Vloc{current} \gets \Current{\Veim{up}}$
\For{each $\var{down} \in \var{transitions}(\var{orig},\var{lhs})$}
\label{line:reduce-one-up-cause-20}
\State \Comment \var{down} is a ``postdot item'', either a Leo memo or an EIM
\If{\var{down} is a Leo memo}
\label{line:reduce-one-up-cause-25}
\State Perform a \Call{Leo reduction operation}{}
\label{line:reduce-one-up-cause-30}
\State \hspace\algorithmicindent for operands \var{current}, \Vleo{down}
\Else
\State Perform a \Call{Earley reduction operation}{}
\label{line:reduce-one-up-cause-50}
\State \hspace\algorithmicindent for operands \var{current}, \Veim{down}, \var{lhs}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Reduce one op-cause}

\section{Reduce one up-cause}
\label{p:reduce-one-up-cause}

To show that
\begin{equation*}
\var{transitions}(\Vloc{origin},\Vsym{lhs})
\end{equation*}
can be traversed in \Oc{} time,
we note
that the number of symbols is a constant
and assume that \Vloc{origin} is implemented
as a link back to the Earley set,
rather than as an integer index.
This requires that \Veim{work}
in \call{Reduction pass}{}
carry a link
back to its origin.
As implemented, Marpa's
Earley items have such links.

Inclusive time
for the loop over the EIM attempts
is charged to each EIM attempt.
Overhead is \Oc{} and caller-included.

\begin{algorithm}[th]
\caption{Earley reduction operation}
\label{alg:earley-reduction-op}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{current}, \Veim{down}, \Vsym{up}}
\State $\Vdr{effect} \gets \GOTO(\DR{\Veim{down}}, \Vsym{up})$
\State \Call{Add EIM set}{\Vdr{effect}, \Origin{\Veim{down}}, \var{current}}
\label{line:earley-reduction-op-20}
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Earley reduction operation}

\section{Earley Reduction operation}
\label{p:reduction-op}

Exclusive time and space is clearly \Oc.
\call{Earley reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.

\begin{algorithm}[th]
\caption{Leo reduction operation}
\label{alg:leo-reduction-op}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{current}, \Vleo{down}}
\State $[\Vdr{down}, \Vsym{up}, \Vloc{origin}] \gets \Vleo{down}$
\State $\Vdr{effect} \gets \GOTO(\DR{\Vdr{down}}, \Vsym{up})$
\State \Call{Add EIM set}{\Vdr{effect}, \Vloc{origin}, \var{current}}
\label{line:leo-reduction-op-20}
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Leo reduction operation}

\section{Leo reduction operation}
\label{p:leo-op}

Exclusive time and space is clearly \Oc.
\call{Leo reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.

\begin{algorithm}[!htp]
\caption{Add EIM set}\label{alg:eim-set}
\begin{algorithmic}[1]
\Procedure{Add EIM set}{$\Vdr{base}, \Vloc{origin}, Vloc{i}$}
\State $\Veim{confirmed} \gets [\Vdr{base}, \Vloc{origin}]$
\If{\Veim{base} is new}
\State Add \Veim{base} to \Vtable{i}\label{line:eim-set-10}
\EndIf
\State $\Vdrset{next} \gets \GOTO(\Vdr{base}, \epsilon)$\label{line:eim-set-20}
\For{every $\Vdr{next} \in  \Vdrset{next}$}
\If{\Vdr{next} is a quasi-prediction}
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{i}, \Vloc{i}]$
\label{line:eim-set-25}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:eim-set-30}
\EndIf
\Else
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{origin}, \Vloc{i}]$
\label{line:eim-set-35}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:eim-set-40}
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}%
\index{recce-algorithms}{Add EIM set}

\section{Adding a set of Earley items}
\label{p:add-eim-set}

\subsection{Complexity}
\label{p:ethereal-closure-op-complexity}

This operation adds the ethereal closure
of a confirmed EIM
item.
Inclusive time and space is charged to the
calling procedure.

By theorem \ref{t:ethereal-closure-Oc},
computing the ethereal closure is \Oc{}.
We show that other time charged is also \Oc{}
by singling out the two non-trivial cases:
checking that an Earley item is new,
and adding it to the Earley set.
\Marpa{} checks whether an Earley item is new
in \Oc{} time
by using a data structure called a PSL.
PSL's are the subject of Section \ref{s:per-set-lists}.
An Earley item can be added to the current
set in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

The space required for added EIM added is at most
for the \Vdr{base},
one for every transition over a null postdot symbol,
and
one for every prediction.
The number of EIM's that result from
transitions over null postdot symbols is limited
by the maximum length of the RHS of a rule,
which is constant for a given \Cg{}.
At any \Vloc{i}, the number of predictions is
at most the number of rules in \Cg{}.
The number of EIM's that result from
predictions
is therefore constant for a given \Cg{}.
Summing the space, we see that all space
requirements are constant, so that the space
is \Oc{} per call.

\subsection{Prediction EIM complexity}
\label{p:prediction-op-complexity}

Looking specifically at predictions,
From the discussion in
\ref{p:ethereal-closure-op-complexity}, we see that
no time or space is ever charged
to a predicted Earley item.
At most one attempt to add a \Veim{predicted} will
be made per attempt to add a \Veim{confirmed},
so that the total resource charged
remains \Oc.

\subsection{Null transition correctness}
\label{p:prediction-op-correct}

\begin{theorem}
\label{t:ethereal-closure-op-correct}
Algorithm \ref{alg:eim-set} adds all and only the EIM's
for the ethereal closure of \Veim{base}.
\end{theorem}

\begin{proof}
We first examine the effect Leo memoization will
have
on Algorithm \ref{alg:eim-set}.
We note the
Algorithm \ref{alg:eim-set} is only called when
\Veim{base} is not Leo memoized.
By Theorem \ref{t:ethereal-closure-is-Leo-disjoint},
we see that if \Veim{base} is not Leo memoized,
none of the other EIM's in its ethereal closure
will be Leo memoized.
Therefore,
we may ignore Leo memoization in what follows.

By inspection, we see that
Algorithm \ref{alg:eim-set} adds items
at lines
\ref{line:eim-set-10},
\ref{line:eim-set-30}
and \ref{line:eim-set-40}.
That the addition of \Veim{base}
at line \ref{line:eim-set-10}
is complete,
consistent and therefore correct,
follows directly from
inspection of
the pseudo-code.

Other than \Veim{base} itself,
all EIM's in the ethereal closure of
\Veim{base} are the product of
a series of null scans
and predictions.
These operations never change the current location ---
it will always be that of \Veim{base}.
The origin only changes if the operation is a prediction ---
the origin of a predictions is the same as its current
location.
This remains true for the null scan of a predictions ---
its origin is that of its top-down cause, but since
that top-down cause is a prediction,
the origin is the same as if it was a prediction.
By induction, we see that the origin of
all quasi predictions must be the same as
the current location of the prediction.

Let \Veim{new} be a EIM in the ethereal
closure,
other than \Veim{base} itself.
From the preceding analysis,
we see that
the current location of
\Veim{new}
depends
only
on its value in \Veim{base}.
The origin of \Veim{new} depends
on two things:
whether or not \Veim{new} is a quasi-prediction,
and the appropriate location value in \Veim{base}.

There is, therefore,
for each dotted rule,
only one correct set of values
for origin and current location.
These locations are
the ones
used by Algorithm \ref{alg:eim-set}
in lines
\ref{line:eim-set-25}
and \ref{line:eim-set-35}.
Therefore the set of correct EIM's corresponds
one-to-one
with the dotted rules
and, from inspection of
Algorithm \ref{alg:eim-set},
this is the set added by
Algorithm \ref{alg:eim-set}
at lines
\ref{line:eim-set-30}
and \ref{line:eim-set-40}.

It remains to show that the set of dotted rules on
which the EIM's added
at lines \ref{line:eim-set-30}
and \ref{line:eim-set-40}
are based
is correct.
The EIM's added at lines
\ref{line:eim-set-30}
and \ref{line:eim-set-40} are based on
the dotted rules found
at line
\ref{line:eim-set-20}.
By theorem \ref{t:ethereal-closure-dr-correct},
line \ref{line:eim-set-20} of
Algorithm \ref{alg:eim-set} the
transitive closure of null transitions from
the dotted rule \Veim{base} found
by line \ref{line:eim-set-20} is
complete, consistent and therefore correct.
\end{proof}

\begin{theorem}\label{t:quasi-completion-correct}
Let \Veim{base} be a quasi-completion.
If Algorithm \ref{alg:eim-set} adds \Veim{base},
it also adds all null transitions from it,
including the completion EIM.
\end{theorem}

\begin{proof}
For null transitions,
the result follows directly
from theorem \ref{t:ethereal-closure-op-correct}.
By the definition of completion EIM,
a completion EIM is the result of null transition
from any of its quasi-completions,
so that case also follows
from theorem \ref{t:ethereal-closure-op-correct}.
\end{proof}

\begin{theorem}\label{t:prediction-correct}
If Algorithm \ref{alg:eim-set} adds \Veim{base},
it also adds all predictions which are null transitions
from it,
including the valid quasi-prediction EIM's.
\end{theorem}

\begin{proof}
The predictions
and quasi-predictions are
null transitions form \Veim{base},
so the result follows directly
from theorem \ref{t:ethereal-closure-op-correct}.
\end{proof}

\section{Per-set lists}
\label{s:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, x]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a link direct to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to or very similar to PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this document.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
the PSL for every previous Earley set, \Vloc{i},
tracks the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{\Cdr},
which is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's each time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

As before,
where \Ves{i} is an Earley set,
let \Vloc{i} be its location,
and vice versa.
\Vloc{i} is an integer which is
assigned as Earley sets are created.
We can easily assign a zero-based numbering
to the dotted rules of the grammar,
call it $\ID{\Vdr{x}}$,
and this can be used as the integer ID of a dotted rule.
Let $\PSL{\Ves{x}}{\var{y}}$
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}.

Consider the case where Marpa is building \Ves{j}
and wants to check whether Earley item \Veim{x} is new,
where $\Veim{x} = [ \Vdr{x}, \Vorig{x} ]$.
To check if \Veim{x} is new,
Marpa checks
\begin{equation*}
\var{time-stamp} = \PSL{\Ves{x}}{\ID{\Vdr{x}}}
\end{equation*}
If the entry has never been used,
we assume that $\var{time-stamp} = \Lambda$.
If $\var{time-stamp} \ne \Lambda \land \var{time-stamp} = \Vloc{j}$,
then \Veim{x} is not new,
and will not be added to the Earley set.

If $\Vloc{p} = \Lambda \lor \var{time-stamp} \ne \Vloc{j}$,
then \Veim{x} is new.
\Veim{x} is added to the Earley set,
and a new time-stamp is set, as follow:
\begin{equation*}
\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets \Vloc{j}.
\end{equation*}

\section{Complexity summary}

For convenience, we summarize
the complexity results
of this section here,
as theorems.

\begin{theorem}
\label{t:added-eim-charge}
The time and space charged to an Earley item
which is actually added to the Earley sets
is \Oc.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

\begin{theorem}
\label{t:dup-eim-time}
The time charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

For evaluation purposes, \Marpa{} adds a link to
each EIM that records each attempt to
add that EIM,
whether originally or as a duplicate.
Traditionally, complexity results treat parsers
as recognizers, and such costs are ignored.
This will be an issue when the space complexity
for unambiguous grammars is considered.

\begin{theorem}
\label{t:dup-eim-space}
The space charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc{} if links are included,
zero otherwise.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

\begin{theorem}
\label{t:prediction-time}
No space or time is charged to predicted Earley items,
or to attempts to add predicted Earley items.
\end{theorem}

\begin{proof}
As noted in Section \ref{p:add-eim-set},
the time and space used by predicted Earley items
and attempts to add them is charged elsewhere.
\end{proof}

\chapter{Correctness}
\label{ch:correctness}

We are now is a position to show that Marpa is correct.
\begin{theorem}
\label{t:marpa-is-correct}
\textup{ $\myL{\Marpa,\Cg} = \myL{\Cg}$ }
\end{theorem}

\begin{proof}
We proceed by induction on the Earley sets.
As the basis of the induction,
we note that
Algorithm \ref{alg:top}
at line \ref{line:top-20},
calls Algorithm \ref{alg:initial}.
By Theorem
\ref{t:initial-op-correct},
after
line \ref{line:top-20},
Earley set 0 is correct.

As the step of the induction,
we assume that we
are
at line \ref{line:top-30}
of
Algorithm \ref{alg:top},
about to process
the Earley set at \Vloc{i}.
We assume for the induction step
that
\begin{equation}
\label{eq:marpa-is-correct-20}
\forall \, \Vloc{h} \mid 0 \le \var{h} < \Vloc{i} \implies \text{\Vtable{h} is correct.}
\end{equation}

Line
\ref{line:top-33}
executes the read pass for Earley set \var{i}.
From
\eqref{eq:marpa-is-correct-20}
and Theorem
\ref{t:read-op-correct},
we see that,
after the execution
of line
\ref{line:top-33},
\begin{equation}
\label{eq:marpa-is-correct-40}
\text{the ethereal closure of the read EIM's at \Vloc{i} is correct.}
\end{equation}

Next,
line \ref{line:top-40}
executes the reduction pass for Earley set \var{i}.
From
\eqref{eq:marpa-is-correct-20},
\eqref{eq:marpa-is-correct-40}
and Theorem
\ref{t:reduction-op-correct},
we see that,
after the execution
of line
\ref{line:top-40},
\begin{equation}
\label{eq:marpa-is-correct-50}
\text{the ethereal closure of the reduced EIM's at \Vloc{i} is correct.}
\end{equation}

By assumption for the step,
Earley set \Vloc{i} cannot be Earley set 0,
so we know vacously that
\begin{equation}
\label{eq:marpa-is-correct-53}
\text{the ethereal closure of start EIM's at \Vloc{i} is correct.}
\end{equation}
From
\eqref{eq:marpa-is-correct-40},
\eqref{eq:marpa-is-correct-50}
and
\eqref{eq:marpa-is-correct-53}
we see that the ethereal closure of
the set of telluric EIM's at \Vloc{i} is correct.
By theorem
\ref{t:ethereal-closure-op-correct}
we know that the set of ethereal EIM's at \Vloc{i}
is correct.
Since every EIM is either telluric or ethereal,
we know that Earley set \Vloc{i} is correct.
This shows the step of the induction,
and the induction.

We know from the induction
and line \ref{line:top-30}
of Algorithm \ref{alg:top},
that the Earley set at \loc{\Vsize{w}}
is correct.
Therefore,
by Theorem
\ref{t:algorithm-correct},
it contains
the accept EIM if and only if
\Cw{} is in the language of the grammar,
$\var{L}(\Cg)$, so that
\begin{equation}
\label{eq:marpa-is-correct-60}
\Veim{accept} \in \Vtables{Marpa} \equiv \Cw \in \var{L}(\Cg).
\end{equation}

From \eqref{eq:def-implementation-accepts},
we know that an algorithm accepts an input if
and only if the accept EIM is in its tables.
By Theorem \ref{t:accept-eim-not-memoized},
we know that the accept EIM is not memoized.
Using
\eqref{eq:def-implementation-accepts} and
\eqref{eq:marpa-is-correct-60}, we have
\begin{equation*}
\Cw{} \in \var{L}(\alg{Marpa}, \Cg) \equiv \Cw \in \var{L}(\Cg).
\end{equation*}
\end{proof}

\chapter{Linear complexity results}
\label{ch:linear}

\section{Nulling symbols}
\label{s:nulling}

Recall that Marpa grammars,
without loss of generality,
contain neither empty rules or
properly nullable symbols.
This corresponds directly
to a grammar rewrite in the \Marpa{} implementation,
and its reversal during \Marpa's evaluation phase.
For the correctness and complexity proofs in this document,
we assume an additional rewrite,
this time to eliminate nulling symbols.

Elimination of nulling symbols is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether a grammar \Cg{} accepts
an input \Cw{}
will not depend on the nulling symbols in its rules.

In its implementation,
\Marpa{} does not directly rewrite the grammar
to eliminate nulling symbols.
But nulling symbols are ignored in
creating the dotted rules,
and must be restored during \Marpa's evaluation phase,
so that the implementation and
this simplification for theory purposes
track each other closely.

\section{Complexity of each Earley item}

For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
We showed that this rewrite
is without loss of generality
in Section \ref{s:nulling},
when we examined correctness.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

\begin{lemma}\label{l:nulling-rewrite}
All time and space required
to rewrite the grammar to eliminate nulling
symbols, and to restore those rules afterwards
in the Earley sets,
can be allocated
to the Earley items
in such a way that each Earley item
requires \Oc{} time and space.
\end{lemma}

\begin{proof}
The time and space used in the rewrite is a constant
that depends on the grammar,
and is charged to the parse.
The reversal of the rewrite can be
done in a loop over the Earley items,
which will have time and space costs
per Earley item,
plus a fixed overhead.
The fixed overhead is \Oc{}
and is charged to the parse.
The time and space per Earley item
is \Oc{}
because the number of
rules into which another rule must be rewritten,
and therefore the number of Earley items
into which another Earley item must be rewritten,
is a constant that depends
on the grammar.
\end{proof}

\begin{theorem}\label{t:O1-time-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item,
and each attempt to
add a duplicate Earley item,
requires \Oc{} time.
\end{theorem}

\begin{theorem}\label{t:O1-space-per-eim}
All space in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item
requires \Oc{} space and,
if links are not considered,
each attempt to add a duplicate
Earley item adds no additional space.
\end{theorem}

\begin{theorem}\label{t:O1-links-per-eim}
If links are considered,
all space in \Marpa{} can be allocated
to the Earley items
in such a way that each Earley item
and each attempt to
add a duplicate Earley item
requires \Oc{} space.
\end{theorem}

\begin{proof}[Proof of Theorems
\ref{t:O1-time-per-eim},
\ref{t:O1-space-per-eim},
and \ref{t:O1-links-per-eim}]
These theorems follows from the observations
in Section \ref{ch:pseudocode}
and from Lemma \ref{l:nulling-rewrite}.
\end{proof}

The same complexity results apply to \Marpa{} as to \Leo,
and the proofs are very similar.
\Leo's complexity results~\cite{Leo1991}
are based on charging
resource to Earley items,
as were the results
in Earley's paper~\cite{Earley1970}.

Earley~\cite{Earley1970} shows that,
for unambiguous grammars,
every attempt to add
an Earley item will actually add one.
In other words, there will be no attempts to
add duplicate Earley items.
Earley's proof shows that for each attempt
to add a duplicate,
the causation must be different ---
that the EIM's causing the attempt
differ in either their dotted
rules or their origin.
Multiple causations for an Earley item
would mean multiple derivations
for the sentential form that it represents.
That in turn would mean that
the grammar is ambiguous,
contrary to assumption.

\begin{theorem}
\label{t:tries}
For an unambiguous grammar,
let \var{z} be the number of EIM's
in all the Marpa tables,
\begin{equation*}
    \var{z} = \Rtablesize{\Marpa}.
\end{equation*}
and let \var{tries} be the
number of attempts to add
Leo memos and Earley items.
Then \[\var{tries} = \order{\var{z}}.\]
\end{theorem}

\begin{proof}
Let
\var{leo-tries} be the number of attempt to add Leo memos,
\var{telluric-tries} be the number of attempts to add telluric EIM's,
and \var{ethereal-tries}
be the number of attempts to add ethereal EIM's.
Then
\begin{multline}
\label{eq:tries-5}
\var{tries} =
\var{telluric-tries} + \var{ethereal-tries} + \var{leo-tries}.
\end{multline}

We consider first the telluric EIM's.
Let
\var{start-tries} be the number of attempts to add the start EIM,
\var{read-tries} be the number of attempts to add read EIM's,
and \var{reduction-tries} be the number of attempts to add reduction EIM's.
\begin{multline}
\label{eq:tries-10}
\var{telluric-tries} = \\
\var{read-tries} + \var{reduction-tries} + \var{start-tries},
\end{multline}

Let \var{start-tries} be the number of attempts to add the initial item to
the Earley sets.
It is clear from the pseudocode
that there will be no attempts to add duplicate start EIM's.
and that
\begin{equation}
\label{eq:tries-15}
\var{start-tries} = 1 = \Oc
\end{equation}

Let \var{read-tries} be the number of attempted read operations in
Earley set \Vloc{j}.
Marpa attempts a read operation,
in the worst case,
once for every EIM in the Earley set
at $\Vloc{i} \subtract 1$.
Therefore, the number of attempts
to add read EIM's at Earley set \Vloc{i}
must be less than equal to \bigsize{\Etable{\var{i} \subtract 1}},
and
the number
of actual Earley items at
$\Vloc{i} \subtract 1$.
\begin{equation}
\label{eq:tries-18}
\var{read-tries} \le \sum\limits_{i=1}^{n}{ \bigsize{\Etable{\decr{i}}} } = \order{\var{z}}
\end{equation}

The most complicated case is Earley reduction.
Recall that \Ves{i} is the current Earley set.
Consider the number of reductions attempted.
\Marpa{} attempts to add an Earley reduction result
once for every pair of matching causes,
\begin{equation*}
[\Veim{down}, \Veim{up}].
\end{equation*}
Let
\begin{equation*}
\begin{split}
& \Veim{up} = [ \Vdr{up}, \Vloc{up-origin}, \var{i} ]  \\
 \land \quad & \Vsym{transition} = \LHS{\Vdr{up}}. \\
\end{split}
\end{equation*}

We now attempt to
put an upper bound on number of possible matching pairs of
causes.
We have $\Veim{up} \in \Ves{i}$,
and therefore
\begin{equation}
\label{eq:tries-20}
\text{there are at most
$\bigsize{\Vtable{i}}$ choices for \Veim{up}.}
\end{equation}

Let the number of dotted rules be \Vsize{\Cdr}.
We can show that the number of possible choices of
\Veim{down} is at most \Vsize{\Cdr}, by a reductio.
Suppose, for the reductio,
there were more than \Vsize{dr} possible choices of \Veim{down}.
Then there are two possible choices of \Veim{down} with
the same dotted rule.
Call these \Veim{choice1} and \Veim{choice2}.
We know, by the definition of matching causes, that
\begin{align*}
 & \Veim{down} \in \Ves{up-origin}
   \quad \text{and therefore we have} \\
 & \Veim{choice1} \in \Ves{up-origin} \quad \text{and} \\
 & \Veim{choice2} \in \Ves{up-origin}.
\end{align*}
Since all EIM's in an Earley set must differ,
and
\Veim{choice1} and \Veim{choice2} both share the same
dotted rule,
so they must differ in their origin.
But two different origins would produce two different derivations for the
reduction,
and by
Theorem \ref{eq:def-implementation-accepts},
this would mean that the parse was ambiguous.
This is contrary to the assumption for the theorem
that the grammar is unambiguous.
This shows the reductio
and that
\begin{equation}
\label{eq:tries-25}
\myparbox{
the number of choices for \Veim{down},
compatible with \Vorig{up}, is as most \Vsize{dr}.
}
\end{equation}

From
\eqref{eq:tries-20}
and
\eqref{eq:tries-25},
we see that the number of possible matching
cause pairs for reductions at the Earley set at \Vloc{i}
is
\begin{equation}
\label{eq:tries-27}
\Vsize{\Cdr} \times \bigsize{\Vtable{i}}.
\end{equation}
Since \Vsize{\Cdr} is a constant that depends on \Cg{},
\begin{align}
\label{eq:tries-28}
\var{reduction-tries} & = \sum\limits_{i=0}^{n}{\Vsize{\Cdr} \times \bigsize{\Vtable{i}}} \\
  & = \Vsize{\Cdr} \times \sum\limits_{i=0}^{n}{\bigsize{\Vtable{i}}} \\
  & = \Oc \times \order{\var{z}} \\
  & = \order{\var{z}}
\end{align}

We have now set a limit on the number of attempts
for all of the telluric EIM's.
We return to
\eqref{eq:tries-10}.
Using
\eqref{eq:tries-15},
\eqref{eq:tries-18}
and
\eqref{eq:tries-28},
\begin{align}
\label{eq:tries-30}
\var{telluric-tries} & = \var{read-tries} + \var{reduction-tries} \\
\notag & \qquad \qquad + \var{start-tries}  \\
  & = \Oc + \order{\var{z}} + \order{\var{z}} \\
  & = \order{\var{z}}
\label{eq:tries-33}
\end{align}

Recall that \var{ethereal-tries} was the number of attempts to add ethereal
items in
Earley set \Vloc{j}.
\Marpa{} includes ethereal closure
in its read and reduction operations.
Ethereal EIM's are only added if their telluric base EIM
is new,
so that there will never be duplicate attempts
to add ethereal EIM's for the same telluric base EIM.
The ethereal EIM's which might be added for a given
telluric base are predictions and null-scans.
The number of null-scans is limited by the RHS length,
call it \var{rhs-max},
of the longest rule in \Cg.
The number of predictions is limited by the number of rules
in Cg{}, \Vsize{\Crules}.
so that
for every telluric Earley item added,
\begin{equation*}
\var{ethereal-tries} = (\var{rhs-max} + \Vsize{Crules})  \times \var{telluric-tries}
\end{equation*}
and because \var{rhs-max} and
\Vsize{\Crules} are constants the depend on \Cg{},
\begin{equation}
\label{eq:tries-42}
\var{ethereal-tries} = \Oc \times \var{telluric-tries}.
\end{equation}
And using
\eqref{eq:tries-30},
\begin{equation}
\label{eq:tries-45}
\var{ethereal-tries} = \order{\var{z}}.
\end{equation}

Recall that \var{leo-tries} was the number of attempted Leo reductions in
Earley set \Vloc{j}.
For Leo reduction,
we note that by its definition,
duplicate attempts at Leo reduction cannot occur.
From the pseudo-code of Sections \ref{p:reduce-one-up-cause}
and \ref{p:leo-op},
we know there will be at most one Leo reduction for
every Earley item in the Earley tables,
so that
\Vloc{j}.
\begin{equation}
\label{eq:tries-50}
\var{leo-tries} = \order{\var{z}}
\end{equation}

Returning to
\eqref{eq:tries-5}:
\begin{multline}
\var{tries} =
\var{telluric-tries} + \var{ethereal-tries} + \var{leo-tries}.
\end{multline}
We have, using the results of
\eqref{eq:tries-33},
\eqref{eq:tries-45}
and
\eqref{eq:tries-50},
\begin{equation}
\var{tries} =
\order{\var{z}}
+ \order{\var{z}}
+ \order{\var{z}}
= \order{\var{z}}.\qedhere
\end{equation}
\end{proof}

As a reminder,
we follow tradition by
stating complexity results in terms of \var{n},
setting $\var{n} = \Vsize{\Cw}$,
the length of the input.

\begin{theorem}\label{t:leo-right-recursion}
Either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let the constant \var{c} be the number
of symbols.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
(Since for the purposes of these
complexity results we ignore nulling symbols,
the rightmost symbol of a string will also be its rightmost
telluric symbol.)
So part of the rightmost derivation must take the form
\begin{equation*}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation*}
But the first step of this derivation sequence must use a rule of the
form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

TODO: Account for effect of not-memoizing at location 0.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in~\cite[p. 173]{Leo1991},
the number of Earley items produced by
\Leo{} when parsing input \Cw{} with an LR-regular grammar \Cg{} is
\begin{equation*}
\order{\Vsize{\Cw}} = \order{\var{n}}.
\end{equation*}
\Marpa{} may produce more Earley items than \Leo{}
because
\Marpa{} does not apply Leo memoization to Leo sequences
which do not contain right recursion.

By the definition of an EIM,
and the construction of a Leo sequence,
it can be seen that a Leo sequence
corresponds step-for-step with a
right derivation.
It can therefore be seen that
the number of EIM's in the Leo sequence
and the number of right derivation steps
in its corresponding right derivation
will be the same.

Consider one EIM that is memoized in \Leo{}.
If not memoized because it is not a right recursion,
this EIM will be expanded to a sequence
of EIM's.
How long will this sequence of non-memoized EIM's
be, if we still continue to memoize EIM's
which correspond to right recursive rules?
The EIM sequence, which was formerly a memoized Leo sequence,
will correspond to a right
derivation that does not include
any steps that use right recursive rules.
By Theorem \ref{t:leo-right-recursion},
such a
right derivation can be
of length at most \var{c1},
where \var{c1} is a constant that depends on \Cg{}.
As noted, this right derivation has
the same length as its corresponding EIM sequence,
so that each EIM not memoized in \Marpa{} will expand
to at most \var{c1} EIM's.

The number of EIM's per Earley set
for an LR-regular grammar in a \Marpa{} parse
is less than
\begin{equation*}
    \var{c1} \times \order{\var{n}} = \order{\var{n}}.
\end{equation*}

LR-regular grammars are unambiguous, so that
by Theorem \ref{t:tries},
the number of attempts that \Marpa{} will make to add
EIM's is less than or equal to
\var{c2} times the number of EIM's,
where \var{c2} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is
\begin{equation*}
    \var{c2} \times \order{\var{n}}
    = \order{\var{n}}.\qedhere
\end{equation*}
\end{proof}

\chapter{Other complexity results}
\label{ch:other-complexity}

\begin{theorem}
\label{t:eim-count}
For a context-free grammar,
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} = \order{\var{n}^2}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
By Theorem \ref{t:es-count},
the size of the Earley set at \Vloc{i}
is $\order{\var{i}}$.
Summing over the length of the input,
$\Vsize{\Cw} = \var{n}$,
the number of EIM's in all of \Marpa's Earley sets
is
\begin{equation*}
\sum\limits_{\Vloc{i}=0}^{\var{n}}{\order{\var{i}}}
= \order{\var{n}^2}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:ambiguous-tries}
For a context-free grammar,
the number of attempts to add
Earley items is $\order{\var{n}^3}$.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries},
we see that the only bound that required
the assumption that \Cg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Recall that \var{z} was the total number
of Earley items in the Earley tables.
By Theorem \ref{t:eim-count},
\begin{equation*}
\var{z} = \Rtablesize{\Marpa} = \order{\var{n}^2}.
\end{equation*}

Looking again at \var{reduction-tries}
for the case of ambiguous grammars,
the only place we used the assumption that \Cg{}
was unambiguous was in
the count of matching pairs.
\begin{equation*}
[\Veim{down}, \Veim{up}].
\end{equation*}
We need to look at this again.
We did not use the fact that the grammar was unambigous in counting
the possibilities for \Veim{up}, but
we did make use of it in determining the count of possibilities
for \Veim{down}.
We know still know that
\begin{equation*}
\Veim{down} \in \Ves{up-origin},
\end{equation*}
where
\Vloc{up-origin} is the origin of \Veim{up}.

In the proof of Theorem \ref{t:tries},
we calculated
the number of possible matching
cause pairs for reductions at the Earley set at \Vloc{i}
in
\eqref{eq:tries-27}
as
\begin{equation}
\label{eq:ambiguous-tries-27}
\Vsize{\Cdr} \times \bigsize{\Vtable{i}},
\end{equation}
where
\Vsize{\Cdr} was the number of dotted rules.
In showing that the number of dotted rules bounded the choice
of \Veim{down},
we used the assumption that \Cg{} was unambiguous.
Relaxing this assumption,
and looking at the worst case,
every EIM in \Ves{up-origin} is a possible
match, so that
the number of possibilities for \Veim{down} now grows to
\size{\Ves{up-origin}}, and
\eqref{eq:ambiguous-tries-27} becomes
\begin{equation}
\label{eq:ambiguous-tries-30}
\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}.
\end{equation}

Revisiting \eqref{eq:tries-28},
it now becomes
\begin{equation}
\label{eq:ambiguous-tries-28}
\var{reduction-tries} = \sum\limits_{i=0}^{n}{\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}}.
\end{equation}
Our best bound for the size of the Earley sets involved comes from
Theorem \ref{t:es-count}, and is
$\Vsize{\Cw} = \var{n}$,
so that
\eqref{eq:ambiguous-tries-28}
becomes
\begin{align}
\label{eq:ambiguous-tries-33}
\var{reduction-tries} & = \sum\limits_{i=0}^{n}{\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}} \\
& = \order{\var{n}^3}.
\label{eq:ambiguous-tries-35}
\end{align}

We now reexamine the count of the number
of attempts to add Leo memos and Earley items,
this time without assuming ambiguity.
Our worst case cannot be worse than the sum of
the entire count under the assumption of unambiguity,
plus any counts that needed to be revised
when the assumption is relaxed.
The count assuming unambiguity was
$\order{\var{z}}$,
and from Theorem \ref{t:es-count} we know
that
\begin{equation}
\label{eq:ambiguous-tries-36}
\order{\var{z}} = \order{\var{n}^2}.
\end{equation}
The one count that we needed to revise
in order to relax the assumption of unambiguity is
\eqref{eq:ambiguous-tries-35}.

Therefore the count for the ambiguous case is not worse
than
\begin{equation*}
\var{tries} = \order{\var{n}^2} + \order{\var{n}^3} = \order{\var{n}^3}.
\qedhere
\end{equation*}
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
By assumption, \Cg{} is unambiguous, so that
by Theorem \ref{t:tries},
and Theorem \ref{t:eim-count},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{}
for unambiguous grammars is \order{\var{n}^2}.
\end{proof}

\begin{FlushLeft}
\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}
\end{FlushLeft}

\begin{proof}
By Theorem \ref{t:O1-time-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\begin{FlushLeft}
\begin{theorem}\label{t:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}
\end{FlushLeft}

\begin{proof}
By Theorem \ref{t:O1-space-per-eim}
and Theorem \ref{t:eim-count}.
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{t:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{FlushLeft}
\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}
\end{FlushLeft}

\begin{proof}
By Theorem \ref{t:O1-links-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\chapter{Acknowledgements}
\label{ch:Acknowledgements}

Ruslan Shvedov%
\index{recce-general}{Shvedov, Ruslan}
and
Ruslan Zakirov%
\index{recce-general}{Zakirov, Ruslan}
made many useful suggestions
that are incorporated in this paper.
Many members of the Marpa community
have helped me in many ways,
and it is risky
to single out one of them.
But Ron Savage%
\index{recce-general}{Savage, Ron}
has been unstinting in
his support.

\bibliographystyle{plain}

\begin{thebibliography}{10}
\RaggedRight

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Compiling
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620--630.

\bibitem{Culik1973}
{\v{C}}ulik, Karel and Cohen, Rina.
\newblock LR-Regular grammarsâ€”an extension of LR (k) grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51--55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Marpa-2013}
Jeffrey~Kegler.
\newblock Marpa, a practical general parser: the recognizer.
\newblock \url{https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2015: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\bibitem{Wich2005}
Klaus Wich.
\newblock Ambiguity functions of context-free grammars and languages.
\newblock Ph.D. Thesis, Universit{\"a}t Stuttgart, 2005.
\newblock \url{http://elib.uni-stuttgart.de/opus/volltexte/2005/2282}

\end{thebibliography}

\clearpage
\def\indexname{General index}
\printindex{recce-general}

\clearpage
\def\indexname{Index of algorithms}
\printindex{recce-algorithms}

\clearpage
\def\indexname{Index of theorems, and lemmas}
\printindex{recce-theorems}

\clearpage
\def\indexname{Index of abbreviations and definitions}
\printindex{recce-definitions}

\clearpage
\def\indexname{Index of notation}
\printindex{recce-notation}

\tableofcontents

\end{document}

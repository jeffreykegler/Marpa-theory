% Copyright 2015 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}

% This is now a "paper", but may be a chapter
% or something else someday
% This command will make any such change easier.
\newcommand{\doc}{paper}

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{#1}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\rightarrow}
\newcommand{\derives}{\Rightarrow}
\newcommand{\nderives}{\not\Rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }
\newcommand{\bool}[1]{\var{#1}_{BOOL}}
\newcommand{\Vbool}[1]{\ensuremath{\bool{#1}}}
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\Ees}[1]{\ensuremath{#1_{ES}}}
\newcommand{\Vlim}[1]{\ensuremath{\var{#1}_{LIM}}}
\newcommand{\Eloc}[1]{\ensuremath{{#1}_{LOC}}}
\newcommand{\Vloc}[1]{\Eloc{\var{#1}}}
\newcommand{\Ves}[1]{\Ees{\var{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vruleset}[1]{\ensuremath{\var{#1}_{\set{RULE}}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\Vstr}[1]{\ensuremath{\var{#1}_{STR}}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\Vsym}[1]{\ensuremath{\var{#1}_{SYM}}}
\newcommand{\Vorig}[1]{\ensuremath{\var{#1}_{ORIG}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}

\newcommand{\Cdr}{\var{dr}}
\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\sym{\Cw[\var{#1}]}}}
\newcommand{\Crules}{\var{rules}}
\newcommand{\GOTO}{\mymathop{GOTO}}
\newcommand{\Next}[1]{\mymathop{Next}(#1)}
\newcommand{\Previous}[1]{\mymathop{Previous}(#1)}
\newcommand{\Predict}[1]{\mymathop{Predict}(#1)}
\newcommand{\Predot}[1]{\mymathop{Predot}(#1)}
\newcommand{\Postdot}[1]{\mymathop{Postdot}(#1)}
\newcommand{\Penult}[1]{\mymathop{Penult}(#1)}
\newcommand{\LHS}[1]{\mymathop{LHS}(#1)}
\newcommand{\RHS}[1]{\mymathop{RHS}(#1)}
\newcommand{\LeoEligible}[1]{\mymathop{Leo-Eligible}(#1)}
\newcommand{\LeoUnique}[1]{\mymathop{Leo-Unique}(#1)}
\newcommand{\ID}[1]{\mymathop{ID}(#1)}
\newcommand{\PSL}[2]{\mymathop{PSL}[#1][#2]}
\newcommand{\myL}[1]{\mymathop{L}(#1)}
\newcommand\Etable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\bigEtable[1]{\ensuremath{\mymathop{table}\bigl[#1\bigr]}}
\newcommand\Rtable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\Rtablesize[1]{\ensuremath{\bigl| \mymathop{table}[#1] \bigr|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\mymathop{table}[#1,#2]}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}[theorem]{Observation}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

\begin{document}

\date{\today}

\title{The Marpa recognizer: a simplification}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2015 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
Marpa is
a practical and fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
The Marpa recognizer is the first
practical implementation
of the improvements
to Earley's algorithm found in
Joop Leo's 1991 paper.
Marpa has a new parse engine that
allows the user to alternate
between Earley-style syntax-driven parsing,
and procedural parsing of the type available
with recursive descent.
Marpa is left-eidetic, so that,
unlike in recursive descent,
the procedural logic has available
full information about
the state of the parse so far.
The Marpa recognizer described
is a simplification of that
in our 2013 paper\cite{Marpa-2013}.
\end{abstract}

\maketitle

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a highly available tool
like those that exist for LALR\cite{Johnson} or
regular expressions.
The Marpa project was intended
to take the best results from the literature
on Earley parsing off the pages
of the journals and bring them
to a wider audience.
This paper describes the algorithm used
in the most recent version of Marpa::R2\cite{Marpa-R2}.
It is a simplification of the one presented in an earlier paper\cite{Marpa-2013}.

As implemented,
Marpa parses,
without exception,
all context-free grammars.
Time bounds are the best of Leo\cite{Leo1991}
and Earley\cite{Earley1970}.
The Leo bound,
\On{} for LR-regular grammars,
is especially relevant to
Marpa's goal of being a practical parser:
If a grammar is in a class of grammar currently in practical use,
Marpa parses it in linear time.

Error-detection properties,
extremely important,
have been overlooked in the past.
Marpa breaks new ground in this respect.
Marpa has the immediate error detection property,
but goes well beyond that:
it is fully aware of the state of the parse,
and can report this to the user while tokens are
being scanned.

Marpa allows the lexer to check its list
of acceptable tokens before a token is scanned.
Because rejection of tokens is easily and
efficiently recoverable,
the lexer is also free to take an event-driven
approach.
Error detection is no longer
an act of desperation,
but a parsing technique in its own right.
If a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be described
as making the parser's
``wishes'' come true,
and we have called this
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of the Marpa::R2\cite{Marpa-R2},
the author has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a very simply and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

Section
\ref{s:preliminaries} describes the notation and conventions
of this \doc.
Section \ref{s:rewrite} deals with Marpa's
grammar rewrites.
Sections \ref{s:earley} and \ref{s:earley-ops}
introduce Earley's algorithm.
Section \ref{s:leo} describes Leo's modification
to Earley's algorithm.
Section \ref{s:pseudocode} presents the pseudocode
for Marpa's recognizer.
Section
\ref{s:proof-preliminaries}
describes notation and deals with other
preliminaries
to the theoretical results.
Section
\ref{s:correct}
contain a proof of Marpa's correctness,
while Section \ref{s:complexity} contains
its complexity results.

\section{Preliminaries}
\label{s:preliminaries}
\label{s:start-prelim}

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.
This \doc{} will
use subscripts to indicate commonly occurring types.
\begin{center}
\begin{tabular}{ll}
$\var{X}_T$ & The variable \var{X} of type $T$ \\
$\var{set-one}_\set{T}$ & The variable \var{set-one} of type set of $T$ \\
$SYM$ & The type for a symbol \\
\Vsym{a} & The variable \var{a} of type $SYM$ \\
\Vsymset{set-two} & The variable \var{set-two}, a set of symbols \\
\end{tabular}
\end{center}
Subscripts may be omitted when the type
is obvious from the context.
The notation for
constants is the same as that for variables.
Multi-character variable names will be common,
and operations will never be implicit.
\begin{center}
\begin{tabular}{ll}
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \cat \var{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\end{tabular}
\end{center}
Type names are often used in the text
as a convenient way to refer to
their type.

Where \Vsymset{syms} is non-empty set of symbols,
let $\var{syms}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Where \Vstr{s} is a string,
let \size{\Vstr{s}} be its length, counted in symbols.
Let $\var{syms}^+$ be
\begin{equation*}
\bigl\{ \Vstr{x}
\bigm| \Vstr{x} \in \var{syms}* \land \Vsize{\Vstr{x}} > 0
\bigr\}.
\end{equation*}

In this \doc{} we use,
without loss of generality,
the grammar \Cg{},
where \Cg{} is the 4-tuple
\begin{equation*}
    (\Vsymset{nt}, \Vsymset{term}, \var{rules}, \Vsym{accept}).
\end{equation*}
\Vsymset{nt} is a set of symbols called non-terminals,
and \Vsymset{term} is a set of symbols called terminals.
Here $\Vsym{accept} \in \var{nt}$.
Call the language of \var{g}, $\myL{\Cg}$,
where $\myL{\Cg} \subseteq \var{term}^\ast$.
The vocabulary of the the grammar is the union of
the sets of terminals and non-terminals:
$$ \Vsymset{vocab} = \Vsymset{nt} \cup \Vsymset{term}. $$
If a string of symbols contains only terminal symbols,
that string is called a \dfn{sentence}.

\Vruleset{rules} is a set of rules (type \type{RULE}),
where a rule is a duple
of the form $[\Vsym{lhs} \de \Vstr{rhs}]$,
such that
\begin{equation*}
\Vsym{lhs} \in \var{nt} \quad \text{and}
\quad \Vstr{rhs} \in \var{vocab}^+.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\Vrule{r}}$ and $\RHS{\Vrule{r}}$, respectively.
This definition follows \cite{AH2002},
which departs from tradition by disallowing an empty RHS.

The rules imply the traditional rewriting system,
in which $\Vstr{x} \derives \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in exactly one step;
$\Vstr{x} \deplus \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in one or more steps;
and $\Vstr{x} \destar \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in zero or more steps.

We say that symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} \destar \epsilon$.
This implies that $\Vstr{y} \derives \epsilon$ if and only
if all of the symbols in \Vstr{y} are nullable.
As a special case,
when \Vstr{z} is an empty string of symbols,
we will say that $\Vstr{z} \derives \epsilon$.

\Vsym{x} is \dfn{nonnull} if and only if it is not nullable.
Following Aycock and Horspool\cite{AH2002},
all nullable symbols in grammar \Cg{} are nulling -- every symbol
which can derive the null string always derives the null string.
It is shown in \cite{AH2002} how to do this without losing generality
or the ability to efficiently evaluate a semantics that is
defined in terms of an original grammar that includes symbols which
are both nullable and non-nulling,
empty rules, etc.

Also without loss of generality,
it is assumed
that there is a dedicated acceptance rule, \Vrule{accept}
and a dedicated acceptance symbol, $\Vsym{accept} = \LHS{\Vrule{accept}}$,
such that
for all \Vrule{x},
\begin{equation*}
\begin{split}
& \Vsym{accept} \notin \RHS{\Vrule{x}} \\
\land \quad & (\Vsym{accept} = \LHS{\Vrule{x}} \implies \Vrule{accept} = \Vrule{x}).
\end{split}
\end{equation*}
We further assume that every symbol is productive --
that is, that it derives a sentence.

Let the input to
the parse be \Cw{} such that $\Cw \in \var{term}^+$.
Locations in the input will be of type \type{LOC}.
Let \Vsize{w} be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.
Let \CVw{i} be character
at position \var{i}
of the input.
String position is zero-based,
so that
$0 \le \Vloc{i} < \Vsize{w}$.
Let $\var{w}[\var{a}, \var{b}]$
be the contiguous substring
from position \var{a} to
position \var{b}, inclusive,
so that
$$ \bigsize{\var{w}[\var{a}, \var{b}]} = (\var{b} \subtract \var{a}) + 1. $$

The alert reader may have noticed that the previous definition
of \Cw{} did not allow zero-length inputs.
To simplify the mathematics, we exclude null parses
and trivial grammars from consideration.
In its implementations,
the Marpa parser
deals with null parses and trivial grammars as special cases.
(Trivial grammars are those that recognize only the null string.)

In the context of a grammar \Cg{} and an input \Cw{},
we will often use location-marked derivations and derivation steps.
Location-marked derivation steps are like the derivation steps
of the traditional rewriting system except that they also contain
location markers of the form $[\var{x}]$, where \var{x} is a
location in \Cw{}.
In its most general form,
a derivation step with a single location marker is
$$ \Vstr{pre} \,[\var{x}]\, \Vstr{post}. $$
It means
\begin{equation}
\label{eq:location-marker-def}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vstr{pre} \cat \Vstr{post} \cat \Vstr{after} \\
&  \land \Vstr{before} \cat \Vstr{pre} \destar \var{w}[0, (\var{x} \subtract 1)] \\
&  \land \Vstr{post} \cat \Vstr{after} \destar \var{w}[\var{x}, (\Vsize{\Cw} \subtract 1)]
\end{split}
\end{equation}

Derivations may have many location markers.
The meaning of a derivation with \var{j} different location markers,
$$ \var{m}[1], \var{m}[2] \ldots \var{m}[j], $$
is the same as the meaning of the conjunction of a ordered set of \var{j} derivations,
where the \var{i}'th member has all the markers removed except for $\var{m}[i]$.
For example,
\begin{equation}\label{eq:location-marker-definition-1}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \,[\var{i}]\, \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \,[\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{equation}
is the equivalent of the logical conjunction of the two derivations:
\begin{equation}\label{eq:location-marker-definition-2}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \,[\var{i}]\, \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \,[\var{i}]\, \Vstr{predot} \Vstr{postdot} \cat \Vstr{after}
\end{split}
\end{equation}
and
\begin{equation}\label{eq:location-marker-definition-3}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \cat \Vstr{predot} \,[\var{j}]\, \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{equation}
In this example, 
\eqref{eq:location-marker-definition-2} and
\eqref{eq:location-marker-definition-3}
imply that
\begin{equation}\label{eq:location-marker-definition-4}
\Vstr{predot} \destar \var{w}[\var{i}, (\var{j} \subtract 1)]
\end{equation}
and therefore
\eqref{eq:location-marker-definition-1}
also implies
\eqref{eq:location-marker-definition-4}.
Derivations with location markers may be
composed in the same way as derivations without them,
as long as the location markers in the combined
derivation are consistent.

In this \doc{},
\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the parser described in
this \doc{}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Cg}$ will be the language accepted by $\alg{Recce}$
when parsing \Cg{}.

\section{Rewriting the grammar}
\label{s:rewrite}

We have already noted
that no rules of \Cg{}
have a zero-length RHS,
and that all symbols must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
\cite{AH2002} shows how to do this
without loss of generality.

Because Marpa claims to be a practical parser,
it is important to emphasize
that all grammar rewrites in this \doc{}
are done in such a way that the semantics
of the original grammar can be reconstructed
simply and efficiently at evaluation time.
As one example,
when a rewrite involves the introduction of new rule,
semantics for the new rule can be defined to pass its operands
up to a parent rule as a list.
Where needed, the original semantics
of a pre-existing parent rule can
be ``wrapped'' to reassemble these lists
into operands that are properly formed
for that original semantics.

As implemented,
the Marpa parser allows users to associate
semantics with an original grammar
that has none of the restrictions imposed
on grammars in this \doc{}.
The user of a Marpa parser
may specify any context-free grammar,
including one with properly nullable symbols,
empty rules, etc.
The user specifies his semantics in terms
of this original, ``free-form'', grammar.
Marpa implements the rewrites,
and performs evaluation,
in such a way as to keep them invisible to
the user.
From the user's point of view,
the ``free-form'' of his grammar is the
one being used for the parse,
and the one to which
his semantics are applied.

\section{Dotted rules}
\label{s:dotted}

Let $\Vrule{r} \in \Crules$
be a rule,
and $\Vsize{r}$ the length of its RHS.
A dotted rule (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \cat \Vsym{Y} \cat \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \var{X} \cat \var{Y} \mydot \var{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.

Let
\begin{align*}
%
\Postdot{\Vdr{x}} & \defined
\begin{cases}
\Vsym{B}, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \mydot \var{B} \cat \var{post}]$} \\
\Lambda, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \cat \var{B} \cat \var{post} \mydot]$}
\end{cases} \\
%
\Predot{\Vdr{x}} & \defined
\begin{cases}
\Vsym{B}, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \cat \var{B} \mydot \var{post}]$} \\
\Lambda, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \cat \var{B} \cat \var{post} \mydot]$}
\end{cases} \\
%
\Next{\Vdr{x}} & \defined
\begin{cases}
[\var{A} \de \var{pre} \cat \Vsym{B} \mydot \var{post}],  \\
\qquad \text{if $\Vdr{X} =
[\var{A} \de \var{pre} \mydot \var{B} \cat \var{post}]$} \\
\Lambda, \quad \text{if $\Postdot{\Vdr{x}} = \Lambda$}
\end{cases} \\
%
\Previous{\Vdr{x}} & \defined
\begin{cases}
[\var{A} \de \var{pre} \mydot \Vsym{B} \cat \var{post}],  \\
\qquad \text{if $\Vdr{X} =
[\var{A} \de \var{pre} \cat \var{B} \mydot \var{post}]$} \\
\Lambda, \quad \text{if $\Predot{\Vdr{x}} = \Lambda$}
\end{cases} \\
%
\end{align*}

The \dfn{start dotted rule} is
\begin{equation}
\label{eq:start-eim-def}
\Vdr{start} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation}
The \dfn{accept dotted rule} is
\begin{equation*}
\label{eq:accept-eim-def}
\Vdr{accept} = [\Vsym{accept} \de \Vsym{start} \mydot ].
\end{equation*}

We divide all Earley items into four disjoint types
based on their dotted rule
\begin{enumerate}
\item
\label{def:start-dr}
If an dotted rule does not have a predot symbol and it is the start dotted rule
its type is \dfn{start} dotted rule as defined above \eqref{eq:start-eim-def}.
\item
\label{def:prediction-dr}
If it does not have a predot symbol and it is not the start dotted rule,
it is a \dfn{prediction} dotted rule.
\item
\label{def:ethereal-dr}
If it does have a predot symbol and that symbol is a nulling terminal,
it is an \dfn{ethereal} dotted rule.
\item
\label{def:read-dr}
If it does have a predot symbol and that symbol is a non-nullable terminal,
it is an \dfn{read} dotted rule.
Note that in \Marpa{} all terminals are either nulling or non-nullable.
\item
\label{def:reduction-dr}
If it does have a predot symbol and that symbol is a non-terminal
it is an \dfn{reduction} dotted rule.
\end{enumerate}

A \dfn{predicted dotted rule}
always has a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{alpha} ].
\end{equation*}
A \dfn{confirmed dotted rule}
is a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule} is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{alpha} \mydot ].
\end{equation*}
Predicted, confirmed and completed dotted rules
are also called, respectively,
\dfn{predictions}, \dfn{confirmations} and \dfn{completions}.

A dotted rule which has only nulling symbols after the dot
is a \dfn{quasi-completion}.
A dotted rule which has only nulling symbols before the dot
is a \dfn{quasi-prediction}.
These definitions may be vacuously true,
so that all predictions are quasi-predictions,
and all completions are quasi-completions.
If in a pair of EIMs,
\begin{equation*}
\begin{split}
& \Vdr{quasi} = [\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} ] \\
& \Vdr{completion} = [\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot ]
\end{split}
\end{equation*}
where \Vstr{nulls} is a string composed entirely of nullable symbols
we say that \Vdr{completion} is the \dfn{completion dotted rule}
of the quasi-completion \Vdr{quasi}.
\begin{equation*}
\end{equation*}

\subsection{The dotted rule transition function}

We define
a partial transition function from
pairs of dotted rule and symbol
to sets of dotted rules.
\begin{equation*}
\GOTO: \Cdr, (\epsilon \cup \var{vocab}) \mapsto 2^\Cdr.
\end{equation*}
$\GOTO(\Vdr{from}, \epsilon)$ is a
\dfn{null transition}
and its result is a \dfn{null transition set}.

If the transition is not a null transition,
the resulting set of dotted rules will be a singleton --
only null transitions result in a set of dotted rules
with a cardinaliy greater than one.
The dotted rules in the set that results from a null transition
will be either predictions or confirmed rules with
a nulling predot symbol.

\section{Earley items}
\label{s:earley-items}

An Earley item (type \type{EIM}) is a triple
\[
    [\Vdr{dotted-rule}, \Vorig{x}, \Vloc{current} ]
\]
of dotted rule, origin, and current location.

The \dfn{origin} is the location where recognition of the rule
started.
It is sometimes called the ``parent''.)
The \dfn{current} or \dfn{dot location} is the location
in \Cw{} of the dot position in \Vdr{dotted-rule}
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.
Tradtionally, an Earley item is shown as a duple,
\[
    [\Vdr{dotted-rule}, \Vorig{x} ]
\]
with \Vloc{current} ommitted.
When the duple form is used,
the current location is specified by the context,
either explicitly or implicitly.
This paper will use Earley items
in both the duple and triple forms.

An EIM has the same type as its dotted rule.
Whenever a dotted rule notion is applied to an EIM,
it refers to the dotted rule of the EIM.
For example,
a completion EIM is an EIM with a completion
dotted rule,
and a predicted EIM is an EIM with a predicted dotted rule.
The completion EIM of a quasi-complete EIM
If is a quasi-complete EIM
$\Veim{quasi} = [ \Vdr{quasi}, \var{i}, \var{j} ]$,
then its completion EIM is
$[ \Vdr{complete}, \var{i}, \var{j} ]$,
where \Vdr{complete} is the completion dotted
rule of \Vdr{quasi}.

\begin{theorem}
\label{eim-types-correct}
Every EIM falls into one of these
five disjoint types:
start, prediction, read, ethereal and reduction.
\end{theorem}

\begin{proof}
The proof follows directly from definitions
\eqref{def:start-dr},
\eqref{def:prediction-dr},
\eqref{def:ethereal-dr},
\eqref{def:read-dr}
and \eqref{def:reduction-dr}
above.
\end{proof}

\subsection{Validity of Earley items}

We say that
an Earley item
\begin{equation*}
\bigl[[\Vsym{A} \de \Vstr{predot} \mydot \Vstr{postdot}], \var{i}, \var{j} \bigr]
\end{equation*}
is \dfn{valid}
if and only if
\begin{equation}
\label{eq:eim-valid-def}
\Vsym{A} \derives [\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot}.
\end{equation}

\subsection{Top-down and bottom-up causes}

We now proceed to define two important concepts:
the top-down cause of an Earley item
and the bottom-up cause of an Earley item.
For the purpose of this definition, we divide Earley items
into three kinds, and proceed by cases.
The three kinds of Earley item are predicted,
scanned,
and reduced.

Without loss of generality, let our reduced Earley item be
\begin{equation}\label{eq:reduced-effect}
[ [ \Vsym{A} \de \Vstr{prefix} \cat \Vsym{B} \mydot \Vstr{suffix} ], \var{i}, \var{k} ]
\end{equation}
Again without loss of generality, its derivation is
\begin{equation}\label{eq:reduced-derivation}
\begin{split}
&  \Vsym{A} \\
&  \qquad \derives \,[\var{i}]\, \Vstr{prefix} \,[\var{j}]\, \Vsym{B} \,[\var{k}]\, \Vstr{suffix} \\
&  \qquad \derives \,[\var{i}]\, \Vstr{prefix} \,[\var{j}]\, \Vstr{B-rhs} \,[\var{k}]\, \Vstr{suffix}. \\
\end{split}
\end{equation}
We say that
\begin{equation*}
[ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{B} \cat \Vstr{suffix} ], \var{i}, \var{j} ]
\end{equation*}
is a \dfn{top-down cause} of \eqref{eq:reduced-effect}.
We say that
\begin{equation*}
[ [ \Vsym{B} \derives \Vstr{B-rhs} \mydot ], \var{j}, \var{k} ]
\end{equation*}
is a \dfn{bottom-up cause} of \eqref{eq:reduced-effect}.
We say that \eqref{eq:reduced-effect} is the \dfn{effect}
of these two causes.

Without loss of generality, let our predicted Earley item be
\begin{equation}\label{eq:predicted-effect}
[ [ \Vsym{B} \de \mydot \Vstr{B-rhs} ], \var{i}, \var{i} ]
\end{equation}
We may use the same derivation as for the case of the reduced Earley item,
and it is again without loss of generality.
We say that
\begin{equation}\label{predicted-top-down-cause}
[ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{B} \cat \Vstr{suffix} ], \var{i}, \var{j} ]
\end{equation}
is a \dfn{top-down cause} of \eqref{eq:predicted-effect}.
From the bottom-up point of view, it is a null transition --
no property of the input is required that is not already required by
\eqref{predicted-top-down-cause}.
We say that its
\dfn{bottom-up cause} is 
null, or \dfn{ethereal}.
We say that \eqref{eq:predicted-effect} is the \dfn{effect}
of these two causes.

\subsection{The start Earley item}
\label{s:start}

\begin{theorem}\label{t:start-eim-is-valid}
The start Earley item is valid.
\end{theorem}

\begin{proof}
Let
\begin{equation}
\label{eq:start-eim-is-valid-5}
\Vsym{accept} \de \Vsym{start}
\end{equation}
be the accept rule,
and let
\begin{equation}
\label{eq:start-eim-is-valid-10}
[ [\Vsym{accept} \de \mydot \Vsym{start} ], 0, 0]
\end{equation}
be the start EIM.

By the definition of EIM validity,
to show that 
\eqref{eq:start-eim-is-valid-10}
is valid,
we need to show
that
\begin{multline}
\label{eq:start-eim-is-valid-15}
\Vsym{accept} \derives [0]\, \Vstr{predot} \,[0]\, \Vstr{postdot}\\ 
\text{where} \qquad \Vstr{predot} \cat \Vstr{postdot} = \Vsym{start}.
\end{multline}
Clearly, $\Vstr{predot} = \epsilon$
so that $\Vstr{postdot} = \Vstr{start}$
so that
\eqref{eq:start-eim-is-valid-15}
simplifies to
\begin{multline}
\label{eq:start-eim-is-valid-18}
\Vsym{accept} \derives \,[0]\, \Vsym{start} \\ 
\end{multline}
To show 
\eqref{eq:start-eim-is-valid-18},
we expand it using
the definition of location marker derivations
\eqref{eq:location-marker-def}.
We let $\Vsym{A} = \Vsym{accept}$
$\Vstr{pre} = \epsilon$,
and $\Vstr{post} = \Vsym{start}$.
With these settings,
\eqref{eq:location-marker-def}
requires us to show that,
for some \Vstr{before}, \Vstr{after},
\begin{gather}
\Vsym{accept} \destar \Vstr{before} \cat \Vsym{start} \cat \Vstr{after}
\label{eq:start-eim-is-valid-20} \\
\land \quad \Vstr{before} \destar \epsilon
\label{eq:start-eim-is-valid-21} \\
\land \quad \Vsym{start} \cat \Vstr{after} \destar \var{w}[0, (\Vsize{\Cw} \subtract 1)]
\label{eq:start-eim-is-valid-22}
\end{gather}
We can set
\begin{equation}
\label{eq:start-eim-is-valid-24}
\Vstr{before} = \Vstr{after} = \epsilon.
\end{equation}
\eqref{eq:start-eim-is-valid-24}
makes
\eqref{eq:start-eim-is-valid-21}
true, so that
\eqref{eq:start-eim-is-valid-20}
simplifies to
\begin{gather}
\Vsym{accept} \destar \Vsym{start}
\label{eq:start-eim-is-valid-30} \\
\land \qquad \Vsym{start} \destar \var{w}[0, (\Vsize{\Cw} \subtract 1)]
\label{eq:start-eim-is-valid-31}
\end{gather}
We know \eqref{eq:start-eim-is-valid-30}
from \eqref{eq:start-eim-is-valid-5}.
By definition, the accept rule derives the input in a successful
parse,
$$ \Vsym{accept} \derives \Vstr{start} \destar \Cw = \var{w}[0, (\Vsize{\Cw} \subtract 1)]$$
and from this we have
\eqref{eq:start-eim-is-valid-31}.
\end{proof}

\subsection{Reductions}
\label{s:reduction}

\begin{theorem}\label{t:reduction-valid}
Let
\begin{equation}\label{eq:reduction-valid-2}
\Veim{top-down} = [ \Vdr{down}, \Vloc{i}, \Vloc{j} ]
\end{equation}
and
\begin{equation}\label{eq:reduction-valid-2a}
\Veim{bottom-up} = [ \Vdr{up}, \Vloc{j}, \Vloc{k} ]
\end{equation}
be valid Earley items,
where
\Vdr{up} is a completion and
\begin{equation}\label{eq:reduction-valid-3}
\Postdot{\Vdr{down}} = \LHS{\Vdr{up}}.
\end{equation}
Then
\begin{equation}\label{eq:reduction-valid-4}
\Veim{reduction} = [ \Next{\Vdr{down}}, \Vloc{i}, \Vloc{k} ]
\end{equation}
is a valid Earley item.
\end{theorem}

\begin{proof}
Without loss of generality,
let
\begin{equation}\label{eq:reduction-valid-4a}
\Vdr{down} = [ \Vsym{down-lhs} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ]
\end{equation}
and
\begin{equation}\label{eq:reduction-valid-5}
\Vdr{up} = [ \Vsym{A} \de \Vstr{A-rhs} \mydot ].
\end{equation}
To show the theorem, we must show that
\begin{equation}\label{eq:reduction-valid-8}
\begin{split}
& \Veim{reduction} =  \\
& \qquad \qquad [ [ \Vsym{down-lhs} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ]
\end{split}
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}\label{eq:reduction-valid-10}
\Vsym{down-lhs} \derives [\var{i}] \Vstr{pre} \cat \Vstr{A} [\var{k}] \Vstr{post}.
\end{equation}

By assumption, \Veim{top-down} is valid.
From
\eqref{eq:reduction-valid-2}
and \eqref{eq:reduction-valid-4a}
and the definition of validity for an Earley item:
\begin{equation}\label{eq:reduction-valid-12}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \cat \Vstr{post}
\end{equation}

From \eqref{eq:reduction-valid-2a} and
\eqref{eq:reduction-valid-5} we know that
\begin{equation}\label{eq:reduction-valid-18}
\Vsym{A} \derives \,[\var{j}]\, \Vstr{A-rhs} \,[\var{k}]\,
\end{equation}
and therefore that
\begin{equation}\label{eq:reduction-valid-21}
\,[\var{j}]\, \Vsym{A} \,[\var{k}]\, \derives \,[\var{j}]\, \Vstr{A-rhs} \,[\var{k}]\,
\end{equation}
or, simplifying,
\begin{equation}\label{eq:reduction-valid-24}
\,[\var{j}]\, \Vsym{A} \,[\var{k}]\,
\end{equation}
We know the location markers in
\eqref{eq:reduction-valid-12}
and
\eqref{eq:reduction-valid-24}
are compatible:
The \var{i} and \var{k} are unrestricted,
while the use of \var{j} in both derivations is compatible.
Composing them we have
\begin{equation}\label{eq:reduction-valid-25}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \,[\var{k}]\, \Vstr{post}
\end{equation}
If we drop the location marker for \var{j}, this
is \eqref{eq:reduction-valid-8},
which is what we needed to show for the theorem.
\end{proof}

\subsection{Reads}
\label{s:read}

Without loss of generality, let our
read Earley item be
\begin{equation}\label{eq:scanned-effect}
[ [ \Vsym{A} \de \Vstr{prefix} \cat \Vsym{B} \mydot \Vstr{suffix} ], \var{i}, \var{k} ].
\end{equation}
where \Vsym{B} must be a non-nullable terminal.
Again without loss of generality, its derivation is
\begin{equation}\label{eq:scanned-derivation}
\begin{split}
&  \Vsym{A} \\
&  \qquad \derives \,[\var{i}]\, \Vstr{prefix} \,[\var{j}]\, \Vsym{B} \,[\var{k}]\, \Vstr{suffix} \\
\end{split}
\end{equation}
We say that
\begin{equation}\label{scanned-top-down-cause}.
[ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{B} \cat \Vstr{suffix} ], \var{i}, \var{j} ]
\end{equation}
is a \dfn{top-down cause} of \eqref{eq:scanned-effect}.
We say that \Vsym{B} is the \dfn{bottom-up cause} of \eqref{eq:scanned-effect}.
\Vsym{B} may be a nulling symbol,
in which no property of the input is required that is not already required by
\eqref{scanned-top-down-cause}.
When \Vsym{B} is zero-length, or null,
we say that
the \dfn{bottom-up cause} of
\eqref{eq:scanned-effect}
is null, or \dfn{ethereal}.
When \Vsym{B} is of length greater than zero
we say that
the \dfn{bottom-up cause}
\eqref{eq:scanned-effect}
is \Vsym{B}.
We say that \eqref{eq:scanned-effect} is the \dfn{effect}
of these two causes.

\begin{theorem}\label{t:scansion-valid}
Let
\begin{equation}\label{eq:scansion-valid-2}
\Veim{top-down} = [ \Vdr{down}, \Vloc{i}, \Vloc{j} ]
\end{equation}
be a valid Earley item
where \Vsym{bottom} is a non-nullable symbol
such that
\begin{equation}\label{eq:scansion-valid-3}
\Postdot{\Vdr{down}} = \Vsym{bottom} = \CVw{j}
\end{equation}
Then
\begin{equation}\label{eq:scansion-valid-4}
\Veim{scansion} = [ \Next{\Vdr{down}}, \Vloc{i}, \Vloc{j}+1 ]
\end{equation}
is a valid Earley item.
\end{theorem}

\begin{proof}
Without loss of generality,
let
\begin{equation}\label{eq:scansion-valid-6}
\Vdr{down} = [ \Vsym{down-lhs} \de \Vstr{pre} \mydot \Vsym{bottom} \cat \Vstr{post} ].
\end{equation}
To show the theorem, we must show that
\begin{equation}\label{eq:scansion-valid-8}
\Veim{scansion} = 
\left[
\begin{aligned}
& \left[
\begin{aligned}
& \Vsym{down-lhs} \de \\
& \qquad \Vstr{pre} \cat \Vsym{bottom} \mydot \Vstr{post}
\end{aligned}
\right], \\
& \var{i}, \\
& \var{j}+1
\end{aligned}
\right]
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}\label{eq:scansion-valid-11}
\Vsym{down-lhs} \derives [\var{i}] \Vstr{pre} \cat \Vsym{bottom} [\var{j}+1] \Vstr{post}.
\end{equation}

By assumption, \Veim{top-down} is valid.
From
\eqref{eq:scansion-valid-2}
and \eqref{eq:scansion-valid-6}
and the definition of validity for an Earley item:
\begin{equation}\label{eq:scansion-valid-12}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{bottom} \cat \Vstr{post}.
\end{equation}

All non-nullable symbols have a length in the input of 1.
From this and \eqref{eq:scansion-valid-3}
we know that
\begin{equation}\label{eq:scansion-valid-18}
\,[\var{j}]\, \Vsym{bottom} \,[\var{j}+1]\,
\end{equation}
We know the location markers in
\eqref{eq:scansion-valid-12}
and
\eqref{eq:scansion-valid-18}
are compatible:
The \var{i} is unrestricted,
while the use of \var{j} in both derivations is compatible.
Composing them we have
\begin{equation}\label{eq:scansion-valid-21}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{bottom} \,[\var{j}+1]\, \Vstr{post}
\end{equation}
If we drop the location marker for $[\var{j}]$, this
is \eqref{eq:scansion-valid-11},
which is what we needed to show for the theorem.
\end{proof}

\subsection{Ethereal scansion}
\label{s:ethereral-scansion}

\begin{theorem}\label{t:null-scansion-valid}
Let
\begin{equation}\label{eq:null-scansion-valid-2}
\Veim{top-down} = [ \Vdr{down}, \Vloc{i}, \Vloc{j} ]
\end{equation}
be a valid Earley item
where \Vsym{bottom} is a nulling symbol
such that
\begin{equation}\label{eq:null-scansion-valid-3}
\Postdot{\Vdr{down}} = \Vsym{bottom}
\end{equation}
Then
\begin{equation}\label{eq:null-scansion-valid-4}
\Veim{scansion} = [ \Next{\Vdr{down}}, \Vloc{i}, \Vloc{j} ]
\end{equation}
is a valid Earley item.
\end{theorem}

\begin{proof}
Without loss of generality,
let
\begin{equation}\label{eq:null-scansion-valid-6}
\Vdr{down} = [ \Vsym{down-lhs} \de \Vstr{pre} \mydot \Vsym{bottom} \cat \Vstr{post} ].
\end{equation}
To show the theorem, we must show that
\begin{equation}\label{eq:null-scansion-valid-8}
\Veim{scansion} = 
\left[
\begin{aligned}
& \left[
\begin{aligned}
& \Vsym{down-lhs} \de \\
& \qquad \Vstr{pre} \cat \Vsym{bottom} \mydot \Vstr{post}
\end{aligned}
\right], \\
& \var{i}, \\
& \var{j}
\end{aligned}
\right]
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}\label{eq:null-scansion-valid-11}
\Vsym{down-lhs} \derives [\var{i}] \Vstr{pre} \cat \Vsym{bottom} [\var{j}] \Vstr{post}.
\end{equation}

By assumption, \Veim{top-down} is valid.
From
\eqref{eq:null-scansion-valid-2}
and \eqref{eq:null-scansion-valid-6}
and the definition of validity for an Earley item:
\begin{equation}\label{eq:null-scansion-valid-12}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{bottom} \cat \Vstr{post}
\end{equation}

By definition,
nullable symbols are zero-length.
From this, \eqref{eq:null-scansion-valid-2},
and \eqref{eq:null-scansion-valid-3}
we know that
\begin{equation}\label{eq:null-scansion-valid-18}
\,[\var{j}]\, \Vsym{bottom} \,[\var{j}]\,
\end{equation}
We know the location markers in
\eqref{eq:null-scansion-valid-12}
and
\eqref{eq:null-scansion-valid-18}
are compatible:
The \var{i} is unrestricted,
while the use of \var{j} in both derivations is compatible.
Composing them we have
\begin{equation}\label{eq:null-scansion-valid-21}
\Vsym{down-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{bottom} \,[\var{j}]\, \Vstr{post}
\end{equation}
If we drop the first location marker for $[\var{j}]$, this
is \eqref{eq:null-scansion-valid-11},
which is what we needed to show for the theorem.
\end{proof}

\subsection{Ethereal transitions}
\label{s:ethereral}

Computing the result of $\GOTO$ when it is not
a null transition is trivial.
But the complexity of null transitions is of interest:
we may want to compute them on the fly,
and in any case,
we want to know if the computation has finite
time complexity.

\begin{algorithm}[!htp]
\caption{Add one null transition generation}
\label{alg:null-transition-generation}
\begin{algorithmic}[1]
\Procedure{Add generation}{\Vdr{dr}, \Vdrset{results}}
\If{\Vdr{dr} has no postdot symbol}
\State return
\EndIf
\State Here \Vdr{dr} is $[ \Vsym{lhs} \de \Vstr{before} \mydot \Vsym{A} \cat \Vstr{after} ]$
\State \Comment We can state this without loss of generality
\If{$\Vsym{A}$ is a nulling symbol}
\State $\Vdr{new} \gets [ \Vsym{lhs} \de \Vstr{before} \cat \Vsym{A} \mydot \Vstr{after} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \ldots
\State $\qquad$ but only if it has never been added before
\State return
\EndIf
\State Here \Vsym{A} must be a non-nullable symbol
\For{ each \Vrule{r} in \Cg{}}
\If{ $\LHS{\Vrule{r}} = \Vsym{A}$ }
\State Here \Vrule{r} is $[ \Vsym{A} \de \Vstr{rhs} ]$
\State \Comment We can state this without loss of generality
\State $\Vdr{new} \gets [ \Vsym{A} \de \mydot \Vstr{rhs} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \dots
\State $\qquad$ but only if it has never been added before
\EndIf
\EndFor
\State return
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!htp]
\caption{Create null transition set}
\label{alg:null-transition}
\begin{algorithmic}[1]
\Function{Create null transition set}{\Vdr{base}}
\State Initialize \Vdrset{result} to the empty set
\State Initialize \Vdrset{work} to the emtpy set
\State \Call{Add generation}{\Vdr{base}}
\While{\Vdrset{work} is not empty}
\State Remove a dotted rule from \Vdrset{work}, call it \Vdr{work}
\State \Call{Add generation}{\Vdr{work}, \Vdrset{result}}
\EndWhile
\State return \Vdrset{result}
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:null-transition} does not correspond closely to
any implementation.
The current Marpa implementation, for example, actually handles
nulling symbols implicitly,
while it computes predicitons at the end.
But for theory purposes, it is convenient to consider all
nulling transitions together.

\begin{theorem}\label{t:null-transition-Oc}
A null transition set can be computed in time \Oc{}.
\end{theorem}

\begin{proof}
We consider Algorithm \ref{alg:null-transition}.
This clearly runs in \Oc{} time if there is a constant
number of calls to
Algorithm \ref{alg:null-transition-generation}.

To finish the proof, we need to show
that
Algorithm \ref{alg:null-transition-generation}.
is called a constant number
of times.
Algorithm \ref{alg:null-transition-generation}
is called
once for the base dotted rule of the computation.
It is called again for every dotted rule added to the working set
of dotted rules, \Vdrset{work}.
We know that no dotted rule is added to
\Vdrset{work} twice.
Therefore 
Algorithm \ref{alg:null-transition-generation}
is called
at most once for each dotted rule.
\Cg{} has a fixed number of dotted rules,
so
that Algorithm \ref{alg:null-transition-generation}
is called
a most \Oc{} times.
\end{proof}

\begin{theorem}\label{t:null-transition-dr-correct}
Algorithm \ref{alg:null-transition} is correct.
\end{theorem}

\begin{proof}
It is directly clear from examining
Algorithm \ref{alg:null-transition}.
that the null transitions 
for nulling postdot symbols are complete and consistent,
and therefore correct.
It remains to consider the null transitions for non-nullable
postdot symbols -- predictions.

Algorithm \ref{alg:null-transition}
clearly adds all predictions derivable in a single step
to its results.
It also
calls the ``Add generation'' function
repeatedly, so that indirect predictions will be added.
But it will refuse to
add a dotted rule to its working set more than once.
It remains to consider whether this means some predictions
will not be derived.

Consider a prediction
\begin{equation}\label{null-transition-correct-5}
\Vsym{postdot-penult} \de \mydot \Vsym{postdot-last} \cat \Vstr{after-last}
\end{equation}
which is derived through a chain
of derivations in which the same dotted rule occurs at least twice.
\begin{equation*}
\begin{split}
& [ \Vsym{lhs-a} \mydot \Vsym{postdot-0} \cat \Vstr{after} ] \qquad \text{Step 0} \\
& \derives [ \mydot \Vsym{postdot-1} \cat \Vstr{after-1} ] \qquad \text{Step 0} \\
& \ldots \\
& \derives [ \mydot \Vsym{postdot--before-i} \cat \Vstr{after-i} ] \qquad \text{Step $i-1$} \\
& \derives [ \mydot \Vsym{postdot-i} \cat \Vstr{after-i} ] \qquad \text{Step $i$} \\
& \ldots \\
& \derives [ \mydot \Vsym{postdot-i} \cat \Vstr{after-i} ] \qquad \text{Step $j$}\\
& \ldots \\
& \derives [ \mydot \Vsym{postdot-penult} \cat \Vstr{after-penult} ] \\
& \derives [ \mydot \Vsym{postdot-last} \cat \Vstr{after-last} ]
\end{split}
\end{equation*}
We can create a shorter derivation by removing the derivation from Step $i$ to Step $j$
and replacing it with Step $i$.
Algorithm \ref{alg:null-transition} will also follow this shorter derivation,
and therefore will add \eqref{null-transition-correct-5}
without needing to add
$$ \Vsym{postdot-before-i} \de \mydot \Vsym{postdot-i} \cat \Vstr{after-i} $$
to \Vdrset{work}
more than once.

We can do this for to every postdot symbol which occurs twice.
From this we see that 
Algorithm \ref{alg:null-transition}
can derive a complete set of predictions
without having to add any dotted rule
to \Vdrset{work}
more than once.
\end{proof}

\subsection{Predictions}
\label{s:prediction}

\begin{theorem}\label{t:prediction-from-dr-valid}
Let
\begin{equation}\label{eq:prediction-from-dr-valid-2}
\Veim{base} = [ \Vdr{base}, \Vloc{i}, \Vloc{j} ]
\end{equation}
be a valid Earley item.
If \Vdr{prediction}  is a prediction generated by
Algorithm \ref{alg:null-transition} from \Vdr{base},
then
\begin{equation}\label{eq:prediction-from-dr-valid-3}
[ \Vdr{prediction}, \Vloc{j}, \Vloc{j} ]
\end{equation}
is a valid Earley item.
\end{theorem}

\begin{proof}
Without loss of generality,
let
\begin{equation}\label{eq:prediction-from-dr-valid-4}
\Vdr{base} = [ \Vsym{base-lhs} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ]
\end{equation}
and
\begin{equation}\label{eq:prediction-from-dr-valid-5}
\Vdr{prediction} = [ \Vsym{A} \de \mydot \Vstr{A-rhs} ].
\end{equation}
To show the theorem, we must show that
\begin{equation}\label{eq:prediction-from-dr-valid-8}
\Veim{prediction} = [ [ \Vsym{A} \de \mydot \Vstr{A-rhs} ], \var{j}, \var{j} ]
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}\label{eq:prediction-from-dr-valid-10}
\Vsym{A} \derives [\var{j}] \Vstr{A-rhs}.
\end{equation}

By assumption, \Veim{base} is valid.
From
\eqref{eq:prediction-from-dr-valid-2}
and \eqref{eq:prediction-from-dr-valid-4}
and the definition of validity for an Earley item:
\begin{equation}\label{eq:prediction-from-dr-valid-12}
\Vsym{base-lhs} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \cat \Vstr{post}
\end{equation}
We can simplify this to the one-step derivation
\begin{equation}\label{eq:prediction-from-dr-valid-14}
\,[\var{j}]\, \Vsym{A}
\end{equation}
From \eqref{eq:prediction-from-dr-valid-5} we know that
$\Vsym{A} \de \Vstr{A-rhs}$
is a rule of the grammar,
and by Theorem \ref{t:null-transition-dr-correct},
we know that the predicted dotted rule is correct,
so that we can extend the derivation of
\eqref{eq:prediction-from-dr-valid-14}
to
\begin{equation}\label{eq:prediction-from-dr-valid-16}
[\var{j}]\, \Vsym{A} \derives [\var{j}]\, \Vsym{A-rhs}
\end{equation}
This is equivalent to
\eqref{eq:prediction-from-dr-valid-8},
which is what we needed to show for the theorem.
\end{proof}

\subsection{Earley sets grow at worst linearly}

\begin{theorem}\label{t:es-count}
For a context-free grammar,
and a parse location \Vloc{i},
\begin{equation*}
\textup{
    $\bigsize{\EVtable{\Marpa}{i}} = \order{\var{i}}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
EIM's have the form $[\Vdr{x}, \Vorig{x}]$.
\Vorig{x} is the origin of the EIM,
which in Marpa cannot be after the current
Earley set  at \Vloc{i},
so that
\begin{equation*}
0 \le \Vorig{x} \le \Vloc{i}.
\end{equation*}
The possibilities for \Vdr{x},
call it $\size{\Cdr}$,
is a finite constant that depend on the grammar,
\Cg{}.
Since duplicate EIM's are never added to an Earley set,
the maximum size of Earley set \Vloc{i} is therefore
\begin{equation*}
\Vloc{i} \times \size{\Cdr} = \order{\Vloc{i}}.\qedhere
\end{equation*}
\end{proof}

\subsection{Causes}

Bottom-up causes are either \dfn{mundane} or \dfn{ethereal}.
The bottom-up cause of a reduced Earley item is always mundane.
The bottom-up cause of a predicted Earley item is always ethereal.
The bottom-up cause of a scanned Earley item is mundane if the scanned symbol
is of zero-length,
and mundane, otherwise.
In determining whether a cause is valid,
an ethereal cause is always considered valid.

\begin{theorem}\label{t:multi-cause-eq-ambiguous-g}
If any Earley item has more than one top-down cause,
or more than one bottom-up cause,
the grammar is ambiguous.
\end{theorem}

\begin{proof}
This may be proved using the derivations in the defintion
of top-down and bottom-up cause.
More than one cause means more than one derivation,
and a grammar with more than one derivation for \Cw{}.
The definition of ambiguity for a grammar is that it
has more than one derivation for a single input.
\end{proof}

\begin{theorem}\label{t:cause-effect-consistent}
If the top-down cause and bottom-up cause of an
effect are both valid,
the effect is a valid Earley item.
\end{theorem}

\begin{proof}
We first examine the case of reduced Earley items.
Top-down and bottom-up cause for a reduced Earley item
has a dual definition.
\eqref{eq:reduced-derivation} in
This definition is a fully general derivation
of both causes.
\eqref{eq:reduced-effect} in the dual definition
is a statement of the cause's effect.
And \eqref{eq:reduced-derivation} shows the validity
of the effect,
according to the definition of validity for Earley items.

The previous reasoning shows the theorem for reduced
Earley items.
Exactly the same argument may be made for predicted
and scanned Earley items.
This shows all cases and therefore the theorem.
\end{proof}

\begin{theorem}\label{t:cause-effect-complete}
If an Earley item is valid,
it has a valid top-down cause
and a valid bottom-up cause.
\end{theorem}

\begin{proof}
Again, the proof is by the
definition of top-down and bottom-up causes.
Looking at the case in that definition for
reduced Earley items,
we see that the definition contained
\eqref{eq:reduced-derivation}:
a fully general demonstration of the validity
of the
\eqref{eq:reduced-effect}, an Earley item
which was reduced, but otherwise arbitrary.
Given the derivation \eqref{eq:reduced-derivation},
the definition showed there must be valid
top-down and bottom-up causes for
\eqref{eq:reduced-effect}.

The same was true for the other two cases,
that of predicted and scanned Earley items.
We therefore have all cases, and the theorem.
\end{proof}

\subsection{Earley algorithms}

An Earley parser builds a table of Earley sets,
\begin{equation*}
\EVtable{\Earley}{i},
\quad \text{where} \quad
0 \le \Vloc{i} \le \size{\Cw}.
\end{equation*}
Earley sets are of type \type{ES}.
Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
The type designator \type{ES} is often omitted to avoid clutter,
especially in cases where the Earley set is not
named by location.

\EVtable{\alg{Recce}}{i} will be
the Earley set at \Vloc{i}
in the table of Earley sets of
the \alg{Recce} recognizer.
For example,
\EVtable{\Marpa}{j} will be Earley set \Vloc{j}
in \Marpa's table of Earley sets.
In contexts where it is clear which recognizer is
intended,
\Vtable{k}, or \Ves{k}, will symbolize Earley set \Vloc{k}
in that recognizer's table of Earley sets.
If \Ees{\var{working}} is an Earley set,
$\size{\Ees{\var{working}}}$ is the number of Earley items
in \Ees{\var{working}}.

\Rtablesize{\alg{Recce}} is the total number
of Earley items in all Earley sets for \alg{Recce},
\begin{equation*}
\Rtablesize{\alg{Recce}} =
     \sum\limits_{\Vloc{i}=0}^{\size{\Cw}}
	{\bigsize{\EVtable{\alg{Recce}}{i}}}.
\end{equation*}
For example,
\Rtablesize{\Marpa} is the total number
of Earley items in all the Earley sets of
a \Marpa{} parse.

An Earley set is \dfn{consistent} if and only if all of its
Earley items are valid.
An Earley set is \dfn{complete} if and only if it contains
all valid Earley items.
An Earley set is \dfn{correct} if and only if it is
consistent and complete.

We say that an Earley implementation is \dfn{correct} if and only
if it the set of strings it accepts is exactly the set of
string in the language of its grammar.

\begin{theorem}\label{t:algorithm-correct}
If an Earley implementation creates a correct Earley set
at location \Vsize{\Cw}, then
\begin{equation}
\label{eq:algorithm-correct-2}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}} \equiv \Cw \in \var{L}(\Cg).
\end{equation}
\end{theorem}

\begin{proof}
We prove the forward direction of the implication first.
We assume that
\begin{equation}
\label{eq:algorithm-correct-3}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
Since the algorithm is correct by assumption for the theorem,
we have, by \eqref{eq:algorithm-correct-3}
and by the definition of valid for an Earley item,
\begin{equation}
\label{eq:algorithm-correct-6}
\begin{split}
& \Vsym{accept} \destar \Vstr{before} \,[0]\, \Vstr{predot} \,[\Vsize{\Cw}]\, \Vstr{postdot} \cat \Vstr{after}
\end{split}
\end{equation}
where the accept dotted rule is
\begin{equation}
\label{eq:algorithm-correct-9}
\Vsym{accept} \de \cat \Vstr{predot} \mydot \Vstr{postdot}.
\end{equation}
Since no terminals can come before location 0 or after
location \Vsize{\Cw},
we see from
\eqref{eq:algorithm-correct-6}
that
\begin{equation}
\Vstr{before} = \Vstr{postdot} = \Vstr{after} = \epsilon.
\end{equation}
so that
\eqref{eq:algorithm-correct-6}
simplifies to
\begin{equation}
\label{eq:algorithm-correct-12}
\Vsym{accept} \destar [0]\, \Vstr{predot} \,[\Vsize{\Cw}]
\end{equation}
which, by the definition of the location markers
means that
\begin{equation}
\label{eq:algorithm-correct-15}
\Vstr{accept} \destar \var{w}[\var{0}, \Vsize{\Cw} \subtract 1] \destar \Cw.
\end{equation}
From
\eqref{eq:algorithm-correct-15},
by the definition of $\var{L}(\Cg)$,
we have that
\begin{equation}
\label{eq:algorithm-correct-27}
\Cw \in \var{L}(\Cg).
\end{equation}
This shows the forward direction of the implication.

To show the reverse direction of the implication, we assume
\eqref{eq:algorithm-correct-27}.
From it,
the definition of the \Vsym{accept} symbol,
and the definition of $\var{L}(\Cg)$,
we have 
\eqref{eq:algorithm-correct-15}.
By the definition of the \Vsym{accept} symbol,
it is only on the LHS of the rule
\eqref{eq:algorithm-correct-12},
so that we have
\begin{equation}
\label{eq:algorithm-correct-40}
\Vsym{accept} \derives [0]\, \Vstr{predot} \,[\Vsize{\Cw}] \destar \Cw.
\end{equation}
By assumption for the theorem, the Earley set at
\Vsize{\Cw} is correct and therefore complete.
If the Earley set at \Vsize{\Cw} is complete,
by \eqref{eq:algorithm-correct-40},
we have
\begin{equation}
\label{eq:algorithm-correct-46}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
This shows the reverse direction of the implication.
We have now shown both directions of the implication,
and therefore the theorem.
\end{proof}

\section{The Leo algorithm}
\label{s:leo}

In \cite{Leo1991}, Joop Leo presented a method for
dealing with right recursion in \On{} time.
Leo shows that,
with his modification, Earley's algorithm
is \On{} for all LR-regular grammars.
(LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.)

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this \doc{} Leo's ``transitive items''
will be called Leo items.
Leo items of the form used in Leo's paper\cite{Leo1991},
will be type \type{LIM}.

In each Earley set, there is at most one Leo item per symbol.
A Leo item (LIM) is the triple
\begin{equation*}
[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]
\end{equation*}
where \Vsym{transition} is the transition symbol,
and
\begin{equation*}
\Veim{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley item to be added on reductions over
\Vsym{transition}.

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
so that the top of the sequence can represent
the entire sequence --
the only role the other EIM's in the sequence
play in the parse is to derive the top EIM.
We will call these memoized sequences, Leo sequences.

To guarantee that a Leo sequence is deterministic,
\Leo{} enforced \dfn{Leo uniqueness}.
We say that \Vdr{q} is a \dfn{quasi-penult}
if and only if it is
\begin{equation*}
\begin{split}
& \Vdr{q} = [ \Vsym{A} \de \Vstr{before} \mydot \Vsym{B} \cat \Vstr{after} ] \\
& \qquad \text{for some $[ \Vsym{A} \de \Vstr{before} \cat \Vsym{B} \cat \Vstr{after} ] \in \Crules$} \\
& \qquad \qquad \text{such that $\Vstr{after} \derives \epsilon \land \Vstr{B} \nderives \epsilon$}.
\end{split}
\end{equation*}
We say that \Veim{x} is \dfn{Leo unique}
if and only if it is
\begin{equation*}
\begin{split}
& \Veim{x} = [ \Vdr{x},  \Vorig{x}, \Vloc{x} ] \\
& \qquad \text{for some \Vdr{x}, \Vorig{x}, \Vloc{x}} \\
& \qquad \text{such that for all $\Veim{y} = [ \Vdr{y},  \Vorig{y}, \Vloc{x} ]$} \\
& \qquad \qquad \Postdot{\Vdr{x}} = \Postdot{\Vdr{y}} \implies \Veim{x} = \Veim{y} .
\end{split}
\end{equation*}

If \Veim{x} is Leo unique, then the symbol \Vdr{x}
and $\Postdot{\Vdr{x}}$ are
also said to be \dfn{Leo unique} in Earley set \Vloc{x}.
In the definition
it is important to emphasize that \Veim{y} ranges over all the dotted
rules of Earley set \Ves{x},
even those which are ineligible for Leo memoization.

Let \var{n} be the length of a Leo sequence.
In \Earley, each such sequence would be expanded in every
Earley set that is the origin of an EIM included in the
sequence, and the total number of EIM's would be
\order{\var{n}^2}.

With Leo memoization, a single EIM stands in for the sequence.
There are \Oc{} Leo items per Earley set,
so the cost of the sequence is \Oc{} per Earley set,
or \On{} for the entire sequence.
If, at evaluation time,
it is desirable to expand the Leo sequence,
only those items actually involved in the parse
need to be expanded.
All the EIM's of a potential right-recursion
will be in one Earley set and the number of EIM's
will be \On{},
so that even including expansion of the Leo sequence
for evaluation, the time and space complexity of
the sequence remains \On{}.

In Leo's original algorithm, any penult
was treated as a potential right-recursion.
\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\begin{equation*}
\Vdr{candidate} = [\Vrule{candidate}, \var{i}]
\end{equation*}
for Leo memoization,
\Vdr{candidate} must be a penult and
\Vrule{candidate} must be right-recursive.

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be infinitely
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff is therefore very limited.
One future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
justify inclusion in the Leo memoizations.
Such cases are unusual, but may occur.

Omission of a memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
Later in this \doc{} we will
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

\subsection{Complexity}

Alongside the pseudocode of this section
are observations about its space and time complexity.
In what follows,
we will charge all time and space resources
to Earley items,
or to attempts to add Earley items.
We will show that,
to each Earley item actually added,
or to each attempt to add a duplicate Earley item,
we can charge amortized \Oc{} time and space.

At points, it will not be immediately
convenient to speak of
charging a resource
to an Earley item
or to an attempt to add a duplicate
Earley item.
In those circumstances,
we speak of charging time and space
\begin{itemize}
\item to the parse; or
\item to the Earley set; or
\item to the current procedure's caller.
\end{itemize}

We can charge time and space to the parse itself,
as long as the total time and space charged is \Oc.
Afterwards, this resource can be re-charged to
the initial Earley item, which is present in all parses.
Soft and hard failures of the recognizer use
worst-case \Oc{} resource,
and are charged to the parse.

We can charge resources to the Earley set,
as long as the time or space is \Oc.
Afterwards,
the resource charged to the Earley set can be
re-charged to an arbitrary member of the Earley set,
for example, the first.
If an Earley set is empty,
the parse must fail,
and the time can be charged to the parse.

In a procedure,
resource can be ``caller-included''.
Caller-included resource is not accounted for in
the current procedure,
but passed upward to the procedure's caller,
to be accounted for there.
A procedure to which caller-included resource is passed will
sometimes pass the resource upward to its own caller,
although of course the top-level procedure does not do this.

For each procedure, we will state whether
the time and space we are charging is inclusive or exclusive.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource charges passed up from called procedures.
Inclusive time or space includes
resource passed upward to the
current procedure from called procedures.

Earley sets may be represented by \Ves{i},
where \var{i} is the Earley set's locaiton \Vloc{i}.
The two notations should be regarded as interchangeable.
The actual implementation of either
should be the equivalent of a pointer to
a data structure containing,
at a minium,
the Earley items,
a memoization of the Earley set's location as an integer,
and a per-set-list.
Per-set-lists will be described in Section \ref{s:per-set-lists}.

\begin{algorithm}[h]
\caption{Marpa Top-level}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initial}{}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\Ves{x}$ is complete, for $0 \le \var{x} < \var{i}$
\State \Call{Scan pass}{$\var{i}, \var{w}[\var{i} \subtract 1]$}
\If{$\size{\Ves{i}} = 0$}
\State reject \Cw{} and return
\EndIf
\State \Call{Reduction pass}{\var{i}}
\EndFor
\If{$[\Vdr{accept}, 0] \in \Etable{\Vsize{w}}$}
\State accept \Cw{} and return
\EndIf
\State reject \Cw{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets.
Inclusive time and space for the final loop to
check for \Vdr{accept} is charged to
the Earley items at location \size{\Cw}.
Overhead is charged to the parse.
All these resource charges are obviously \Oc.

\subsection{Ruby Slippers parsing}
This top-level code represents a significant change
from previous versions of Earley's algorithm.
\call{Scan pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when the scanning of tokens that start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.


\begin{algorithm}[h]
\caption{Initialization}
\label{a:initial}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add EIM pair}{$0_{ES}, \dr{start}, 0$}
\label{line:initial-10}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}
\label{p:initial-op}

\subsubsection{Initialization complexity}
\label{p:initial-op-complexity}
Inclusive time and space is \Oc{}
and is charged to the parse.

\subsubsection{Initialization correctness}
\label{p:initial-op-correct}

\begin{theorem}\label{t:initial-op-correct}
Initialization is correct.
\end{theorem}

\begin{proof}
We first note that Leo memoization will have
no effect on the set of
EIMs in Earley set 0.
This is because no EIM is Leo memoized,
except because of a LIM in a prior Earley
set,
and no Earley set is prior to Earley set 0.

Since the bottom-up causes of both
mundane scansions and reductions
has an input length of greater than 0,
these EIMs cannot appear in Earley set 0.
All EIMs in Earley set 0 are therefore
\begin{itemize}
\item the start Earley item,
\item predictions, or
\item ethereal scansions.
\end{itemize}

The set of start Earley items that we will
add to Earley set 0 is the singleton set
\begin{multline}\label{eq:initial-op-correct}
\left\lbrace \bigl[ [ \Vsym{accept} \de \mydot \Vsym{start} ], 0, 0 \bigr] \right\rbrace \\
\text{where $[ \Vsym{accept} \de \mydot \Vstr{start} ]$ is the start rule.}
\end{multline}
We first show that this set is correct.
It is consistent by theorem \ref{t:start-eim-is-valid}.
It is complete because the start rule is by definition unique.
Therefore the set of start EIMs
added by
Algorithm \ref{a:initial} to Earley set 0
is correct.

The predictions and ethereal scansions
must have a top-down cause in the same
Earley set.
Together, the predictions and ethereal scansions
are exactly the EIMs with null transitions.
So null transition EIMs
must have some other EIM in Earley set 0
as either a direct or an indirect cause.
The start Earley item is 
the only remaining possibility,
and it is in Earley set 0.
Therefore the start Earley item is the direct
or indirect cause of all other EIMs in Earley set 0.

From these considerations, we see that Earley 0
consists of the start Earley item and the transitive
closure of null transition from it.
By \eqref{t:null-transition-op-correct},
this is exactly the set of EIMs added to Earley set 0
in line 
\ref{line:initial-10}
of Algorithm \ref{a:initial}.
\end{proof}

\begin{algorithm}[h]
\caption{Marpa Scan pass}
\begin{algorithmic}[1]
\Procedure{Scan pass}{$\Vloc{i},\Vsym{a}$}
\State Note: Each pass through this loop is an EIM attempt
\For{each $\Veim{predecessor} \in \var{transitions}((\var{i} \subtract 1),\var{a})$}
\State $[\Vdr{from}, \Vloc{origin}] \gets \Veim{predecessor}$
\State $\Vdr{to} \gets \GOTO(\Vdr{from}, \Vsym{a})$
\State \Call{Add EIM pair}{$\Ves{i}, \Vdr{to}, \Vloc{origin}$}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Scan pass}
\label{p:scan-op}

\subsubsection{Scan pass complexity}
\label{p:scan-op-complexity}

\var{transitions} is a set of tables, one per Earley set.
The tables in the set are indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
since the number of Earley sets grows with
the length of the parse,
it cannot be assumed that Earley sets can be indexed by location
in \Oc{} time.
For the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be in \Oc{} time,
\Vloc{l} must represent a link directly to the Earley set.
In the case of scanning,
the lookup is always in the previous Earley set,
which can easily be tracked in \Oc{} space
and retrieved in \Oc{} time.
Inclusive time and space can be charged to the
\Veim{predecessor}.
Overhead is charged to the Earley set at \Vloc{i}.

\subsubsection{Scan pass correctness}
\label{p:scan-op-correct}

\begin{theorem}\label{t:scan-op-correct}
The scan pass is correct.
\end{theorem}

\begin{proof}
Omitted.
\end{proof}

\begin{algorithm}[h]
\caption{Reduction pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\State Note: \Vtable{i} may include EIM's added by
\State \hspace{2.5em} by \Call{Reduce one LHS}{} and
\State \hspace{2.5em} the loop must traverse these
\For{each Earley item $\Veim{work} \in \Vtable{i}$}
\State $[\Vdr{work}, \Vloc{origin}] \gets \Veim{work}$
\State $\Vsymset{lh-sides} \gets$ a set containing the LHS
\State \hspace\algorithmicindent of every completed rule in \Veim{work}
\For{each $\Vsym{lhs} \in \Vsymset{lh-sides}$}
\State \Call{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\EndFor
\EndFor
\State \Call{Memoize transitions}{\Vloc{i}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduction pass}

\subsubsection{Reduction pass complexity}
\label{p:reduction-op-complexity}

The loop over \Vtable{i} must also include
any items added by \call{Reduce one LHS}{}.
This can be done by implementing \Vtable{i} as an ordered
set and adding new items at the end.

Exclusive time is clearly \Oc{} per
\Veim{work},
and is charged to the \Veim{work}.
Additionally,
some of the time required by
\call{Reduce one LHS}{} is caller-included,
and therefore charged to this procedure.
Inclusive time from \call{Reduce one LHS}{}
is \Oc{} per call,
as will be seen in section \ref{p:reduce-one-lhs},
and is charged to the \Veim{work}
that is current
during that call to \call{Reduce one LHS}{}.
Overhead may be charged to the Earley set at \Vloc{i}.

\subsubsection{Reduction pass correctness}
\label{p:reduction-op-correct}

\begin{theorem}\label{t:reduction-op-correct}
The reduction pass is correct.
\end{theorem}

\begin{proof}
Omitted.
\end{proof}

\begin{algorithm}[h]
\caption{Memoize transitions}
\begin{algorithmic}[1]
\Procedure{Memoize transitions}{\Vloc{i}}
\For{every \Vsym{postdot}, a postdot symbol of $\Ves{i}$}
\State Note: \Vsym{postdot} is ``Leo eligible" if it is
\State \hspace\algorithmicindent  Leo unique and its rule is right recursive
\If{\Vsym{postdot} is Leo eligible}
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to a LIM
\Else
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to the set of EIM's at \Vloc{i} that have
\State \hspace\algorithmicindent \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize transitions}

The \var{transitions} table for \Ves{i}
is built once all EIMs have been
added to \Ves{i}.
We first look at the resource,
excluding the processing of Leo items.
The non-Leo processing can be done in
a single pass over \Ves{i},
in \Oc{} time per EIM.
Inclusive time and space are charged to the
Earley items being examined.
Overhead is charged to \Ves{i}.

We now look at the resource used in the Leo processing.
A transition symbol \Vsym{transition}
is Leo eligible if it is Leo unique
and its rule is right recursive.
(If \Vsym{transition} is Leo unique in \Ves{i}, it will be the
postdot symbol of only one rule in \Ves{i}.)
All but one of the determinations needed to decide
if \Vsym{transition} is Leo eligible can be precomputed
from the grammar,
and the resource to do this is charged to the parse.
The precomputation, for example,
for every rule, determines if it is right recursive.

One part of the test for
Leo eligibility cannot be done as a precomputation.
This is the determination whether there is only one dotted
rule in \Ves{i} whose postdot symbol is
\Vsym{transition}.
This can be done
in a single pass over the EIM's of \Ves{i}
that notes the postdot symbols as they are encountered
and whether any is enountered twice.
The time and space,
including that for the creation of a LIM if necessary,
will be \Oc{} time per EIM examined,
and can be charged to EIM being examined.

\begin{algorithm}[h]
\caption{Reduce one LHS symbol}
\begin{algorithmic}[1]
\Procedure{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\State Note: Each pass through this loop is an EIM attempt
\For{each $\var{pim} \in \var{transitions}(\Vloc{origin},\Vsym{lhs})$}
\State \Comment \var{pim} is a ``postdot item'', either a LIM or an EIM
\If{\var{pim} is a LIM, \Vlim{pim}}
\State Perform a \Call{Leo reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Vlim{pim}
\Else
\State Perform a \Call{Earley reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Veim{pim}, \Vsym{lhs}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduce one LHS}
\label{p:reduce-one-lhs}

To show that
\begin{equation*}
\var{transitions}(\Vloc{origin},\Vsym{lhs})
\end{equation*}
can be traversed in \Oc{} time,
we note
that the number of symbols is a constant
and assume that \Vloc{origin} is implemented
as a link back to the Earley set,
rather than as an integer index.
This requires that \Veim{work}
in \call{Reduction pass}{}
carry a link
back to its origin.
As implemented, Marpa's
Earley items have such links.

Inclusive time
for the loop over the EIM attempts
is charged to each EIM attempt.
Overhead is \Oc{} and caller-included.

\begin{algorithm}[h]
\caption{Earley reduction operation}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{i}, \Veim{from}, \Vsym{trans}}
\State $[\Vdr{from}, \Vloc{origin}] \gets \Veim{from}$
\State $\Vdr{to} \gets \GOTO(\Vdr{from}, \Vsym{trans})$
\State \Call{Add EIM pair}{\Ves{i}, \Vdr{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Earley Reduction operation}
\label{p:reduction-op}

\begin{sloppypar}
Exclusive time and space is clearly \Oc.
\call{Earley reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.
\end{sloppypar}

\begin{algorithm}[h]
\caption{Leo reduction operation}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{i}, \Vlim{from}}
\State $[\Vdr{from}, \Vsym{trans}, \Vloc{origin}] \gets \Vlim{from}$
\State $\Vdr{to} \gets \GOTO(\Vdr{from}, \Vsym{trans})$
\State \Call{Add EIM pair}{\Ves{i}, \Vdr{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Leo reduction operation}
\label{p:leo-op}

Exclusive time and space is clearly \Oc.
\call{Leo reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.

\begin{algorithm}[!htp]
\caption{Add EIM pair}\label{a:pair}
\begin{algorithmic}[1]
\Procedure{Add EIM pair}{$\Ves{i},\Vdr{base}, \Vloc{origin}$}
\State $\Veim{confirmed} \gets [\Vdr{base}, \Vloc{origin}]$
\If{\Veim{base} is new}
\State Add \Veim{base} to \Vtable{i}\label{line:pair-10}
\EndIf
\State $\Vdrset{next} \gets \GOTO(\Vdr{base}, \epsilon)$\label{line:pair-20}
\For{every $\Vdr{next} \in  \Vdrset{next}$}
\If{\Vdr{next} is a quasi-prediction}
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{i}, \Vloc{i}]$
\label{line:pair-25}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:pair-30}
\EndIf
\Else
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{origin}, \Vloc{i}]$
\label{line:pair-35}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:pair-40}
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Adding a pair of Earley items}
\label{p:add-eim-pair}

\subsubsection{Complexity}
\label{p:null-transition-op-complexity}

This operation adds a confirmed EIM
item and all of its
null-transitions.
Inclusive time and space is charged to the
calling procedure.

By theorem \ref{t:null-transition-Oc},
computing the null transitions is \Oc{}.
We show that other time charged is also \Oc{}
by singling out the two non-trivial cases:
checking that an Earley item is new,
and adding it to the Earley set.
\Marpa{} checks whether an Earley item is new
in \Oc{} time
by using a data structure called a PSL.
PSL's are the subject of Section \ref{s:per-set-lists}.
An Earley item can be added to the current
set in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

The space required for added EIM added is at most
for the \Vdr{base},
one for every transition over a null postdot symbol,
and 
one for every prediction.
The number of EIMs that result from
transitions over null postdot symbols is limited
by the maximum length of the RHS of a rule,
which is constant for a given \Cg{}.
At any \Vloc{i}, the number of predictions is
at most the number of rules in \Cg{}.
The number of EIMs that result from
predictions 
is therefore constant for a given \Cg{}.
Summing the space, we see that all space
requirement are constant, so that the space
is \Oc{} per call.

\subsubsection{Prediction EIM complexity}
\label{p:prediction-op-complexity}

Looking specifically at predictions,
From the discussion in 
\ref{p:null-transition-op-complexity}, we see that
no time or space is ever charged
to a predicted Earley item.
At most one attempt to add a \Veim{predicted} will
be made per attempt to add a \Veim{confirmed},
so that the total resource charged
remains \Oc.

\subsubsection{Null transition correctness}
\label{p:prediction-op-correct}

\begin{theorem}\label{t:null-transition-op-correct}
Algorithm \ref{a:pair} adds \Veim{base},
all of its null transitions
and no other EIMs.
\end{theorem}

\begin{proof}
By inspection, we see that 
Algorithm \ref{a:pair} adds items
at lines 
\ref{line:pair-10},
\ref{line:pair-30}
and \ref{line:pair-40}.
That the addition of \Veim{base}
at line \ref{line:pair-10}
is complete,
consistent and therefore correct,
follows directly from
inspection of
the pseudo-code.

By the definition of validity for null-scanned
and predicted Earley items, we see that
in all cases,
the origin and current location of the new
EIMs depends, directly or indirectly,
only on their values in \Veim{base}.
Therefore there is,
for each dotted rule,
only one correct set of values
for origin and current location
--
the ones
used by Algorithm \ref{a:pair}
in lines
\ref{line:pair-25}
and \ref{line:pair-35}.
Therefore the set of correct EIMs corresponds
one-to-one
with the dotted rules
and, from inspection of
Algorithm \ref{a:pair},
this is the set added by
Algorithm \ref{a:pair}
at lines
\ref{line:pair-30}
and \ref{line:pair-40}.

It remains to show that the set of dotted rules on
which the EIMs added
at lines \ref{line:pair-30}
and \ref{line:pair-40}
are based
is correct.
The EIMs added at lines
\ref{line:pair-30}
and \ref{line:pair-40} are based on
the dotted rules found
at line
\ref{line:pair-20}.
By theorem \ref{t:null-transition-dr-correct},
line \ref{line:pair-20} of
Algorithm \ref{a:pair} the
transitive closure of null transitions from
the dotted rule \Veim{base} found
by line \ref{line:pair-20} is
complete, consistent and therefore correct.
\end{proof}

\begin{theorem}\label{t:quasi-completion-correct}
Let \Veim{base} be a quasi-completion.
If Algorithm \ref{a:pair} adds \Veim{base},
it also adds all null transitions from it,
including the the completion EIM.
\end{theorem}

\begin{proof}
For null transitions,
the result follows directly
from theorem \ref{t:null-transition-op-correct}.
By the definition of completion EIM,
a completion EIM is the result of null transition
from any of its quasi-completions,
so that case also follows
from theorem \ref{t:null-transition-op-correct}.
\end{proof}

\begin{theorem}\label{t:prediction-correct}
If Algorithm \ref{a:pair} adds \Veim{base},
it also adds all predictions which are null transitions
from it,
including the valid quasi-prediction EIMs.
\end{theorem}

\begin{proof}
The predictions
and quasi-predictions are
null transitions form \Veim{base},
so the result follows directly
from theorem \ref{t:null-transition-op-correct}.
\end{proof}

\subsection{Per-set lists}
\label{s:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, x]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a link direct to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to or very similar to PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this \doc{}.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
the PSL for every previous Earley set, \Vloc{i},
tracks the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{\Cdr},
which is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's each time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

As before,
where \Ves{i} is an Earley set,
let \Vloc{i} be its location,
and vice versa.
\Vloc{i} is an integer which is
assigned as Earley sets are created.
We can easily assign a zero-based numbering
to the dotted rules of the grammar,
call it $\ID{\Vdr{x}}$,
and this can be used as the integer ID of a dotted rule.
Let $\PSL{\Ves{x}}{\var{y}}$
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}.

Consider the case where Marpa is building \Ves{j}
and wants to check whether Earley item \Veim{x} is new,
where $\Veim{x} = [ \Vdr{x}, \Vorig{x} ]$.
To check if \Veim{x} is new,
Marpa checks
\begin{equation*}
\var{time-stamp} = \PSL{\Ves{x}}{\ID{\Vdr{x}}}
\end{equation*}
If the entry has never been used,
we assume that $\var{time-stamp} = \Lambda$.
If $\var{time-stamp} \ne \Lambda \land \var{time-stamp} = \Vloc{j}$,
then \Veim{x} is not new,
and will not be added to the Earley set.

If $\Vloc{p} = \Lambda \lor \var{time-stamp} \ne \Vloc{j}$,
then \Veim{x} is new.
\Veim{x} is added to the Earley set,
and a new time-stamp is set, as follow:
\begin{equation*}
\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets \Vloc{j}.
\end{equation*}

\subsection{Complexity summary}

For convenience, we collect and summarize here
some of the observations of this section.

\begin{observation}
The time and space charged to an Earley item
which is actually added to the Earley sets
is \Oc.
\end{observation}

\begin{observation}
The time charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc.
\end{observation}

For evaluation purposes, \Marpa{} adds a link to
each EIM that records each attempt to
add that EIM,
whether originally or as a duplicate.
Traditionally, complexity results treat parsers
as recognizers, and such costs are ignored.
This will be an issue when the space complexity
for unambiguous grammars is considered.

\begin{observation}
The space charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc{} if links are included,
zero otherwise.
\end{observation}

As noted in Section \ref{p:add-eim-pair},
the time and space used by predicted Earley items
and attempts to add them is charged elsewhere.

\begin{observation}
No space or time is charged to predicted Earley items,
or to attempts to add predicted Earley items.
\end{observation}

\section{Preliminaries to the theoretical results}
\label{s:proof-preliminaries}

\subsection{Nulling symbols}
\label{s:nulling}

Recall that Marpa grammars,
without loss of generality,
contain neither empty rules or
properly nullable symbols.
This corresponds directly
to a grammar rewrite in the \Marpa{} implementation,
and its reversal during \Marpa's evaluation phase.
For the correctness and complexity proofs in this \doc{},
we assume an additional rewrite,
this time to eliminate nulling symbols.

Elimination of nulling symbols is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether a grammar \Cg{} accepts
an input \Cw{}
will not depend on the nulling symbols in its rules.

In its implementation,
\Marpa{} does not directly rewrite the grammar
to eliminate nulling symbols.
But nulling symbols are ignored in
creating the dotted rules,
and must be restored during \Marpa's evaluation phase,
so that the implementation and
this simplification for theory purposes
track each other closely.

\subsection{Comparing Earley items}

\begin{definition}
An Earley item
$\Veim{x} = [\Vdr{x}, \Vorig{x}]$
\dfn{corresponds}
to another Earley item \Veim{y}
if and only if
$\Veim{y} = [\Vdr{y}, \Vorig{x}].$
\end{definition}

\begin{definition}
The Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{consistent} if and only if
all of its EIM's correspond to
EIM's in
\EVtable{\Leo}{i}.
\end{definition}

\begin{definition}
The Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{complete} if and only if for every
Earley item in \EVtable{\Leo}{i}
there is a corresponding Earley item in
\EVtable{\Marpa}{i}.
\end{definition}

\begin{definition}
A Marpa Earley set is \dfn{correct}
if and only that Marpa Earley set is complete
and consistent.
\end{definition}

\section{Marpa is correct}
\label{s:correct}

We are now is a position to show that Marpa is correct.
\begin{theorem}
\textup{ $\myL{\Marpa,\Cg} = \myL{\Cg}$ }
\end{theorem}

\begin{proof}
Omitted.
\end{proof}

\section{Marpa recognizer complexity}
\label{s:complexity}

\subsection{Complexity of each Earley item}

For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
We showed that this rewrite
is without loss of generality
in Section \ref{s:nulling},
when we examined correctness.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

\begin{lemma}\label{l:nulling-rewrite}
All time and space required
to rewrite the grammar to eliminate nulling
symbols, and to restore those rules afterwards
in the Earley sets,
can be allocated
to the Earley items
in such a way that each Earley item
requires \Oc{} time and space.
\end{lemma}

\begin{proof}
The time and space used in the rewrite is a constant
that depends on the grammar,
and is charged to the parse.
The reversal of the rewrite can be
done in a loop over the Earley items,
which will have time and space costs
per Earley item,
plus a fixed overhead.
The fixed overhead is \Oc{}
and is charged to the parse.
The time and space per Earley item
is \Oc{}
because the number of
rules into which another rule must be rewritten,
and therefore the number of Earley items
into which another Earley item must be rewritten,
is a constant that depends
on the grammar.
\end{proof}

\begin{theorem}\label{t:O1-time-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item,
and each attempt to
add a duplicate Earley item,
requires \Oc{} time.
\end{theorem}

\begin{theorem}\label{t:O1-space-per-eim}
All space in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item
requires \Oc{} space and,
if links are not considered,
each attempt to add a duplicate
Earley item adds no additional space.
\end{theorem}

\begin{theorem}\label{t:O1-links-per-eim}
If links are considered,
all space in \Marpa{} can be allocated
to the Earley items
in such a way that each Earley item
and each attempt to
add a duplicate Earley item
requires \Oc{} space.
\end{theorem}

\begin{proof}[Proof of Theorems
\ref{t:O1-time-per-eim},
\ref{t:O1-space-per-eim},
and \ref{t:O1-links-per-eim}]
These theorems follows from the observations
in Section \ref{s:pseudocode}
and from Lemma \ref{l:nulling-rewrite}.
\end{proof}

The same complexity results apply to \Marpa{} as to \Leo,
and the proofs are very similar.
\Leo's complexity results\cite{Leo1991}
are based on charging
resource to Earley items,
as were the results
in Earley's paper\cite{Earley1970}.

Earley\cite{Earley1970} shows that,
for unambiguous grammars,
every attempt to add
an Earley item will actually add one.
In other words, there will be no attempts to
add duplicate Earley items.
Earley's proof shows that for each attempt
to add a duplicate,
the causation must be different --
that the EIM's causing the attempt
differ in either their dotted
rules or their origin.
Multiple causations for an Earley item
would mean multiple derivations
for the sentential form that it represents.
That in turn would mean that
the grammar is ambiguous,
contrary to assumption.

\begin{theorem}\label{t:tries-O-eims}
For an unambiguous grammar,
the number of attempts to add
Earley items will be less than or equal to
\begin{equation*}
\textup{
    $\var{c} \times \Rtablesize{\Marpa}$,
}
\end{equation*}
where \var{c} is a constant
that depends on the grammar.
\end{theorem}

\begin{proof}
Let \var{initial-tries} be the number of attempts to add the initial item to
the Earley sets.
For Earley set 0, it is clear from the pseudocode
that there will be no attempts to add duplicate EIM's:
\begin{equation*}
\var{initial-tries} = \bigsize{\Vtable{0}}
\end{equation*}

Let \var{leo-tries} be the number of attempted Leo reductions in
Earley set \Vloc{j}.
For Leo reduction,
we note that by its definition,
duplicate attempts at Leo reduction cannot occur.
From the pseudo-code of Sections \ref{p:reduce-one-lhs}
and \ref{p:leo-op},
we know there will be at most one Leo reduction for
each each dotted rule in the current Earley set,
\Vloc{j}.
\begin{equation*}
\var{leo-tries} \le \bigsize{\Vtable{j}}
\end{equation*}

Let \var{scan-tries} be the number of attempted scan operations in
Earley set \Vloc{j}.
Marpa attempts a scan operation,
in the worst case,
once for every EIM in the Earley set
at $\Vloc{j} \subtract 1$.
Therefore, the number of attempts
to add scans
must be less than equal to \bigsize{\Etable{\var{j} \subtract 1}},
the number
of actual Earley items at
$\Vloc{j} \subtract 1$.
\begin{equation*}
\var{scan-tries} \le \bigsize{\Etable{\var{j} \subtract 1}}
\end{equation*}

Let \var{predict-tries} be the number of attempted predictions in
Earley set \Vloc{j}.
\Marpa{} includes prediction
in its scan and reduction operations,
and the number of attempts to add duplicate predicted EIM's
must be less than or equal
to the number of attempts
to add duplicate confirmed EIM's
in the scan and reduction operations.
\begin{equation*}
\var{predict-tries} \le \var{reduction-tries} + \var{scan-tries}
\end{equation*}

The final and most complicated case is Earley reduction.
Recall that \Ves{j} is the current Earley set.
Consider the number of reductions attempted.
\Marpa{} attempts to add an Earley reduction result
once for every triple
\begin{equation*}
[\Veim{predecessor}, \Vsym{transition}, \Veim{component}].
\end{equation*}
where
\begin{equation*}
\begin{split}
& \Veim{component} = [ \Vdr{component}, \Vloc{component-origin} ]  \\
 \land \quad & \Vsym{transition} = \LHS{\Vdr{component}}. \\
\end{split}
\end{equation*}

We now put an upper bound on number of possible values of this triple.
The number of possibilities for \Vsym{transition} is clearly at most
\size{\var{symbols}},
the number of symbols in \Cg{}.
We have $\Veim{component} \in \Ves{j}$,
and therefore there are at most
$\bigsize{\Ves{j}}$ choices for \Veim{component}.

Let the number of dotted rules be \Vsize{\Cdr}.
We can show that the number of possible choices of
\Veim{predecessor} is at most \Vsize{\Cdr}, by a reductio.
Suppose, for the reductio,
there were more than \Vsize{dr} possible choices of \Veim{predecessor}.
Then there are two possible choices of \Veim{predecessor} with
the same dotted rule.
Call these \Veim{choice1} and \Veim{choice2}.
We know, by the definition of Earley reduction, that
$\Veim{predecessor} \in \Ves{j}$,
and therefore we have
$\Veim{choice1} \in \Ves{j}$ and
$\Veim{choice2} \in \Ves{j}$.
Since all EIM's in an Earley set must differ,
and
since \Veim{choice1} and \Veim{choice2} both share the same
dotted rule,
they must differ in their origin.
But two different origins would produce two different derivations for the
reduction, which would mean that the parse was ambiguous.
This is contrary to the assumption for the theorem
that the grammar is unambiguous.
This shows the reductio
and that the number of choices for \Veim{predecessor},
compatible with \Vorig{component}, is as most \Vsize{dr}.

\begin{sloppypar}
Collecting the results we see that the possibilities for
each \Veim{component} are
\begin{equation*}
\begin{alignedat}{2}
& \Vsize{\Cdr} &&
\qquad \text{choices of \Veim{predecessor}} \\
\times \; & \Vsize{symbols} &&
\qquad \text{choices of \Vsym{transition}} \\
\times \; & \size{\Ves{j}} &&
\qquad \text{choices of \Veim{component}} \\
\end{alignedat}
\end{equation*}
\end{sloppypar}

The number of reduction attempts will therefore be at most
\begin{equation*}
\var{reduction-tries} \leq \Vsize{\Cdr} \times \Vsize{symbols} \times \bigsize{\Ves{j}}.
\end{equation*}

Summing
\begin{multline*}
\var{tries} =
\var{scan-tries} +
\var{leo-tries} + \\
\var{predict-tries} +
\var{reduction-tries} +
\var{initial-tries},
\end{multline*}
we have,
where $\var{n} = \Vsize{\Cw}$,
the size of the input,
\begin{equation*}
\begin{alignedat}{2}
& \bigsize{\Vtable{0}} & \quad &
\qquad \text{initial EIM's} \\
+ \; & \sum\limits_{i=0}^{n}{
\bigsize{\Vtable{j}}
} &&
\qquad \text{LIM's} \\
+ \; & 2 \times \sum\limits_{i=1}^{n}{
\bigsize{\Etable{\var{j} \subtract 1}}
} &&
\qquad \text{scanned EIM's} \\
+ \; & 2 \times \sum\limits_{i=0}^{n}{\Vsize{\Cdr} \times \Vsize{symbols} \times \bigsize{\Ves{j}}} &&
\qquad \text{reduction EIM's}.
\end{alignedat}
\end{equation*}
In this summation,
\var{prediction-tries} was accounted for by counting the scanned and predicted
EIM attempts twice.
Since \Vsize{\Cdr} and \Vsize{symbols} are both constants
that depend only on \Cg{},
if we collect the terms of the summation,
we will find a constant \var{c}
such that
\begin{equation*}
\var{tries} \leq \var{c} \times \sum\limits_{i=0}^{n}{\bigsize{\Vtable{j}}},
\end{equation*}
and
\begin{equation*}
\var{tries} \leq \var{c} \times \Rtablesize{\Marpa},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.\qedhere
\end{proof}

As a reminder,
we follow tradition by
stating complexity results in terms of \var{n},
setting $\var{n} = \Vsize{\Cw}$,
the length of the input.

\begin{theorem}\label{t:eim-count}
For a context-free grammar,
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} = \order{\var{n}^2}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
By Theorem \ref{t:es-count},
the size of the Earley set at \Vloc{i}
is $\order{\var{i}}$.
Summing over the length of the input,
$\Vsize{\Cw} = \var{n}$,
the number of EIM's in all of \Marpa's Earley sets
is
\begin{equation*}
\sum\limits_{\Vloc{i}=0}^{\var{n}}{\order{\var{i}}}
= \order{\var{n}^2}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:ambiguous-tries}
For a context-free grammar,
the number of attempts to add
Earley items is $\order{\var{n}^3}$.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries-O-eims},
we see that the only bound that required
the assumption that \Cg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Let \var{other-tries}
be attempts to add EIM's other than
as the result of Earley reductions.
By Theorem \ref{t:eim-count},
\begin{equation*}
\Rtablesize{\Marpa} = \order{\var{n}^2},
\end{equation*}
and by Theorem \ref{t:tries-O-eims},
\begin{equation*}
\var{other-tries} \le \var{c} \times \Rtablesize{\Marpa},
\end{equation*}
so that
$\var{other-tries} = \order{\var{n}^2}$.

\begin{sloppypar}
Looking again at \var{reduction-tries}
for the case of ambiguous grammars,
we need to look again at the triple
\begin{equation*}
[\Veim{predecessor}, \Vsym{transition}, \Veim{component}].
\end{equation*}
We did not use the fact that the grammar was unambigous in counting
the possibilities for \Vsym{transition} or \Veim{component}, but
we did make use of it in determining the count of possibilities
for \Veim{predecessor}.
We know still know that
\begin{equation*}
\Veim{predecessor} \in \Ves{component-origin},
\end{equation*}
where
\Vloc{component-origin} is the origin of \Veim{component}.
Worst case, every EIM in \Ves{component-origin} is a possible
match, so that
the number of possibilities for \Veim{predecessor} now grows to
\size{\Ves{component-origin}}, and
\begin{equation*}
\var{reduction-tries} =
\bigsize{\Ves{component-origin}} \times \Vsize{symbols} \times \bigsize{\Ves{j}}.
\end{equation*}
\end{sloppypar}

In the worst case $\var{component-origin} \simeq \var{j}$,
so that by Theorem \ref{t:es-count},
\begin{equation*}
\size{\Ves{component-origin}} \times \size{\Ves{j}} = \order{\var{j}^2}.
\end{equation*}
Adding \var{other-tries}
and summing over the Earley sets,
we have
\begin{equation*}
\order{\var{n}^2} +
\! \sum\limits_{\Vloc{j}=0}^{n}{\order{\var{j}^2}} = \order{\var{n}^3}.
\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:leo-right-recursion}
Either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let the constant \var{c} be the number
of symbols.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
(Since for the purposes of these
complexity results we ignore nulling symbols,
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.)
So part of the rightmost derivation must take the form
\begin{equation*}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation*}
But the first step of this derivation sequence must use a rule of the
form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

\subsection{The complexity results}
We are now in a position to show
specific time and space complexity results.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
the number of Earley items produced by
\Leo{} when parsing input \Cw{} with an LR-regular grammar \Cg{} is
\begin{equation*}
\order{\Vsize{\Cw}} = \order{\var{n}}.
\end{equation*}
\Marpa{} may produce more Earley items than \Leo{}
because
\Marpa{} does not apply Leo memoization to Leo sequences
which do not contain right recursion.

By the definition of an EIM,
and the construction of a Leo sequence,
it can be seen that a Leo sequence
corresponds step-for-step with a
right derivation.
It can therefore be seen that
the number of EIM's in the Leo sequence
and the number of right derivation steps
in its corresponding right derivation
will be the same.

Consider one EIM that is memoized in \Leo{}.
If not memoized because it is not a right recursion,
this EIM will be expanded to a sequence
of EIM's.
How long will this sequence of non-memoized EIM's
be, if we still continue to memoize EIM's
which correspond to right recursive rules?
The EIM sequence, which was formerly a memoized Leo sequence,
will correspond to a right
derivation that does not include
any steps that use right recursive rules.
By Theorem \ref{t:leo-right-recursion},
such a
right derivation can be
of length at most \var{c1},
where \var{c1} is a constant that depends on \Cg{}.
As noted, this right derivation has
the same length as its corresponding EIM sequence,
so that each EIM not memoized in \Marpa{} will expand
to at most \var{c1} EIM's.

The number of EIM's per Earley set
for an LR-regular grammar in a \Marpa{} parse
is less than
\begin{equation*}
    \var{c1} \times \order{\var{n}} = \order{\var{n}}.
\end{equation*}

LR-regular grammars are unambiguous, so that
by Theorem \ref{t:tries-O-eims},
the number of attempts that \Marpa{} will make to add
EIM's is less than or equal to
\var{c2} times the number of EIM's,
where \var{c2} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is
\begin{equation*}
    \var{c2} \times \order{\var{n}}
    = \order{\var{n}}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
By assumption, \Cg{} is unambiguous, so that
by Theorem \ref{t:tries-O-eims},
and Theorem \ref{t:eim-count},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{}
for unambiguous grammars is \order{\var{n}^2}.
\end{proof}

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-time-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\begin{theorem}\label{t:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-space-per-eim}
and Theorem \ref{t:eim-count}.
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{t:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-links-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Marpa-2013}
Jeffrey~Kegler.
\newblock Marpa, a practical general parser: the recognizer.
\newblock \url{https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2015: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}

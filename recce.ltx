% Copyright 2015 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{hyperref}
\usepackage{ragged2e}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{algorithm, algpseudocode}
\usepackage{url}
\usepackage{amsfonts}% to get the \mathbb alphabet
\makeindex

% This is used to find the font size if need be
% its uses are usually commented out
\makeatletter
\newcommand\thefontsize[1]{{#1 The current font size is: \f@size pt\par}}
\makeatother

% \DeclareMathSizes{6}{6}{6}{6}
% \DeclareMathSizes{8}{8}{8}{8}
% \DeclareMathSizes{10}{10}{10}{10}
% \DeclareMathSizes{10.95}{10.95}{10.95}{10.95}
% \DeclareMathSizes{12}{12}{12}{12}
% \DeclareMathSizes{14.4}{14.4}{14.4}{14.4}
% \DeclareMathSizes{20.74}{20.74}{20.74}{20.74}
% \DeclareMathSizes{24.88}{24.88}{24.88}{24.88}

% This is now a "paper", but may be a chapter
% or something else someday
% This command will make any such change easier.
\newcommand{\doc}{paper}

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{#1}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{}
\newcommand{\Cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}
\newcommand{\decr}[1]{\ensuremath{#1 \subtract 1}}
\newcommand{\Vdecr}[1]{\decr{\var{#1}}}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\de}{\rightarrow}
\newcommand{\derives}{\Rightarrow}
\newcommand{\nderives}{\not\Rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\ndestar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\not\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\leo}[1]{#1_{LEO}}
\newcommand{\Vleo}[1]{\ensuremath{\var{#1}_{LEO}}}
\newcommand{\inst}[1]{#1_{INST}}
\newcommand{\Vinst}[1]{\ensuremath{\var{#1}_{INST}}}
\newcommand{\Ees}[1]{\ensuremath{#1_{ES}}}
\newcommand{\loc}[1]{\ensuremath{{#1}_{LOC}}}
\newcommand{\Vloc}[1]{\loc{\var{#1}}}
\newcommand{\Ves}[1]{\Ees{\var{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vruleset}[1]{\ensuremath{\var{#1}_{\set{RULE}}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\str}[1]{\ensuremath{#1_{STR}}}
\newcommand{\Vstr}[1]{\ensuremath{\langle\langle\var{#1}\rangle\rangle}}
\newcommand{\sym}[1]{\ensuremath{#1_{SYM}}}
\newcommand{\Vsym}[1]{\ensuremath{\langle\var{#1}\rangle}}
\newcommand{\Vorig}[1]{\ensuremath{\var{#1}_{ORIG}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\mk}[1]{\lfloor#1\rceil}
% \newcommand{\mkdelim}{\,\,}
\newcommand{\mkdelim}{}
\newcommand{\mkr}[1]{\mkdelim \mk{#1}}
\newcommand{\mkl}[1]{\mk{#1} \mkdelim}
\newcommand{\mkm}[1]{\mkdelim \mk{#1} \mkdelim}
\newcommand{\Vmk}[1]{\mk{\var{#1}}}
\newcommand{\Vmkl}[1]{\Vmk{#1} \mkdelim}
\newcommand{\Vmkr}[1]{\mkdelim \Vmk{#1}}
\newcommand{\Vmkm}[1]{\mkdelim \Vmk{#1} \mkdelim}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}

\newcommand{\Cdr}{\var{dr}}
\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\sym{\Cw[\var{#1}]}}}
\newcommand{\Crules}{\var{rules}}
\newcommand{\GOTO}{\ensuremath{\mymathop{GOTO}}}
\newcommand{\Alt}[1]{\ensuremath{\mymathop{Alt}(#1)}}
\newcommand{\Next}[1]{\mymathop{Next}(#1)}
\newcommand{\Prev}[1]{\mymathop{Prev}(#1)}
\newcommand{\Predict}[1]{\mymathop{Predict}(#1)}
\newcommand{\Left}[1]{\ensuremath{\mymathop{Left}(#1)}}
\newcommand{\Right}[1]{\ensuremath{\mymathop{Right}(#1)}}
\newcommand{\Predot}[1]{\ensuremath{\mymathop{Predot}(#1)}}
\newcommand{\Postdot}[1]{\ensuremath{\mymathop{Postdot}(#1)}}
\newcommand{\Penult}[1]{\mymathop{Penult}(#1)}
\newcommand{\LHS}[1]{\ensuremath{\mymathop{LHS}(#1)}}
\newcommand{\RHS}[1]{\mymathop{RHS}(#1)}
\newcommand{\Origin}[1]{\ensuremath{\mymathop{Origin}(#1)}}
\newcommand{\DR}[1]{\ensuremath{\mymathop{DR}(#1)}}
\newcommand{\Current}[1]{\ensuremath{\mymathop{Current}(#1)}}
\newcommand{\Valid}[1]{\ensuremath{\mymathop{Valid}(#1)}}
\newcommand{\Memoized}[1]{\ensuremath{\mymathop{Memoized}(#1)}}
\newcommand{\Symbol}[1]{\ensuremath{\mymathop{Symbol}(#1)}}
\newcommand{\LeoUnique}[1]{\mymathop{Leo-Unique}(#1)}
\newcommand{\ID}[1]{\mymathop{ID}(#1)}
\newcommand{\PSL}[2]{\mymathop{PSL}[#1][#2]}
\newcommand{\myL}[1]{\mymathop{L}(#1)}

\newcommand\Ctables{\var{tables}}
\newcommand\Vtables[1]{\Ctables[\alg{#1}]}

\newcommand\Etable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\bigEtable[1]{\ensuremath{\mymathop{table}\bigl[#1\bigr]}}
\newcommand\Rtable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\Rtablesize[1]{\ensuremath{\bigl| \mymathop{table}[#1] \bigr|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\mymathop{table}[#1,#2]}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

% I don't like to put whole paragraphs in italics,
% so I make this simple variation on the "plain" theoremstyle
\newtheoremstyle{myplain}
  {10pt}   % ABOVESPACE
  {10pt}   % BELOWSPACE
  {\normalfont}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC

\theoremstyle{myplain}

\newtheorem{oldtheorem}{Theorem}[section]
\newenvironment{theorem}
{
  \begin{oldtheorem}
}
{
  % \begin{center}
  % \vspace{-.4\baselineskip}
  % \rule{3em}{.5pt}
  % \end{center}
  \end{oldtheorem}
}

\newtheorem{lemma}[oldtheorem]{Lemma}
\index{Lemmas|see{Theorems}}

\newtheorem{baredefinition}[oldtheorem]{Definition}
\newenvironment{definition}{ \begin{baredefinition}
}{ \begin{center}
\vspace{-.4\baselineskip}
\rule{3em}{.5pt}
\end{center} \end{baredefinition} }

\newtheorem{bareobservation}[oldtheorem]{Definition}
\newenvironment{observation}{ \begin{bareobservation}
}{ \begin{center}
\vspace{-.4\baselineskip}
\rule{3em}{.5pt}
\end{center} \end{bareobservation} }

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}
\hyphenation{al-ti-tude}

% I use parboxes in equations.  This sets a useful width for them.
\newlength{\mathparwidth}
\newlength{\longtagwidth}
\settowidth{\longtagwidth}{(999)\qquad}
\setlength{\mathparwidth}{\dimexpr\textwidth-\longtagwidth}
\newcommand{\myparbox}[1]{
  \parbox{\mathparwidth}{ \begin{RaggedRight}
#1 \par
\end{RaggedRight} }
}

\begin{document}

\date{\today}

\title{The Marpa recognizer: a simplification}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2015 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
{\bf
This is a draft.
}
Marpa is
a practical and fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
The Marpa recognizer is the first
practical implementation
of the improvements
to Earley's algorithm found in
Joop Leo's 1991 paper.
Marpa has a new parse engine that
allows the user to alternate
between Earley-style syntax-driven parsing,
and procedural parsing of the type available
with recursive descent.
Marpa is left-eidetic so that,
unlike recursive descent,
Marpa's procedural logic has available
full information about
the state of the parse so far.
The Marpa recognizer described
is a simplification of that
in our 2013 paper\cite{Marpa-2013}.
\end{abstract}

\maketitle

\section{Status of this paper}
This paper is a draft.
Please use the date at the bottom of the
first page
to make sure this is the latest revision.
If this revision is not the latest, please ignore it.
If this does seems to be the latest version,
and you are adventurous,
then read on.

All sections of this paper are in \dfn{early draft} stage.
``Early draft'' means that the author's thoughts are not well settled,
and the sections are likely to contain inconsistencies and errors.
Comments and corrections on early draft
sections are not encouraged --
the material may be already be slated for deletion,
rewriting or rethinking.

The author hopes to move some sections into
\dfn{advanced draft} status soon.
Advanced draft sections are still subject to revision,
but the author hopes they are stable enough
to make comments and corrections useful.
Readers should note that changes in early draft sections
sometimes require changes
to sections whose content was thought to
be settled.
Therefore, it is possible that
even sections in advanced draft status
will change dramatically.

\section{Introduction}

The first stable version of Marpa was uploaded to
a public archive on Solstice Day 2011.
Prior to this,
general context-free parsing
had never incorporated in a practical, highly available tool
like those that had long existed
for LALR\cite{Johnson} and
regular expressions.
This was in spite of the strong academic literature behind
general context-free parsers.

The Marpa project was intended
to take the best results from the literature
on Earley parsing off the pages
of the journals and bring them
to a wider audience.
This paper describes the algorithm used
in the most recent version of Marpa,
Marpa::R2\cite{Marpa-R2}.
It is a simplification of the one presented
in our earlier paper\cite{Marpa-2013}.

As implemented,
Marpa parses
all context-free grammars
which are free of cycles,
unproductive symbols,
and 
inaccessible symbols.
Time bounds are the best of Leo\cite{Leo1991}
and Earley\cite{Earley1970}.
The Leo bound,
\On{} for LR-regular grammars,
is especially relevant to
Marpa's goal of being a practical parser.
It includes all of the deterministic context-free grammars:
If a grammar is in a class currently in practical use,
Marpa parses it in linear time.

Error-detection properties
have been overlooked in the past.
They are, in fact, extremely important.
In many parsing applications,
such as compilers,
the quality of the error messages is nearly
as important as the quality of the output
on success.
Marpa has the immediate error detection property,
but it goes well beyond that:
Marpa is left-eidetic.

As a parser in the Earley lineage,
Marpa has available in its tables,
full information 
about partial and full rule instances recognized.
Marpa's parse engine is arranged to make this
information available at every parse location,
before any symbols are scanned.

As one example,
left-eideticsim allows the lexer to check its list
of acceptable tokens before any token is scanned.
Marpa's most popular implementation uses this information
to implement a event-driven approach.
This is possible because
rejection of tokens is easily and
efficiently recoverable in Marpa.

In traditional parser practice,
error detection is an act of desperation:
a technique not to be used unless necessary.
With left-eideticism and trouble-free recovery
from token rejection,
error detection becomes
a parsing technique in its own right.
If a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called this
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of the Marpa::R2\cite{Marpa-R2},
the author has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

Section
\ref{s:preliminaries} describes the notation and conventions
of this \doc.
Section \ref{s:rewrite} deals with Marpa's
grammar rewrites.
The next sections develop the ideas for Earley's algorithm.
Section \ref{s:dotted} describes dotted rules.
Section \ref{s:earley-items} describes Earley items.
Section \ref{s:earley-implementation} describes Earley sets
and
develops the rest of the basic ideas behind Earley implementations.

Section \ref{s:leo} describes Leo's modification
to Earley's algorithm.
Section \ref{s:pseudocode} presents the pseudocode
for Marpa's recognizer.
Section
\ref{s:correctness}
contain a proof of Marpa's correctness.
Section \ref{s:linear} sets out our linear
time and space complexity results.
Section \ref{s:other-complexity} presents out
other complexity results.

\section{Preliminaries}
\label{s:preliminaries}

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.
This \doc{} will
use subscripts to indicate commonly occurring types.
\begin{center}
\begin{tabular}{ll}
$\var{X}_T$ & The variable \var{X} of type $T$ \\
$\var{set-one}_\set{T}$ & The variable \var{set-one} of type set of $T$ \\
$SYM$ & The type for a symbol \\
$STR$ & The type for a string \\
$EIM$ & The type for an Earley item \\
\sym{\var{a}} & A variable \var{a} of type $SYM$ \\
\str{\var{a}} & A variable \var{a} of type $STR$ \\
\Veim{\var{a}} & A variable \var{a} of type $EIM$ \\
\Vsymset{set-two} & The variable \var{set-two}, a set of strings \\
\Veimset{set-two} & The variable \var{set-two}, a set of Earley items
\end{tabular}
\end{center}
Strings and symbols occur so frequently that they have a special
notation:
\begin{center}
\begin{tabular}{ll}
\Vsym{a} & \var{a}, a symbol variable \\
\Vstr{a} & \var{a}, a string variable
\end{tabular}
\end{center}
Subscripts may be omitted when the type
is obvious from the context.
The notation for
constants is the same as that for variables.
Multi-character variable names will be common.
\begin{center}
\begin{tabular}{ll}
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \Cat \var{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\end{tabular}
\end{center}
Operations are never implicit, with the exception of concatenation.
Concatenation is not shown when its use is obvious.
Type names are often used in the text
as a convenient way to refer to
their type.

Where \Vsymset{syms} is non-empty set of symbols,
let $\var{syms}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Where \Vstr{s} is a string,
let \size{\Vstr{s}} be its length, counted in symbols.
Let $\var{syms}^+$ be
\begin{equation*}
\bigl\{ \Vstr{x}
\bigm| \Vstr{x} \in \var{syms}* \;\; \land \;\; \Vsize{\Vstr{x}} > 0
\bigr\}.
\end{equation*}

In this \doc{} we use,
without loss of generality,
the grammar \Cg{},
where \Cg{} is the 4-tuple
\begin{equation*}
    (\Vsymset{nt}, \Vsymset{term}, \var{rules}, \Vsym{accept}).
\end{equation*}
\Vsymset{nt} is a set of symbols called non-terminals,
and \Vsymset{term} is a set of symbols called terminals.
Here $\Vsym{accept} \in \var{nt}$.
The vocabulary of the the grammar is the union of
the sets of terminals and non-terminals:
$$ \Vsymset{vocab} = \Vsymset{nt} \cup \Vsymset{term}. $$
If a string of symbols contains only terminal symbols,
that string is called a \dfn{sentence}.

\Vruleset{rules} is a set of rules (type \type{RULE}),
where a rule is a duple
of the form $[\Vsym{lhs} \de \Vstr{rhs}]$,
such that
\begin{equation*}
\Vsym{lhs} \in \var{nt} \quad \text{and}
\quad \Vstr{rhs} \in \var{vocab}^+.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\Vrule{r}}$ and $\RHS{\Vrule{r}}$, respectively.
This definition follows \cite{AH2002},
which departs from tradition by disallowing an empty RHS.
This restriction applies only to Marpa's internal grammar.
As explained in Section \ref{s:rewrite},
it is invisible to Marpa's users.

The rules imply the traditional rewriting system,
in which
\begin{center}
\begin{tabular}{lp{.6\textwidth}}
$\Vstr{x} \derives \Vstr{y}$ &
states that \Vstr{x} derives \Vstr{y} in exactly one step; \\
$\Vstr{x} \deplus \Vstr{y}$ &
states that \Vstr{x} derives \Vstr{y} in one or more steps;
and \\
$\Vstr{x} \destar \Vstr{y}$ &
states that \Vstr{x} derives \Vstr{y} in zero or more steps.
\end{tabular}
\end{center}


The language of \var{g} is $\myL{\Cg}$, where
\begin{equation}
\label{eq:def-L-g}
\myL{\Cg} \defined \left\lbrace
\Vstr{z} \mid \Vstr{z} \in \var{term}^\ast \land \Vsym{accept} \destar \Vstr{z}
\right\rbrace
\end{equation}

We say that symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} \destar \epsilon$.
We say that symbol \Vsym{x} is \dfn{nulling} if and only if
$$\forall \Vstr{y} \mid \Vsym{x} \destar \Vstr{y}
  \implies \Vstr{y} = \epsilon $$.
This implies that $\Vstr{y} \derives \epsilon$ if and only
if all of the symbols in \Vstr{y} are nullable.
As a special case,
when \Vstr{z} is an empty string of symbols,
we will say that $\Vstr{z} \derives \epsilon$.

Following Aycock and Horspool\cite{AH2002},
all nullable symbols in grammar \Cg{} are nulling -- every symbol
which can derive the null string always derives the null string.
This restriction applies only to Marpa's internal grammar.
As explained in Section \ref{s:rewrite},
this is invisible to Marpa's users.

If a symbol is not nullable, we say that is
\dfn{non-nullable}.
In this paper, non-nullable is usually an antonym
of nulling, which is not the case in traditional
parsing theory.
To prevent this confusion,
and as a better fit to terminology yet to be
introduced,
we more often say that a non-nullable symbol
is \dfn{telluric}.

Consider the derivation
\begin{equation}
\Vsym{A} \derives \Vstr{rhs} \destar \Vstr{left} \cat \Vsym{A} \cat \Vstr{right}
\end{equation}
We say that the rule $\Vsym{A} \de \Vstr{rhs}$
and the symbol \Vsym{A} are
\begin{center}
\begin{tabular}{ll}
\dfn{middle-recursive} & if and only if
  $\Vstr{left} \ndestar \epsilon \land \Vstr{right} \ndestar \epsilon$ \\
\dfn{left-recursive} &  if and only if $\Vstr{left} \destar \epsilon$ \\
\dfn{right-recursive} &  if and only if $\Vstr{right} \destar \epsilon$ \\
\dfn{cyclic} & if and only if
  $\Vstr{left} \destar \epsilon \land \Vstr{right} \destar \epsilon$.
\end{tabular}
\end{center}
We assume that a Marpa grammar is cycle-free --
that none of its rules are cyclic.

We say that \Vstr{desc} is a direct descendant of \Vsym{A} if
it is \Vstr{A-rhs} where $\Vsym{A} \de \Vstr{A-rhs}$ is a rule,
or if it is the empty string where \Vsym{A} is a nulling terminal.
We say that a derivation is \dfn{leftmost} if at each of its steps,
its leftmost nonterminal is replaced by one of its direct descendants.
We say that a derivation is \dfn{rightmost} if at each of its steps,
its rightmost nonterminal is replaced by one of its direct descendants.

Also without loss of generality,
it is assumed
that there is a dedicated acceptance rule, \Vrule{accept}
and a dedicated acceptance symbol, $\Vsym{accept} = \LHS{\Vrule{accept}}$,
such that
for all \Vrule{x},
\begin{multline*}
\Vsym{accept} \notin \RHS{\Vrule{x}} \land \\
\Vsym{accept} = \LHS{\Vrule{x}} \implies \Vrule{accept} = \Vrule{x}.
\end{multline*}

We assume that every symbol is productive --
that is, that it derives a sentence.
We assume that every symbol is accessible --
that is, that it is derivable from the start symbol.

Let the input to
the parse be \Cw{} such that $\Cw \in \myL{\Cg}$.
Locations in the input will be of type \type{LOC}.
Let \Vsize{w} be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.
Let \CVw{i} be character
at position \var{i}
of the input.
String position is zero-based,
so that
$0 \le \Vloc{i} < \Vsize{w}$.
Let $\var{w}[\var{a}, \var{b}]$
be the contiguous substring
from position \var{a} to
position \var{b}, inclusive,
so that
$$ \bigsize{\var{w}[\var{a}, \var{b}]} = (\var{b} \subtract \var{a}) + 1. $$

The alert reader may have noticed that the previous definition
of \Cw{} did not allow zero-length inputs.
To simplify the mathematics, we exclude null parses
and nulling grammars from consideration.
(Nulling grammars are those that recognize only the null string.)
In its implementations,
the Marpa parser
deals with null parses and nulling grammars as special cases.

In the context of a grammar \Cg{} and an input \Cw{},
we will often use location-marked derivations.
Location-marked derivation steps are like the derivation steps
of the traditional rewriting system except that they also contain
location markers of the form $\mk{x}$, where \var{x} is a
location in \Cw{}.\footnote{
\cite{Wich2005} shows the importance of new notation
for investigators of Earley's algorithm.
Our use of the location marker notation
was inspired by him.}
In its most general form,
a derivation step with a single location marker is
$$ \Vstr{pre} \Vmkm{x} \Vstr{post}. $$
It means
\begin{equation}
\label{eq:location-marker-def}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vstr{pre} \cat \Vstr{post} \cat \Vstr{after} \\
&  \land \quad \Vstr{before} \cat \Vstr{pre} \destar \var{w}[0, (\Vdecr{x})] \\
&  \land \quad \Vstr{post} \cat \Vstr{after} \destar \var{w}[\var{x}, (\Vsize{\Cw} \subtract 1)]
\end{split}
\end{equation}

Derivations may have many location markers.
The meaning of a derivation with \var{j} different location markers,
$$ \var{m}[1], \var{m}[2] \ldots \var{m}[\var{j}], $$
is the same as the meaning of the conjunction of a ordered set of \var{j} derivations,
where the \var{i}'th member has all the markers removed except for $\var{m}[\var{i}]$.
For example,
\begin{equation}\label{eq:location-marker-definition-1}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \Vmkm{i} \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \Vmkm{i} \Vstr{predot} \Vmkm{j} \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{equation}
is the equivalent of the logical conjunction of two derivations:
\begin{gather}
\label{eq:location-marker-definition-2}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \,[\var{i}]\, \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \,[\var{i}]\, \Vstr{predot} \Vstr{postdot} \cat \Vstr{after}
\end{split} \\
\intertext{and}
\label{eq:location-marker-definition-3}
\begin{split}
&  \Vsym{accept} \destar \Vstr{before} \cat \Vsym{A} \cat \Vstr{after} \\
&  \qquad \derives \Vstr{before} \cat \Vstr{predot} \,[\var{j}]\, \Vstr{postdot} \cat \Vstr{after}.
\end{split}
\end{gather}
In this example,
\eqref{eq:location-marker-definition-2} and
\eqref{eq:location-marker-definition-3}
imply that
\begin{equation}\label{eq:location-marker-definition-4}
\Vstr{predot} \destar \var{w}[\var{i}, (\var{j} \subtract 1)]
\end{equation}
and therefore
\eqref{eq:location-marker-definition-1}
also implies
\eqref{eq:location-marker-definition-4}.
Derivations with location markers may be
composed in the same way as derivations without them,
as long as the location markers in the combined
derivation are consistent.

In this \doc{},
\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the parser described in
this \doc{}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Cg}$ will be the language accepted by $\alg{Recce}$
when parsing \Cg{}.

\section{Rewriting the grammar}
\label{s:rewrite}

We have already noted
that no rules of \Cg{}
have a zero-length RHS,
and that all symbols must be either nulling or telluric.
These restrictions follow Aycock and Horspool\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
\cite{AH2002} shows how to do this
without loss of generality.

Because Marpa claims to be a practical parser,
it is important to emphasize
that all grammar rewrites in this \doc{}
are done in such a way that the semantics
of the original grammar can be reconstructed
simply and efficiently at evaluation time.
As implemented,
the Marpa parser allows users to associate
semantics with an original grammar
that has none of the restrictions imposed
on grammars in this \doc{}.
The user of a Marpa parser
may specify any context-free grammar,
including one with properly nullable symbols,
empty rules, etc.
The user specifies his semantics in terms
of this original, ``free-form'', grammar.
Marpa implements the rewrites,
and performs evaluation,
in such a way as to keep them invisible to
the user.
From the user's point of view,
the ``free-form'' of his grammar is the
one being used for the parse,
and the one to which
his semantics are applied.

The rewrite currently used by Marpa is an improvement over
that of \cite{AH2002}.
Rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
(A ``proper nullable'' is a nullable symbol which is not nulling.)
This is similar to a rewrite into Chomsky form.

The proper nullable symbol is then cloned into two others:
one nulling and one telluric.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols,
and new rules are created as necessary to ensure that all possible combinations
of nulling and telluric symbols are accounted for.

The rewrite in \cite{AH2002} was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the internal grammar could be
an exponential function of numbers in the external grammar.
In our version, the worst case growth in the number of rules in linear.

This rewrite can be undone easily.
In fact,
in the current implementation of Marpa,
the reverse rewrite,
from internal to external,
is often done ``on the fly'',
as the parse proceeds.
Marpa allows the user to use Marpa's left-eideticism
to define events.
Events can trigger when rules are complete,
when rules are predicted,
or when symbols are nulled.
The user, of course, needs to define events,
and receive notice of them,
in terms of his external symbols.
In Marpa::R2\cite{Marpa-R2},
this translation back and forth
between internal and external can be accomplished
quickly.

Future plans for Marpa include more aggressive use
of rewrites.
It should be possible, not only to eliminate proper
nullables from the internal grammar,
but also nulling symbols.
We conjecture that elimination of nulling symbols
from the internal grammar will greatly simplify the implementation.
The reader of this paper may note that it would be
simpler and shorter if it did not have to deal with nulling
symbols.

Not all rewrites lend themselves to easy translation
and reversal.
As a future direction, we will look at a general schema
for ``safe'' grammar rewrites.
In this schema, Marpa's internal grammar will have
``brick'' and ``mortar'' symbols.
Internal brick symbols correspond, many-to-one, to
external symbols.
Internal mortar symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.

Assume that we have a parse, using internal symbols.
We define a ``brick traversal'' from a ``root'' brick non-terminal instance.
(A symbol instance is an instance of a symbol in a parse tree,
with is right and left locations specified.)
The ``brick traversal'' is pre-order
and stops traversing any path when it hits
a brick symbol instance other than the ``brick root''.
In this way, it traces a subtree of the parse,
where the root of the subtree is the brick root symbol instance,
and the leaves of the subtree is a sequence of other brick
symbol instances.
The leaves of the subtree, as encountered in pre-order,
constitute its ``brick frontier''.
Mortar symbols will only occur in the interior of this ``brick'' subtree,
never as its root or its leaf.

For a rewrite to be ``safe'':
\begin{itemize}
\item Every brick symbol must translate to exactly one
external symbol
\item Every terminal symbol instance must be a brick symbol,
and the input in terms of internal terminal instances must translate
exactly to external terminal instances.
\item Every brick traveral must translate to an external rule
instance, as follows:
\begin{itemize}
\item The brick root symbol must translate to
the LHS of the rule,
and the brick frontier to the RHS of the rule.
In the case of the frontier-to-RHS translation,
the two sequences must match exactly,
symbol-for-symbol,
preserving the order of the symbols.
\item The brick root symbol instance must translate
exactly to an instance of the LHS of the rule.
\item Each of the brick frontier symbol instances
must translate exactly to an external symbol instance.
\end{itemize}
\end{itemize}

\section{Dotted rules}
\label{s:dotted}

Let $\Vrule{r} \in \Crules$
be a rule,
and $\Vsize{r}$ the length of its RHS.
A dotted rule (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \cat \Vsym{Y} \cat \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \var{X} \cat \var{Y} \mydot \var{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.

Let
\begin{align*}
%
\Postdot{\Vdr{x}} & \defined
\begin{cases}
\begin{aligned}
& \Vsym{B}, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{pre} \mydot \Vsym{B} \cat \Vstr{post}]$} \\
& \Lambda, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{rhs} \mydot]$}
\end{aligned}
\end{cases} \\
%
\Predot{\Vdr{x}} & \defined
\begin{cases}
\begin{aligned}
& \Vsym{B}, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{pre} \cat \Vsym{B} \mydot \Vstr{post}]$} \\
& \Lambda, \; && \text{if $\Vdr{x} = [\Vsym{A} \de \mydot \Vstr{rhs} ]$}
\end{aligned}
\end{cases} \\
%
\Next{\Vdr{x}} & \defined
\begin{cases}
[\Vsym{A} \de \Vstr{pre} \cat \Vsym{B} \mydot \Vstr{post}],  \\
\begin{aligned}
& \qquad && \text{if $\Vdr{X} =
[\Vsym{A} \de \Vstr{pre} \mydot \Vsym{B} \cat \Vstr{post}]$} \\
& \Lambda, && \text{if $\Vdr{x} = [\Vsym{A} \de \Vstr{rhs} \mydot]$}
\end{aligned}
\end{cases} \\
%
\Prev{\Vdr{x}} & \defined
\begin{cases}
[\Vsym{A} \de \Vstr{pre} \mydot \Vsym{B} \cat \Vstr{post}],  \\
\begin{aligned}
& \qquad && \text{if $\Vdr{X} =
[\Vsym{A} \de \Vstr{pre} \cat \Vsym{B} \mydot \Vstr{post}]$} \\
& \Lambda, && \text{if $\Vdr{x} = [\Vsym{A} \de \mydot \Vstr{rhs} ]$}
\end{aligned}
\end{cases} \\
%
\end{align*}

The \dfn{start dotted rule} is
\begin{equation}
\label{eq:start-rule-def}
\Vdr{start} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation}
The \dfn{accept dotted rule} is
\begin{equation*}
\label{eq:accept-rule-def}
\Vdr{accept} = [\Vsym{accept} \de \Vsym{start} \mydot ].
\end{equation*}

We divide all dotted rules into five disjoint types:
start, prediction, null-scan, read and reduction.

\begin{baredefinition}
\label{def:start-dr}
If an dotted rule does not have a predot symbol and it is the start dotted rule
its type is \dfn{start} dotted rule as defined above \eqref{eq:start-rule-def}.
\end{baredefinition}

\begin{baredefinition}
\label{def:prediction-dr}
If it does not have a predot symbol and it is not the start dotted rule,
it is a \dfn{prediction} dotted rule.
\end{baredefinition}

\begin{baredefinition}
\label{def:null-scan-dr}
If it does have a predot symbol and that symbol is a nulling terminal,
it is an \dfn{null-scan} dotted rule.
\end{baredefinition}

\begin{baredefinition}
\label{def:read-dr}
If it does have a predot symbol and that symbol is a telluric terminal,
it is an \dfn{read} dotted rule.
Note that in \Marpa{} all terminals are either nulling or telluric.
\end{baredefinition}

\begin{definition}
\label{def:reduction-dr}
If it does have a predot symbol and that symbol is a non-terminal
it is an \dfn{reduction} dotted rule.
\end{definition}

Prediction and null-scan dotted rules are called \dfn{ethereal} dotted rules.
All other dotted rules are called \dfn{telluric} dotted rules.

A \dfn{predicted dotted rule}
always has a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{alpha} ].
\end{equation*}
A \dfn{confirmed dotted rule}
is a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule} is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{alpha} \mydot ].
\end{equation*}
Predicted, confirmed and completed dotted rules
are also called, respectively,
\dfn{predictions}, \dfn{confirmations} and \dfn{completions}.

A dotted rule which has only nulling symbols after the dot
is \dfn{quasi-complete}.
A quasi-complete dotted rule is called an
\dfn{quasi-completion}.
If a dotted rule is not quasi-complete,
it is said to be \dfn{quasi-incomplete},
or a \dfn{quasi-incompletion}.
These definitions may be vacuously true:
all completions are quasi-completions.

If in a pair of dotted rules,
\begin{equation*}
\begin{split}
& \Vdr{quasi} = [\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} ] \\
& \Vdr{completion} = [\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot ]
\end{split}
\end{equation*}
where \Vstr{nulls} is a string composed entirely of nullable symbols
we say that \Vdr{completion} is the \dfn{completion dotted rule}
of the quasi-completion \Vdr{quasi}.

A dotted rule which has only nulling symbols before the dot
is a \dfn{quasi-prediction}.
This definition may be vacuously true:
all predictions are quasi-predictions.
If a dotted rule is not a
quasi-prediction,
then it is a \dfn{quasi-confirmed}.

\begin{theorem}
\label{t:quasi-drs-disjoint}
In Marpa grammars,
no quasi-completion is a quasi-prediction.
\end{theorem}

\begin{proof}
The rewrite of
Marpa grammars
eliminates all nullable rules.
So every rule must have a telluric symbol.
In a dotted rule, therefore,
there must be
at least one telluric symbol
and it must come either before the dot
or after it.
If a telluric symbol comes before the dot,
the dotted rule might be a quasi-completion,
but it cannot be a quasi-prediction.
If a telluric symbol comes after the dot,
the dotted rule might be a quasi-prediction,
but it cannot be a quasi-completion.
\end{proof}

We also divide dotted rules into two disjoint classes
based on their postdot symbol.
A dotted rule with a null postdot symbol is called \dfn{fleeting}.
Any other dotted rule is called \dfn{lasting}.
A completion is always a lasting dotted rule.

Every dotted rule has a lasting equivalent.
If the dotted rule is lasting, it is its own \dfn{lasting equvialent}.
If the dotted rule is fleeting and quasi-incomplete,
so that without loss of generality it is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} \cat \Vsym{B} \cat \Vstr{after} ]
\end{equation}
where $\Vstr{nulls} \destar \epsilon$,
and $\Vsym{B}$ is a telluric symbol,
then its \dfn{lasting equivalent} is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot \Vsym{B} \cat \Vstr{after} ].
\end{equation}
If the dotted rule is fleeting and quasi-complete,
so that without loss of generality it is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \mydot \Vstr{nulls} ]
\end{equation}
where $\Vstr{nulls} \destar \epsilon$,
then its \dfn{lasting equivalent} is
\begin{equation}
[\Vsym{A} \de \Vstr{alpha} \cat \Vstr{nulls} \mydot ].
\end{equation}

\subsection{The dotted rule transition function}

We define
a partial transition function from
pairs of dotted rule and symbol
to sets of dotted rules.
\begin{equation*}
\GOTO: \Cdr, (\epsilon \cup \var{vocab}) \mapsto 2^\Cdr.
\end{equation*}
$\GOTO(\Vdr{from}, \epsilon)$ is a
\dfn{null transition}
and its result is a \dfn{null transition set}.
``null'' is an overloaded term,
so we more often call the null transition
an \dfn{ethereal transition}
and the null transition set
an \dfn{ethereal transition set}.
If a transition is not an ethereral transition,
it is a \dfn{telluric transition},
and if a transition set
is not an ethereal transition set,
it is a \dfn{telluric transition set}.

A telluric transition set is always the empty set
or a singleton set.
Only ethereal transition sets have
with cardinalities greater than one.
The dotted rules in the set that results from an ethereal transition
will be either predictions or confirmed rules with
a nulling predot symbol.

Where the transition is over a symbol,
call it \Vsym{A},
the behavior of \GOTO{} is straightforward:
\begin{equation*}
\GOTO(\Vdr{from}, \Vsym{A}) =
\begin{cases}
\begin{aligned}
& \rlap{$\left\lbrace \Next{\Vdr{from}} \right\rbrace,$} && \\
& && \text{if $\Vsym{A} = \Postdot{\Vdr{from}}$,} \\
& \emptyset, && \text{if $\Vsym{A} \ne \Postdot{\Vdr{from}}$},
\end{aligned}
\end{cases}
\end{equation*}

Ethereal transitions are more complicated.
The result of \GOTO{} is always the empty set
if there is
no postdot symbol:
\begin{equation*}
\Postdot{\Vdr{from}} = \Lambda \implies \GOTO(\Vdr{from}, \Vsym{A}) = \emptyset.
\end{equation*}
To find $\GOTO(\Vdr{from}, \epsilon)$ where
$\Postdot{\Vdr{from}} \neq \Lambda$,
we need to make some definitions,
many of which will be useful later.

Let \var{null-scan-one} be the set of
pairs of dotted rules
\begin{gather*}
\left\lbrace \Vdr{cause}, \Vdr{effect} \right\rbrace \quad \text{such that} \\
\Vdr{cause} = [ \Vsym{A} \de \Vstr{before-B} \mydot \Vsym{B} \cat \Vstr{after-B} ] \quad \text{and} \\
\Vdr{effect} = [ \Vsym{A} \de \Vstr{before-B} \cat \Vsym{B} \mydot \Vstr{after-B} ] \quad \text{and} \\
\Vsym{B} \derives \epsilon.
\end{gather*}
We say that \Vdr{cause} is the \dfn{top-down cause} of \Vdr{effect},
and that \Vsym{B} is the \dfn{bottom-up cause}.

We can use \var{null-scan-one} to define an equivalence relation.
Intuitively, two dotted rules, \Vdr{dr1}
and \Vdr{dr2} are \dfn{ethereally equivalent} if
\Vdr{dr1} can be changed into \Vdr{dr2} by a series of null-scans.
More formally, we define \var{eth-eq} to be the reflexive, symmetric
and transitive closure of
\var{null-scan-one}.
We say that \Vdr{dr1} is
\dfn{ethereally equivalent} to \Vdr{dr2} if
and only if
\Vdr{dr1} is an element of the
equivalence class of \var{eth-eq} under \Vdr{dr2}.

Let \var{predict-one} be the set of
pairs of dotted rules
\begin{gather*}
\quad \left\lbrace \Vdr{cause}, \Vdr{effect} \right\rbrace \,\, \text{such that} \\
 \quad \Vdr{cause} = [ \Vsym{A} \de \Vstr{before-B} \mydot \Vsym{B} \cat \Vstr{after-B} ] \quad \text{and} \\
 \quad \Vdr{effect} = [ \Vsym{B} \de \mydot \Vsym{C} \cat \Vstr{after-C} ]
\end{gather*}
We say that \Vdr{cause} is the \dfn{top-down cause} of \Vdr{effect}.
For symmetry, we say that \Vdr{cause}
has a bottom-up cause,
but that the \dfn{bottom-up cause} is ethereal.

The
\dfn{ethereal closure}
is the closure of union
\var{null-scan-one} and \var{predict-one}.
$$\var{ethereal-closure} \defined (\var{null-scan-one} \cup \var{predict-one})^\ast.$$

We are now in a position to define the ethereal transition of \GOTO{}
from the dotted rule \Vdr{base},
when
\Vdr{base} has a postdot symbol.
It is the ethereal closure
over the singleton set containing the dotted rule argument of \GOTO{}.
\begin{multline}
\label{eq:def-ethereal-closure}
\Vsym{A} = \Postdot{\Vdr{base}} \land \Vsym{A} \neq \Lambda \implies \\
\GOTO(\Vdr{base}, \Vsym{A}) =  \var{ethereal-closure}(\lbrace \Vdr{base} \rbrace)
\end{multline}
We say that
the ethereal closure for a dotted rule is the ethereal closure of the singleton
set containing that dotted rule:
\begin{multline*}
\var{ethereal-closure}(\Vdr{base}) \defined  \var{ethereal-closure}(\lbrace \Vdr{base} \rbrace)
\end{multline*}

Let
\begin{equation}
\Vdrset{ec} = \var{ethereal-closure}(\lbrace \Vdr{base} \rbrace).
\end{equation}
We also call \Vdrset{ec} an \dfn{ethereal closure}
and we say that \Vdr{base} is its \dfn{base}.
If \Vdr{base} is telluric, we say that
\Vdr{base} is a \dfn{telluric base}.
We call \Vdr{tell} a telluric base
of a dotted rule \Vdr{dr2} if and only if
$$ \Vdr{dr2} \in \var{ethereal-closure}(\lbrace \Vdr{tell} \rbrace). $$

Predicted dotted rules may have more than one telluric base,
but for null-scan dotted rules the telluric base
will be unique.

\begin{theorem}
\label{eth-eq-share-telluric-base}
If the dotted rules \Vdr{dr1}
and \Vdr{dr2} are ethereally equivalent,
and \Vdr{dr1} is quasi-confirmed,
then both \Vdr{dr1}
and \Vdr{dr2} have the same telluric base.
\end{theorem}

\begin{proof}
By assumption for the theorem,
\Vdr{dr1} is quasi-confirmed,
so that
by Theorem \ref{t:quasi-drs-disjoint},
\Vdr{dr1}
is not a quasi-prediction.
Therefore, \Vdr{dr1} has a telluric symbol before
the dot.
Therefore,
\Vdr{dr1} has a telluric base.
Without loss of generality,
we let
\begin{equation}
\Vdr{dr1} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1} \mydot \Vstr{post1} ],
\end{equation}
and let the telluric base be
\begin{equation}
\label{eq:eth-eq-share-telluric-base-20}
\Vdr{tell} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \mydot \Vstr{nulls1} \cat \Vstr{post1} ],
\end{equation}
where $\Vstr{nulls1} \destar \epsilon$.

We now proceed by overlapping cases.
In the first case,
the dot in \Vdr{dr2} comes at or after the dot in \Vdr{dr1}.
We may write
\begin{multline*}
\Vdr{dr2} =
  [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1} \cat \Vstr{nulls2} \mydot \Vstr{post2} ],
\end{multline*}
where $\Vstr{post1} = \Vstr{nulls2} \cat \Vstr{post2},$
so that
by \eqref{eq:eth-eq-share-telluric-base-20}
and the definition of ethereal closure,
$$ \Vdr{dr2} \in \var{ethereal-closure}(\lbrace \Vdr{tell} \rbrace).$$
By the definition of telluric base,
\Vdr{tell} is the telluric base of \Vdr{dr2}.

In the second case
the dot in \Vdr{dr2} comes at or before the dot in \Vdr{dr1}.
We may write
\begin{equation*}
\Vdr{dr2} = [ \Vsym{A} \de \Vstr{pre} \cat \Vsym{tell} \cat \Vstr{nulls1a} \mydot \Vstr{nulls1b} \cat \Vstr{post1} ],
\end{equation*}
where $\Vstr{nulls1} = \Vstr{nulls1a} \cat \Vstr{nulls1b}.$
Again,
by the definition of telluric base,
\Vdr{tell} is the telluric base of \Vdr{dr2}.

We have shown that, in both cases,
\Vdr{tell} is the telluric base of \Vdr{dr2}.
\end{proof}

\begin{theorem}
\label{quasi-confirmed-unique-telluric-base}
If a dotted rule is quasi-confirmed,
its telluric base is unique.
\end{theorem}

\begin{proof}
Let the dotted rule be \Vdr{dr}.
This theorem follows
directly
from Theorem \ref{eth-eq-share-telluric-base},
if you set both of its dotted rules to \Vdr{dr}.
\end{proof}

The complexity of the ethereal closure is of interest:
we may want to compute it on the fly,
and in any case,
we certainly want to show that
the ethereal closure has finite time complexity.
\begin{algorithm}[!htp]
\caption{Add a new generation of dotted rules to the ethereal closure}
\label{alg:ethereal-generation}
\begin{algorithmic}[1]
\Procedure{Ethereal next}{\Vdr{base}, \Vdrset{results}}
\If{\Vdr{base} has no postdot symbol}
\State return
\EndIf
\State Here \Vdr{base} is $[ \Vsym{lhs} \de \Vstr{before} \mydot \Vsym{A} \cat \Vstr{after} ]$
\label{line:ethereal-generation-20}
\State \Comment We can state this without loss of generality
\If{$\Vsym{A}$ is a nulling symbol}
\State $\Vdr{new} \gets [ \Vsym{lhs} \de \Vstr{before} \cat \Vsym{A} \mydot \Vstr{after} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \ldots
\State $\qquad$ but only if it has never been added before
\State return
\EndIf
\label{line:ethereal-generation-40}
\State Here \Vsym{A} must be a telluric symbol
\For{ each \Vrule{r} in \Cg{}}
\If{ $\LHS{\Vrule{r}} = \Vsym{A}$ }
\State Here \Vrule{r} is $[ \Vsym{A} \de \Vstr{rhs} ]$
\State \Comment We can state this without loss of generality
\State $\Vdr{new} \gets [ \Vsym{A} \de \mydot \Vstr{rhs} ]$
\State Add \Vdr{new} to \Vdrset{results} \ldots
\State $\qquad$ but only if it has never been added before
\State Add \Vdr{new} to \Vdrset{work} \dots
\label{line:ethereal-generation-60}
\State $\qquad$ but only if it has never been added before
\EndIf
\EndFor
\State return
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!htp]
\caption{Create ethereal closure}
\label{alg:ethereral-closure}
\begin{algorithmic}[1]
\Function{Create ethereal closure}{\Vdr{base}}
\State $\Vdrset{result} \gets \emptyset$
\State $\Vdrset{work} \gets \emptyset$
\State \Call{Ethereal next}{\Vdr{base}}
\While{$\Vdrset{work} \neq \emptyset$}
\State Remove a dotted rule from \Vdrset{work}, call it \Vdr{work}
\State \Call{Ethereal next}{\Vdr{work}, \Vdrset{result}}
\EndWhile
\State return \Vdrset{result}
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:ethereral-closure}
is not actually used by any
of Marpa's versions --
it is chosen because it is
convenient for exploring the theory.
The current Marpa implementation handles
null-scans and predictions with two completely different
mechanisms.
In the actual implementation,
null-scans are dealt with implicitly,
while predictions are explicitly computed after
each Earley set is otherwise complete.

\begin{theorem}\label{t:ethereal-closure-Oc}
Ethereal closure has time complexity \Oc{}.
\end{theorem}

\begin{proof}
We consider Algorithm \ref{alg:ethereral-closure}.
This clearly runs in \Oc{} time if there is a constant
number of calls to
Algorithm \ref{alg:ethereal-generation}.

To finish the proof, we need to show
that
Algorithm \ref{alg:ethereal-generation}
is called a constant number
of times.
Algorithm \ref{alg:ethereal-generation}
is called
once for the base dotted rule of the computation.
It is called again for every dotted rule added to the working set
of dotted rules, \Vdrset{work}.
We know that no dotted rule is added to
\Vdrset{work} twice.
Therefore
Algorithm \ref{alg:ethereal-generation}
is called
at most once for each dotted rule.
\Cg{} has a fixed number of dotted rules,
so
that Algorithm \ref{alg:ethereal-generation}
is called
a most \Oc{} times.
\end{proof}

\begin{theorem}\label{t:ethereal-closure-dr-correct}
Algorithm \ref{alg:ethereral-closure} is correct.
\end{theorem}

\begin{proof}
From examining
Algorithm \ref{alg:ethereral-closure},
in particular 
lines
\ref{line:ethereal-generation-20}-\ref{line:ethereal-generation-40}
of
Algorithm \ref{alg:ethereal-generation},
we see that
\begin{equation}
\label{ethereal-closure-correct-2}
\myparbox{
the null transitions
for nulling postdot symbols are complete and consistent,
and therefore correct.
}
\end{equation}

From examining
Algorithm \ref{alg:ethereral-closure},
in particular 
lines
\ref{line:ethereal-generation-20}-\ref{line:ethereal-generation-40}
of
Algorithm \ref{alg:ethereal-generation},
we see that the null transitions for predictions are
properly made,
so that
\begin{equation}
\label{ethereal-closure-correct-3}
\myparbox{
the set of predictions is consistent.
}
\end{equation}

It remains to show that the set of predictions is complete.
Algorithm \ref{alg:ethereral-closure}
clearly adds all predictions derivable in a single step
to its results.
It also
calls the ``Ethereal next'' function
repeatedly, so that indirect predictions will be added.
But it will refuse to
add a dotted rule to its working set more than once.
We need to consider whether this means some predictions
will not be derived.

Consider a prediction
\begin{equation}
\label{ethereal-closure-correct-5}
\Vsym{lhs-pred} \de \mydot \Vsym{pred-rhs}
\end{equation}
which is derived through a series of dotted rule predictions
added to the work list at line
\ref{line:ethereal-generation-60}
of Algorithm \ref{alg:ethereal-generation}.
For a reductio,
assume that one prediction,
call it
\begin{equation}
\label{ethereal-closure-correct-15}
[ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ],
\end{equation}
occurs twice.
Without loss of generality, let that chain be
\begin{align*}
& [ \Vsym{lhs0} \de \mydot \Vsym{rhs0} ] && \text{Step 0} \\
& [ \Vsym{lhs1} \de \mydot \Vsym{rhs1} ] && \text{Step 1} \\
& \ldots && \\ 
& [ \Vsym{lhs-predup} \de \mydot \Vsym{lhs-dup} \cat \Vstr{after-predup} ] && \text{Step \Vdecr{i}} \\
& [ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ] && \text{Step \var{i}} \\
& \ldots && \\
& [ \Vsym{lhs-predup2} \de \mydot \Vsym{lhs-dup} \cat \Vstr{after-predup2} ] && \text{Step \Vdecr{j}} \\
& [ \Vsym{lhs-dup} \de \mydot \Vsym{rhs-dup} ] && \text{Step \var{j}}\\
& \ldots && \\
& [ \Vsym{lhs-penult} \de \mydot \Vsym{rhs-penult} ] && \\
& [ \Vsym{lhs-last} \de \mydot \Vsym{rhs-last} ]
\end{align*}
where Step \var{i} is the first occurrence of 
\eqref{ethereal-closure-correct-15},
and Step \var{j} is the last.
We can create a shorter chain of predictions by removing the steps in
the chain from Step $\var{i}+1$ to Step \var{j}.
Call this process of removing steps, ``pruning duplicates''.

By pruning duplicates for every prediction which occurs twice in
the chain,
we see that we can create a chain that results in
\eqref{ethereal-closure-correct-5},
but which does not contain any prediction more than once.
We can also see that, since
Algorithm \ref{alg:ethereral-closure} follows all chains
that contain no duplicate steps,
that 
Algorithm \ref{alg:ethereral-closure} will add
\eqref{ethereal-closure-correct-5} to its result.

Since
\eqref{ethereal-closure-correct-5} was chosen without loss
of generality,
we see that every prediction can be reached by following
a chain of predictions with no duplicate steps,
and that therefore
\begin{equation}
\label{ethereal-closure-correct-40}
\myparbox{
Algorithm \ref{alg:ethereral-closure} adds a complete
set of predictions.
}
\end{equation}

We now summarize our results.
By definition, the ethereal closure is the
transitive closure of the union
of predictions and null-scans.
In \eqref{ethereal-closure-correct-2}
we showed that the sets of
null-scans added is correct.
In \eqref{ethereal-closure-correct-3}
that the set of predictions added is consistent.
In
\eqref{ethereal-closure-correct-40}
we showed
that the set of predictions
added is complete.
This shows the theorem.
\end{proof}

\section{Earley items}
\label{s:earley-items}

An Earley item (type \type{EIM}) is a triple
\[
    [\Vdr{dotted-rule}, \Vorig{x}, \Vloc{current} ]
\]
of dotted rule, origin, and current location.

The \dfn{origin} is the location where recognition of the rule
started.
(It is sometimes called the ``parent''.)
The \dfn{current} or \dfn{dot location} is the location
in the input, \Cw{}, of the dot position in \Vdr{dotted-rule}.
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.  Where
\begin{gather*}
    \Veim{x} = [\Vdr{x}, \Vorig{x}, \Vloc{x} ] \\
\text{we say that $\DR{\Veim{x}} = \Vdr{x}$,} \\
\Origin{\Veim{x}} = \Vorig{x} \\
\text{and $\Current{\Veim{x}} = \Vloc{x}$.}
\end{gather*}

Traditionally, an Earley item is shown as a duple,
\[
    [\Vdr{dotted-rule}, \Vorig{x} ]
\]
with \Vloc{current} ommitted.
When the duple form is used,
the current location is specified by the context,
either explicitly or implicitly.
This paper will use Earley items
in both the duple and triple forms.

Whenever a dotted rule notion is applied to an EIM,
it refers to the dotted rule of the EIM.
For example,
a completion EIM is an EIM with a completion
dotted rule,
and a predicted EIM is an EIM with a predicted dotted rule.
If
$$\Veim{quasi} = [ \Vdr{quasi}, \var{i}, \var{j} ]$$
is a quasi-complete EIM,
then its lasting completion EIM is
$$[ \Vdr{complete}, \var{i}, \var{j} ],$$
where \Vdr{complete} is the completion dotted
rule of \Vdr{quasi}.

The \dfn{start EIM} is
\begin{equation}
\label{eq:start-eim-def}
\Veim{start} = [ [\Vsym{accept} \de \mydot \Vsym{start} ], 0, 0 ].
\end{equation}
The \dfn{accept EIM} is
\begin{equation}
\label{eq:accept-eim-def}
\Vdr{accept} = [ [\Vsym{accept} \de \Vsym{start} \mydot ], 0, \Vsize{\Cw} ].
\end{equation}

\begin{theorem}
\label{eim-types-correct}
Every EIM falls into one of these
five disjoint types:
start, prediction, read, null-scan and reduction.
\end{theorem}

\begin{proof}
Recall that EIMs take their type from their dotted rule.
The proof then follows directly from definitions
\ref{def:start-dr},
\ref{def:prediction-dr},
\ref{def:null-scan-dr},
\ref{def:read-dr}
and \ref{def:reduction-dr}
above.
\end{proof}

Following the convention for dotted rules,
EIMs are telluric if they are the start EIM,
a read EIM or a reduction EIM.
An EIM is ethereal if it is not telluric.
The idea is that telluric dotted rules are ``grounded'' either
in the input or in the initial state of the parse,
while
ethereal dotted rules emerge out of an invisible realm.

\begin{definition}
\label{def:eim-valid}
We say that
an Earley item
\begin{equation*}
\bigl[[\Vsym{A} \de \Vstr{predot} \mydot \Vstr{postdot}], \var{i}, \var{j} \bigr]
\end{equation*}
is \dfn{valid}
if and only if
\begin{equation}
\label{eq:eim-valid-def}
\Vsym{A} \derives [\var{i}]\, \Vstr{predot} \,[\var{j}]\, \Vstr{postdot}.
\end{equation}
If \Veim{x} is valid, we also say \Valid{\Veim{x}}.
\end{definition}

\begin{theorem}\label{t:start-eim-is-valid}
The start Earley item is valid.
\end{theorem}

\begin{proof}
By the definition of EIM validity,
to show that the start EIM
\eqref{eq:start-eim-def}
is valid,
we need to show
that
\begin{equation}
\label{eq:start-eim-is-valid-15}
\Vsym{accept} \derives \mk{0}\; \Vsym{start} \\
\end{equation}
By the definition of \Vsym{accept},
it is on the LHS of only one rule,
\eqref{eq:accept-rule-def}.
All symbols in \Cg{} are productive,
so that
\begin{equation}
\label{eq:start-eim-is-valid-16}
\Vsym{accept} \derives \Vsym{start} \destar \Vstr{sent}
\end{equation}
where \Vstr{sent} is a sentence.
Since \Vstr{sent} is a sentence,
using \eqref{eq:def-L-g},
we have that
\begin{equation*}
\Vstr{sent} \in \myL{\Cg}.
\end{equation*}
Our convention is that \Cw{}
is the input, or sentence, of interest in
context, so here we assume that,
without loss of generalization,
\begin{equation}
\label{eq:start-eim-is-valid-18}
\Vstr{sent} = \Cw = \Cw[0, \Vdecr{\Vsize{\Cw}}].
\end{equation}
From
\eqref{eq:start-eim-is-valid-16}
and
\eqref{eq:start-eim-is-valid-18},
we have
\begin{align}
& \Vsym{accept} \derives \Vstr{start} \destar \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\therefore \quad & \Vsym{accept} \derives \Vstr{start} \destar \mk{0} \; \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\therefore \quad & \Vsym{accept} \derives \mk{0} \Vstr{start} \destar \mk{0} \; \Cw[0, \Vdecr{\Vsize{\Cw}}] \notag\\
\label{eq:start-eim-is-valid-25}
\therefore \quad & \Vsym{accept} \derives \mk{0} \Vstr{start}
\end{align}
Where 
\eqref{eq:start-eim-is-valid-25}
is
\eqref{eq:start-eim-is-valid-15},
which is what we needed to show for the theorem.
\end{proof}

\subsection{Parse instances}

A \dfn{parse instance} is either a symbol instance or an EIM.
Intuitively, a \dfn{symbol instance} is
a symbol in the context of a parse.
More formally, a symbol instance is a triple whose elements
are a left location, a symbol name and a right location.
We often represent symbol instances in the form
$$\Vmkl{j} \Vsym{up} \Vmkr{k}.$$

When it is clear in context, we will often say ``symbol''
when we mean ``symbol instance''.
A parse instance is often called simply an \dfn{instance}.

\subsection{Top-down and bottom-up causes}

We referred to top-down and bottom-up causes earlier,
when introducing dotted rules.
We now revisit these concepts in the context of Earley items.

\begin{sloppypar}
We write \Left{\Vinst{inst}} for the left location of \Vinst{inst}
and \Right{\Vinst{inst}} for the right location of \Vinst{inst}.
If \var{inst} is an Earley item,
call it \Veim{inst}, then
\begin{gather*}
\Left{\Veim{inst}} = \Origin{\Veim{inst}} \\
\text{and} \quad \Right{\Veim{inst}} = \Current{\Veim{inst}}.
\end{gather*}
If \var{inst} is a symbol instance,
call it,
without loss of generality,
\begin{gather*}
\Vinst{inst} = \Vmk{i} \Vsym{A} \Vmk{j}, \\
\text{then} \quad \Left{\Veim{inst}} = \Vloc{i} \\
\text{and} \quad \Right{\Veim{inst}} = \Vloc{j}.
\end{gather*}

\end{sloppypar}

\begin{definition}
\label{def:causes}
We first consider an Earley item
with a predot symbol instance.
Without loss of generality,
let the Earley item be
\begin{equation*}
\Veim{effect} = [ [ \Vsym{A} \de \Vstr{prefix} \cat \Vsym{up} \mydot \Vstr{suffix} ], \var{i}, \var{k} ].
\end{equation*}
and let the instance be
$$\Vmkl{j} \Vsym{up} \Vmkr{k}.$$
Then we say that
\begin{equation*}
\Veim{down} = [ [ \Vsym{A} \de \Vstr{prefix} \mydot \Vsym{up} \cat \Vstr{suffix} ], \var{i}, \var{j} ]
\end{equation*}
is a \dfn{top-down cause} of \Veim{effect};
we say that \Vsym{up} is a \dfn{bottom-up cause} of \Veim{effect};
and we say that \Veim{effect} is the \dfn{effect} of \Veim{down} and \Vsym{up}.

We now consider an Earley item
with no predot symbol instance.
Without loss of generality,
call it \Veim{effect},
and let
\begin{gather*}
\Vdr{effect} = [ \Vsym{A} \de \mydot \Vstr{rhs} ], \\
\Veim{effect} = [ \Vdr{effect}, \var{k}, \var{k} ] \quad \text{and} \\
\Veim{down} = [ \Vdr{down}, \var{i}, \var{k} ]
\end{gather*}
where \Vdr{down}
is any top-down dotted rule cause of \Vdr{effect}.
Then we call \Veim{down}
a \dfn{top-down cause} of \Veim{effect};
and we say that the \dfn{bottom-up cause} of \Veim{effect} is ethereal.
We call \Veim{effect} the \dfn{effect} of \Veim{down}.

\end{definition}

An effect is always an Earley item.
A top-down cause is always an Earley item.

A bottom-up cause may be ethereal or telluric.
If a bottom-up cause is ethereal, it may be because the effect has no
predot symbol,
or because the bottom-up cause is a nulling terminal symbol instance.
If a bottom-up cause is telluric, it may be because it is
an Earley item,
because it is a non-terminal symbol instance,
or because it is a telluric terminal symbol instance.
Recall that rules in \Marpa{} cannot be
nulling,
and therefore Earley items
and non-terminal symbol instances cannot be nulling.

We have already defined validity for Earley items.
We now extend the concept of validity to other types of
causes.
An ethereal cause is always valid.
If \Vsym{T} is a telluric terminal symbol,
\begin{equation*}
\Valid{[\var{i}]\, \Vsym{T} \,[\var{j}]} \quad \defined \quad
\Vsym{A} = \CVw{i} \; \land \; \var{j} = \var{i} + 1.
\end{equation*}
If \Vsym{N} is a non-terminal symbol,
\begin{equation*}
\Valid{[\var{i}]\, \Vsym{N} \,[\var{j}]}
\quad \defined \quad
\Vsym{N} \deplus \var{w}[\var{i}, (\var{j} \subtract 1)].
\end{equation*}

\begin{theorem}
\label{t:valid-eim-from-symbol}
Let the non-terminal symbol instance
\begin{equation}
\label{eq:valid-eim-from-symbol-3}
\,[\var{i}]\, \Vsym{up} \,[\var{j}]
\end{equation}
be valid.
Then \Veim{up}
is a valid completed Earley item,
where
\begin{gather}
\label{eq:valid-eim-from-symbol-6}
\Veim{up} = [ [ \Vsym{up} \de \Vstr{rhs} \mydot ], \var{i}, \var{j} ]
\\
\label{eq:valid-eim-from-symbol-9}
\land \quad [ \Vsym{up} \de \Vstr{rhs} ] \in \Crules.
\end{gather}
\end{theorem}

\begin{proof}
To be valid, \Vsym{up}
must derive the sentence
$\var{w}[\var{i}, (\var{j}\subtract 1)]$.
\Vsym{up} must do this using at least one derivation step,
since it is a non-terminal.
Therefore \Vsym{up} derives a sentence through a derivation
whose next step uses a rule in \Crules,
where \Vsym{up} is the LHS.
This shows \eqref{eq:valid-eim-from-symbol-9}.
It remain to show
\eqref{eq:valid-eim-from-symbol-6}.

Using
\eqref{eq:valid-eim-from-symbol-3}
and \eqref{eq:valid-eim-from-symbol-9},
we have
\begin{alignat*}{3}
& [\var{i}]\, \Vsym{up} \,[\var{j}] && \derives \Vstr{rhs} \\
\therefore \quad & [\var{i}]\, \Vsym{up} \,[\var{j}] && \derives
[ \var{i}]\, \Vstr{rhs} \,[\var{j}] \\
\therefore \quad & \Vsym{up} && \derives
[\var{i}]\, \Vstr{rhs} \,[\var{j}]
\end{alignat*}
If,
in the definition of EIM validity
\eqref{eq:eim-valid-def},
we set the predot string to
\Vstr{rhs},
the postdot string to $\epsilon$,
and the LHS to \Vsym{up},
we see that \Veim{up} in
\eqref{eq:valid-eim-from-symbol-6} is valid.
\end{proof}

For every bottom-up cause that is a symbol
or an EIM,
its ``symbolic equivalent'' is defined.
We write the symbolic equivalent of \Vinst{x}
as \Symbol{\var{x}}.
If the bottom-up cause is a symbol instance,
it is its own \dfn{symbolic equivalent}.
If the bottom-up cause is an EIM,
its symbolic equivalent is the instance
of its LHS symbol
with the same left and right locations
as the EIM.
That is, letting the bottom-up cause EIM be,
without loss of generality,
\begin{equation}
\label{eq:def-symbolic-equivalent-10}
\Veim{up} = [ [ \Vsym{up} \de \Vstr{up-rhs} \mydot ], \var{i}, \var{j} ]
\end{equation}
then
\begin{equation}
\label{eq:def-symbolic-equivalent-20}
\Symbol{\Veim{up}} = \Vmkl{i} \Vsym{up} \Vmkr{j}.
\end{equation}
We say that \Veim{up}
is the \dfn{EIM equivalent} of
\var{inst}.

\begin{theorem}
\label{t:valid-symbolic-equivalent-valid}
If a bottom-up EIM cause is valid, its symbolic equivalent is valid.
\end{theorem}

\begin{proof}
A bottom-up cause must be a completion.
We may therefore,
without loss of generality,
let the bottom-up EIM be
\eqref{eq:def-symbolic-equivalent-10}
and its symbolic equivalent is therefore
\eqref{eq:def-symbolic-equivalent-20}.
\eqref{eq:def-symbolic-equivalent-10}
is valid by assumption for the theorem,
so we have
\begin{alignat}{3}
& \Vsym{up} && \derives
[ \var{i}]\, \Vstr{up-rhs} \,[\var{j}] \\
\therefore \quad
& [\var{i}]\, \Vsym{up} \,[\var{j}] && \derives
[\var{i}]\, \Vstr{up-rhs} \,[\var{j}] \\
\label{eq:valid-eim-from-symbol-20}
\therefore \quad
& [\var{i}]\, \Vsym{up} \,[\var{j}] &&
\end{alignat}
\eqref{eq:valid-eim-from-symbol-20} is
\eqref{eq:def-symbolic-equivalent-10},
which shows the theorem.
\end{proof}

We say that a bottom-up cause is \dfn{symbolic},
if it has a symbolic equivalent.
The bottom-up causes of predictions
do not have a symbolic equivalent,
and are not therefore not symbolic.
All other bottom-up causes are symbolic.

\begin{definition}
\label{def:matching-causes}
Let \Veim{down},
be a top-down cause and
let \Vinst{up}
be a symbolic bottom-up cause.
We say that \Veim{down} and \Vinst{up}
are
\dfn{matching causes}
if and only if
\begin{gather*}
\Symbol{\Veim{down}} = \Symbol{\Vinst{up}}, \\
\Left{\Veim{down}} = \Left{\Vinst{up}} \quad \text{and} \\
\Right{\Veim{down}} = \Right{\Vinst{up}}.
\end{gather*}
When \Veim{down} and \Vinst{up} are matching causes,
we also say that
\Veim{down} and \Vinst{up} \dfn{match}.
\end{definition}

\begin{theorem}
\label{t:effect-from-symbolic-causes}
If a symbolic bottom-up cause
and a top-down cause match
and are both valid,
their effect is valid.
\end{theorem}

\begin{proof}
Without loss of generality,
let the top-down cause be
\begin{equation}
\label{eq:effect-from-symbolic-causes-3}
\begin{split}
& \Veim{down} =  \\
& \qquad \qquad [ [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j} ].
\end{split}
\end{equation}
and let the symbolic equivalent of the bottom-up cause be
\begin{equation}
\label{eq:effect-from-symbolic-causes-6}
\,[\var{j}]\, \Vsym{A} \,[\var{k}]\, .
\end{equation}

To show the theorem, we must show that
\begin{equation}
\label{eq:effect-from-symbolic-causes-9}
\begin{split}
& \Veim{effect} =  \\
& \qquad \qquad [ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ].
\end{split}
\end{equation}
is valid.
By the definition of validity for an Earley item,
we will have shown this if we can show that
\begin{equation}
\label{eq:effect-from-symbolic-causes-12}
\Vsym{down} \derives [\var{i}] \Vstr{pre} \cat \Vstr{A} [\var{k}] \Vstr{post}.
\end{equation}

By assumption,
\eqref{eq:effect-from-symbolic-causes-3}
is valid.
From the definition of validity for an Earley item:
\begin{equation}
\label{eq:effect-from-symbolic-causes-20}
\Vsym{down} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \cat \Vstr{post}
\end{equation}

We see that the location markers in
\eqref{eq:effect-from-symbolic-causes-6}
and
\eqref{eq:effect-from-symbolic-causes-20}
are compatible:
\var{i} and \var{k} are unrestricted,
while the use of \var{j} in both derivations is compatible.
Composing them we have
\begin{alignat}{2}
\label{eq:effect-from-symbolic-causes-25}
& \Vsym{down} \derives [\var{i}]\, \Vstr{pre} \,[\var{j}]\, \Vsym{A} \,[\var{k}]\, \Vstr{post} \\
\therefore \quad & \Vsym{down} \derives [\var{i}]\, \Vstr{pre} \cat \Vsym{A} \,[\var{k}]\, \Vstr{post}
\label{eq:effect-from-symbolic-causes-28}
\end{alignat}
Where
\eqref{eq:effect-from-symbolic-causes-28}
is
\eqref{eq:effect-from-symbolic-causes-12},
which is what we needed to show for the theorem.
\end{proof}

\begin{theorem}
\label{t:symbolic-causes-from-effect}
If
$$\Veim{effect} = [ \Vdr{effect}, \var{i}, \var{k} ],$$
is a valid, confirmed EIM,
then
\begin{enumerate}
\RaggedRight
\item \Veim{effect} must have a valid symbolic bottom-up cause,
call it $\Vinst{up}$,
\label{item:symbolic-causes-from-effect-3}
\item \Veim{effect} must have a valid top-down cause,
call it $\Veim{down}$,
\label{item:symbolic-causes-from-effect-6}
\item \Vinst{up} and \Veim{down} must be matching causes,
\label{item:symbolic-causes-from-effect-9}
\item $\Right{\Vinst{up}}  = \Vloc{k}$,
\label{item:symbolic-causes-from-effect-12}
\item $\Left{\Veim{down}} = \Vloc{i}$,
\label{item:symbolic-causes-from-effect-15}
\item
the cause symbol of \Vinst{up} must be \Predot{\Veim{effect}},
\label{item:symbolic-causes-from-effect-18}
\item $\Predot{\Veim{effect}} = \Postdot{\Veim{down}}$, and
\label{item:symbolic-causes-from-effect-19}
\item $\Next{\Vdr{down}} = \Vdr{effect}$
\label{item:symbolic-causes-from-effect-21}
\end{enumerate}
\end{theorem}

\begin{proof}
Without loss of generality,
let \Veim{effect} be
\begin{equation}
\label{eq:symbolic-causes-from-effect-20}
\begin{split}
& \Veim{effect} =  \\
& \qquad \qquad [ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ].
\end{split}
\end{equation}
By assumption,
\eqref{eq:symbolic-causes-from-effect-20}
is valid, so that
\begin{equation}
\label{eq:symbolic-causes-from-effect-23}
\Vsym{down} \derives \Vmkm{i} \Vstr{pre} \cat \Vstr{A} \Vmkm{k} \Vstr{post}.
\end{equation}
Using
\eqref{eq:symbolic-causes-from-effect-23},
and the definition of location markers,
we have
\begin{equation}
\label{eq:symbolic-causes-from-effect-29}
\Vstr{pre} \cat \Vstr{A} \destar \var{w}[\var{i}, (\var{k} \subtract 1)],
\end{equation}
so that there must be some \Vloc{j}, $\Vloc{i} \le \Vloc{j} \le \Vloc{k}$,
such that
\begin{equation}
\label{eq:symbolic-causes-from-effect-32}
\Vsym{down} \derives \Vmkl{i} \Vstr{pre} \Vmkm{j} \Vstr{A} \Vmkm{k} \Vstr{post}.
\end{equation}
Simplifying
\eqref{eq:symbolic-causes-from-effect-32},
we have
\begin{equation}
\label{eq:symbolic-causes-from-effect-35}
\Vmkl{j} \Vstr{A} \Vmkr{k}
\end{equation}
A different simplification of
\eqref{eq:symbolic-causes-from-effect-32}
produces
\begin{equation}
\label{eq:symbolic-causes-from-effect-38}
\Vsym{down} \derives \Vmkl{i} \Vstr{pre} \Vmkm{j} \Vstr{A} \cat \Vstr{post}.
\end{equation}
and
from
\eqref{eq:symbolic-causes-from-effect-38},
by the definition of EIM validity,
\begin{equation}
\label{eq:symbolic-causes-from-effect-41}
\Veim{down} = [ [ \Vsym{down} \derives \Vstr{pre} \cat \Vstr{A} \mydot \Vstr{post} ], \var{i}, \var{j} ]
\end{equation}

To show requirement \ref{item:symbolic-causes-from-effect-3},
we say that
\eqref{eq:symbolic-causes-from-effect-35}
is our bottom-up
cause.
To show requirement \ref{item:symbolic-causes-from-effect-6},
we say that
\eqref{eq:symbolic-causes-from-effect-41} is our top-down
cause.
To fulfill requirement \ref{item:symbolic-causes-from-effect-9},
we note that they match,
because \Postdot{\Veim{down}} = \Vsym{A},
and because the left position of
\eqref{eq:symbolic-causes-from-effect-35}
is the right position of \Veim{down}.
Examination of
\eqref{eq:symbolic-causes-from-effect-20},
\eqref{eq:symbolic-causes-from-effect-35},
and
\eqref{eq:symbolic-causes-from-effect-41}
shows that requirements
\ref{item:symbolic-causes-from-effect-12},
\ref{item:symbolic-causes-from-effect-15},
\ref{item:symbolic-causes-from-effect-18},
\ref{item:symbolic-causes-from-effect-19}
and
\ref{item:symbolic-causes-from-effect-21}
are also met.
\end{proof}

\begin{theorem}
\label{t:effect-from-symbol}
Let
\begin{equation}
\var{A-inst} = \Vmkm{j} \Vsym{A} \Vmkm{k}
\end{equation}
be a valid symbol instance.
If $\Vsym{A} \neq \Vsym{accept}$
then
\begin{enumerate}
\item
\label{req:effect-from-symbol-1}
\var{A-inst} is the bottom-up cause of a valid effect, call
it \Veim{effect}
\item
\label{req:effect-from-symbol-2}
\var{A-inst} has a matching top-down cause, call it
it \Veim{down},
\item
\label{req:effect-from-symbol-3}
\Vloc{j} is such that for some \Vloc{i}, \Vdr{down},
$$\Veim{down} = [\Vdr{down}, \Vloc{i}, \Vloc{j}]$$
\item
\label{req:effect-from-symbol-4}
and $\Vsym{A} = \Postdot{\Veim{down}} = \Predot{\Veim{effect}}.$
\end{enumerate}
\end{theorem}

\begin{proof}
Without loss of generality,
we let the symbolic equivalent of the bottom-up cause be
\begin{equation}
\label{eq:effect-from-symbol-6}
\Vmkl{j} \Vsym{A} \Vmkr{k}.
\end{equation}
By assumption for the theorem,
$\Vsym{A} \neq \Vsym{accept}$,
there is at least one derivation step before
\eqref{eq:effect-from-symbol-6},
so that, without loss of generality, we may write
\begin{equation}
\label{eq:effect-from-symbol-9}
\Vsym{down} \destar \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post}.
\end{equation}
By the definition of location markers there is
some \Vstr{before}
such that
\begin{gather}
\label{eq:effect-from-symbol-12}
\Vsym{accept} \destar \Vstr{before} \cat \Vsym{down} \cat \Vstr{after} \\
\label{eq:effect-from-symbol-13}
\derives \Vstr{before} \cat \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post} \cat \Vstr{after} \\
\label{eq:effect-from-symbol-14}
\text{and} \qquad \Vstr{before} \cat \Vstr{pre} \destar \var{w}[0, \var{j} \subtract 1].
\end{gather}
From \eqref{eq:effect-from-symbol-14} we know there must be some \Vloc{i},
$0 \le \var{i} < \Vloc{j}$,
such that
\begin{multline}
\label{eq:effect-from-symbol-22}
\Vsym{accept} \destar \Vstr{before} \cat \Vsym{down} \cat \Vstr{after} \\
\derives \Vstr{before} \Vmkm{i} \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post} \cat \Vstr{after}
\end{multline}
Simplifying
\eqref{eq:effect-from-symbol-22},
we have
\begin{equation}
\label{eq:effect-from-symbol-25}
\Vsym{down} \derives \Vmkm{i} \Vstr{pre} \Vmkm{j} \Vsym{A} \Vmkm{k} \Vstr{post}.
\end{equation}
From
\eqref{eq:effect-from-symbol-25},
and the definition of validity for an Earley item,
we know that
\begin{equation}
\label{eq:effect-from-symbol-28}
\Veim{down} =
[ [ \Vsym{down} \de \Vstr{pre} \mydot \Vsym{A} \cat \Vstr{post} ], \var{i}, \var{j} ] \\
\end{equation}
and
\begin{equation}
\label{eq:effect-from-symbol-29}
\Veim{effect} =
[ [ \Vsym{down} \de \Vstr{pre} \cat \Vsym{A} \mydot \Vstr{post} ], \var{i}, \var{k} ]
\end{equation}
are valid.
\eqref{eq:effect-from-symbol-28}
is requirement
\ref{req:effect-from-symbol-1} to show the theorem.
Requirements
\ref{req:effect-from-symbol-2},
\ref{req:effect-from-symbol-3},
and
\ref{req:effect-from-symbol-4}
follow from
Theorem
\ref{t:symbolic-causes-from-effect}
and \eqref{eq:effect-from-symbol-28}.
\end{proof}

\begin{theorem}
\label{t:terminal-cause-unique}
Let an instance of the terminal
\Vsym{term} be the bottom-up cause of an effect,
\Veim{effect}.
Then that instance is the only
bottom-up cause of \Veim{effect}.
\end{theorem}

\begin{proof}
Since \Vsym{term} is a terminal
it cannot appear on the LHS of a rule.
Since \Vsym{term} cannot be the LHS of a rule,
by the definition of EIM equivalent,
\Vinst{inst} has no EIM equivalent.
Let
\begin{equation}
\Vinst{inst1} = \Vmkm{l1} \Vsym{term} \Vmkm{r1}
\end{equation}
Assume for a reductio that
\Vinst{inst2} is another
another symbolic bottom-up cause of \Veim{effect},
\begin{gather}
\Vinst{inst2} = \Vmkm{l2} \Vsym{sym2} \Vmkm{r2} \\
\label{eq:terminal-cause-unique-5}
\text{where $\var{l1} \neq \var{l2}$ or $\Vsym{term} \neq \Vsym{sym2}$ or $\var{r1} \neq \var{r2}.$}
\end{gather}

But by Theorem \ref{t:symbolic-causes-from-effect},
the bottom-up cause symbol of \Veim{effect} must be
\Predot{\Veim{effect}},
so that
\begin{equation}
\label{eq:terminal-cause-unique-10}
\Vsym{term} = \Predot{\Veim{effect}} = \Vsym{sym2}.
\end{equation}

Also by Theorem \ref{t:symbolic-causes-from-effect},
the right location of its bottom-up cause must be
\Right{\Veim{effect}}, so that
\begin{equation}
\label{eq:terminal-cause-unique-20}
\var{r1} = \Right{\Veim{effect}} = \var{r2}.
\end{equation}

Finally,
if \Vsym{term} is terminal,
it must be telluric in which case its length is 1,
or nulling, in which case its length is 0.
In either case its length is fixed.
Let $\Vsize{\Vsym{term}} = \var{len}$.
Then
\begin{equation}
\label{eq:terminal-cause-unique-30}
\var{l1} = \Right{\Veim{effect}} \subtract \var{len} = \var{l2}.
\end{equation}

Gathering
\eqref{eq:terminal-cause-unique-10},
\eqref{eq:terminal-cause-unique-20}
and
\eqref{eq:terminal-cause-unique-30},
we see that
\eqref{eq:terminal-cause-unique-5} is false,
which is contrary to the assumption for the reductio.
So \Vinst{inst1} must be unique.
Our choice of \Vinst{inst1} was without loss of generality,
so this shows the theorem.
\end{proof}

\begin{theorem}\label{t:multi-cause-eq-ambiguous-g}
If any Earley item has more than one top-down cause,
or more than one bottom-up cause,
the grammar is ambiguous.
\end{theorem}

\begin{proof}
% TODO -- write a more careful proof
This may be proved using the derivations in the definition
of top-down and bottom-up cause.
More than one cause means more than one derivation.
Since every symbol in a Marpa grammar is production,
if a grammar allows more than one derivation for any of its symbols,
there exists some input for which it is ambiguous.
\end{proof}

Recall that dotted rule notions applied to EIMs
refer to the dotted rule of the EIM.
Therefore, a fleeting EIM is an EIM
with a nulling postdot symbol;
and a lasting EIM is any EIM which is not
a fleeting EIM.

Let
\begin{equation*}
\var{eim1} = [ \Vdr{dr1}, \Vorig{i}, Vloc{j} ].
\end{equation*}
Let \Vdr{lasting1} be the lasting equivalent of \Vdr{dr1}.
Then the lasting equivalent of \var{eim1}
is
\begin{equation*}
[ \Vdr{lasting1}, \Vorig{i}, Vloc{j} ].
\end{equation*}

\begin{theorem}
If an EIM is valid, so is its lasting equivalent.
\end{theorem}

\begin{proof}
Omitted.
% TODO
\end{proof}

We say that the \dfn{lasting effect} of a cause
is the lasting equivalent of the effect of that cause.

TODO: Delete this definition of stack and altitude,
in favor of the Leo ones?

Let \var{stack} be a sequence
of \var{n} items,
numbered
consecutively starting at zero.
We call a sequence of items a \dfn{cause stack},
if and only if
\begin{itemize}
\item
its first item, $\var{stack}[0]$, is a terminal symbol instance;
and
\item
For all $1 \le \var{i} \le \Vdecr{n}$,
$\var{stack}[\var{i}]$ is the lasting effect of $\var{stack}[\Vdecr{i}]$.
\end{itemize}
We call $\var{stack}[0]$ the \dfn{bottom} of the stack.
We call $\var{stack}[\Vdecr{n}]$ the \dfn{top} of the stack.

\begin{definition}
\label{s:def-altitude}

Where $\var{stack}[\var{i}]$ is an item in a cause stack,
we say that its stack index, \var{i},
is its \dfn{absolute altitude} in that stack.
We will often refer to an item's absolute altitude as,
simply, it \dfn{altitude}.

If EIM is a part of more than one cause stack,
its stack index may take on different value.
The altitude of the EIM will be the minimum of
the values of its stack indexes.
\end{definition}

Where
$\var{stack}[\var{i}]$
and
$\var{stack}[\var{j}]$
are items in a cause stack,
$\var{i} \le \var{j}$,
we say that $\var{j} \subtract \var{i}$
is the altitude of
$\var{stack}[\var{j}]$
relative to
$\var{stack}[\var{i}]$.
Where a base \var{i} is understood in context,
we say that
is the altitude of
$\var{stack}[\var{j}]$
relative to
$\var{stack}[\var{i}]$
is the \dfn{relative altitude}
of $\var{stack}[\var{j}]$.
The altitude of a stack item relative to
$\var{stack}[0]$
is the same as its absolute altitude.

\begin{theorem}
All the items of a cause stack have the same
right location.
\end{theorem}

\begin{proof}
Omitted.
% TODO
\end{proof}

\begin{theorem}
\label{t:completion-cause-stack}
Let \Veim{top} be an completed EIM.
Then it is in a cause stack,
and it has a finite altitude.
\end{theorem}

\begin{proof}
TODO: Is this theorem used?  If not, delete it.
\end{proof}

\begin{theorem}
\label{t:quasi-completion-cause-stack}
Let \Veim{top} be a quasi-completed EIM.
Then its lasting effect is in a cause stack.
\end{theorem}

\begin{proof}
TODO: Is this theorem used?  If not, delete it.
\end{proof}

\section{Earley implementations}
\label{s:earley-implementation}

In the context of a specific \Cg{},
a specific \Cw{},
and a specific Earley implementation,
an Earley parser builds a table of Earley sets,
\Ctables.
Let \alg{Impl} be an Earley implementation.
When we wish to make clear that
we are speak of the tables of the \alg{Impl},
its tables are
\Vtables{Impl}.
For example, the tables for the Marpa
are \Vtables{Marpa}.

Traditionally, the tables of an Earley algorithm
are grouped into sets,
one \dfn{Earley set} for each location of \Cw{}:
\begin{equation*}
\EVtable{Impl}{i},
\quad \text{where} \quad
0 \le \Vloc{i} \le \size{\Cw}.
\end{equation*}
Earley sets are of type \type{ES}.
Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
The type designator \type{ES} is often omitted to avoid clutter,
especially in cases where the Earley set is not
named by location.

\EVtable{\alg{Impl}}{i} will be
the Earley set at \Vloc{i}
in the table of Earley sets of
the \alg{Impl} implementation.
For example,
\EVtable{\Marpa}{j} will be Earley set \Vloc{j}
in \Marpa's table of Earley sets.
In contexts where it is clear which recognizer is
intended,
\Vtable{k}, or \Ves{k}, will symbolize Earley set \Vloc{k}
in that recognizer's table of Earley sets.
If \Ees{\var{working}} is an Earley set,
$\size{\Ees{\var{working}}}$ is the number of Earley items
in \Ees{\var{working}}.

\Rtablesize{\alg{Recce}} is the total number
of Earley items in all Earley sets for \alg{Recce},
\begin{equation*}
\Rtablesize{\alg{Recce}} =
     \sum\limits_{\Vloc{i}=0}^{\size{\Cw}}
	{\bigsize{\EVtable{\alg{Recce}}{i}}}.
\end{equation*}
For example,
\Rtablesize{\Marpa} is the total number
of Earley items in all the Earley sets of
a \Marpa{} parse.

An Earley item may be memoized.
An Earley item is \dfn{memoized} if and if it is
not kept in an Earley set,
but is kept in a form from which it can be recovered.
A later section will show one way in which Earley items
can be memoized.
To say that \Veim{x} is memoized,
we also say \Memoized{\Veim{x}}.

An set of EIMs (not necessarily an Earley set)
is \dfn{consistent} if and only if all of its
EIMs are valid and unmemoized.
For example,
the Earley set \Ves{i}
is \dfn{consistent} if and only if
every EIM in the Earley set at \Vloc{i}
is valid and unmemoized:
\begin{equation}
\label{eq:def-complete-1}
\Veim{eim} \in \Veimset{x} \implies \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}}
\end{equation}

Let \Veimset{x} be a set of EIMs.
\Veimset{x} is not necessary an Earley set.
We say that
\Veimset{x} is \dfn{complete} for a predicate \var{phi},
if and only if,
for all \Veim{eim},
\begin{equation}
\label{eq:def-complete-2}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \land \var{phi}(\var{eim}) \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation}
If we say that an EIM set, \Veimset{x},
is \dfn{self-complete},
then \var{phi} is membership in \Veimset{x}:
$$ \lambda \Veim{eim} . \var{eim} \in \Veimset{x}, $$
so that 
\eqref{eq:def-complete-2} simplifies to
\begin{equation}
\label{eq:def-complete-4}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation}

For example,
the EIM set \Veimset{x} is complete
for the predicate
\begin{equation}
\label{eq:def-complete-6}
\lambda \var{eim} . \Current{\var{eim}} = \Vloc{i}
\end{equation}
if and only if,
for all \Veim{eim},
\begin{equation*}
\begin{split}
& \Valid{\Veim{eim}} \land \neg \Memoized{\var{eim}} \land \Current{\var{eim}} = \Vloc{i} \\
& \qquad \qquad \implies \Veim{eim} \in \Veimset{x}.
\end{split}
\end{equation*}
If $\Veimset{x} = \Ves{i}$, then
saying that \Ves{i} is complete for 
\eqref{eq:def-complete-6}
is the same as saying that
\Ves{i} is self-complete.

We say that
\Veimset{x} is \dfn{correct} for a predicate \var{phi},
if and only if it is consistent
and complete for a predicate \var{phi}.
We say that
\Veimset{x} is \dfn{self-correct},
if and only if it is consistent
and self-complete.

We often say that an EIM set is complete or correct,
in a context where no predicate is specified.
In that case,
we mean that the EIM set is self-complete or self-correct.

We call
\begin{equation}
\Veim{accept} = [\Vdr{accept}, 0, \Vsize{\Cw}]
\end{equation}
the \dfn{accept EIM}.
Let \alg{Impl} be an Earley implementation.
We say that \alg{Impl}
\dfn{accepts} an input
if and only if \Veim{accept} is in its Earley tables:
\begin{equation}
\label{eq:def-implementation-accepts}
\myL{\alg{Impl},\Cg}
\defined
\Veim{accept} \in \Vtables{Impl}
\end{equation}
We say that an Earley implementation is \dfn{correct} if and only
if the set of strings it accepts is exactly the set of
strings in the language of its grammar.

\begin{theorem}\label{t:algorithm-correct}
If an Earley implementation creates a correct Earley set
at location \Vsize{\Cw}, then
that implementation accepts all and only the
correct inputs:
\begin{equation}
\label{eq:algorithm-correct-2}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}} \equiv \Cw \in \var{L}(\Cg).
\end{equation}
\end{theorem}

\begin{proof}
We prove the forward direction of the implication first.
We assume that
\begin{equation}
\label{eq:algorithm-correct-3}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
Since the algorithm is correct by assumption for the theorem,
we have, by \eqref{eq:algorithm-correct-3}
and by the definition of valid for an Earley item,
\begin{equation}
\label{eq:algorithm-correct-6}
\begin{split}
& \Vsym{accept} \destar \Vstr{before} \,[0]\, \Vstr{predot} \,[\Vsize{\Cw}]\, \Vstr{postdot} \cat \Vstr{after}
\end{split}
\end{equation}
where the accept dotted rule is
\begin{equation}
\label{eq:algorithm-correct-9}
\Vsym{accept} \de \cat \Vstr{predot} \mydot \Vstr{postdot}.
\end{equation}
Since no terminals can come before location 0 or after
location \Vsize{\Cw},
we see from
\eqref{eq:algorithm-correct-6}
that
\begin{equation}
\Vstr{before} = \Vstr{postdot} = \Vstr{after} = \epsilon.
\end{equation}
so that
\eqref{eq:algorithm-correct-6}
simplifies to
\begin{equation}
\label{eq:algorithm-correct-12}
\Vsym{accept} \destar [0]\, \Vstr{predot} \,[\Vsize{\Cw}]
\end{equation}
which, by the definition of the location markers
means that
\begin{equation}
\label{eq:algorithm-correct-15}
\Vstr{accept} \destar \var{w}[\var{0}, \Vsize{\Cw} \subtract 1] \destar \Cw.
\end{equation}
From
\eqref{eq:algorithm-correct-15},
by the definition of $\var{L}(\Cg)$,
we have that
\begin{equation}
\label{eq:algorithm-correct-27}
\Cw \in \var{L}(\Cg).
\end{equation}
This shows the forward direction of the implication.

To show the reverse direction of the implication, we assume
\eqref{eq:algorithm-correct-27}.
From it,
the definition of the \Vsym{accept} symbol,
and the definition of $\var{L}(\Cg)$,
we have
\eqref{eq:algorithm-correct-15}.
By the definition of the \Vsym{accept} symbol,
it is only on the LHS of the rule
\eqref{eq:algorithm-correct-12},
so that we have
\begin{equation}
\label{eq:algorithm-correct-40}
\Vsym{accept} \derives [0]\, \Vstr{predot} \,[\Vsize{\Cw}] \destar \Cw.
\end{equation}
By assumption for the theorem, the Earley set at
\Vsize{\Cw} is correct and therefore complete.
If the Earley set at \Vsize{\Cw} is complete,
by \eqref{eq:algorithm-correct-40},
we have
\begin{equation}
\label{eq:algorithm-correct-46}
[\Vdr{accept}, 0] \in \bigEtable{\Vsize{\Cw}}.
\end{equation}
This shows the reverse direction of the implication.
We have now shown both directions of the implication,
and therefore the theorem.
\end{proof}

\begin{theorem}
\label{t:es-count}
The size of the Earley set at \Vloc{i} is,
worst case, $\order{\var{i}}$:
\begin{equation*}
\textup{
    $\bigsize{\EVtable{\Marpa}{i}} = \order{\var{i}}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
EIM's have the form $[\Vdr{x}, \Vorig{x}]$.
\Vorig{x} is the origin of the EIM,
which in Marpa cannot be after the current
Earley set  at \Vloc{i},
so that
\begin{equation*}
0 \le \Vorig{x} \le \Vloc{i}.
\end{equation*}
The possibilities for \Vdr{x},
call it $\size{\Cdr}$,
is a finite constant that depend on the grammar,
\Cg{}.
Since duplicate EIM's are never added to an Earley set,
the maximum size of Earley set \Vloc{i} is therefore
\begin{equation*}
\Vloc{i} \times \size{\Cdr} = \order{\Vloc{i}}.\qedhere
\end{equation*}
\end{proof}

\section{The Leo algorithm}
\label{s:leo}

In \cite{Leo1991}, Joop Leo presented a method for
dealing with right recursion in \On{} time.
Leo shows that,
with his modification, Earley's algorithm
is \On{} for all LR-regular grammars.
(LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.)

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this \doc{} Leo's ``transitive items''
will be called Leo items.
Leo items of the form used in Leo's paper\cite{Leo1991},
will be type \type{LIM}.

In each Earley set, there is at most one Leo item per symbol.
A Leo item (LIM) is the triple
\begin{equation*}
[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]
\end{equation*}
where \Vsym{transition} is the transition symbol,
and
\begin{equation*}
\Veim{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley item to be added on reductions over
\Vsym{transition}.

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
so that the top of the sequence can represent
the entire sequence --
the only role the other EIM's in the sequence
play in the parse is to derive the top EIM.
We will call these memoized sequences, Leo sequences.

To guarantee that a Leo sequence is deterministic,
\Leo{} enforced \dfn{Leo uniqueness}.
We say that \Vdr{q} is a \dfn{quasi-penult}
if and only if it is
\begin{equation*}
\begin{split}
& \Vdr{q} = [ \Vsym{A} \de \Vstr{before} \mydot \Vsym{B} \cat \Vstr{after} ] \\
& \qquad \text{for some $[ \Vsym{A} \de \Vstr{before} \cat \Vsym{B} \cat \Vstr{after} ] \in \Crules$} \\
& \qquad \qquad \text{such that $\Vstr{after} \derives \epsilon \land \Vstr{B} \nderives \epsilon$}.
\end{split}
\end{equation*}
We say that \Veim{x} is \dfn{Leo unique}
if and only if it is
\begin{equation*}
\begin{split}
& \Veim{x} = [ \Vdr{x},  \Vorig{x}, \Vloc{x} ] \\
& \qquad \text{for some \Vdr{x}, \Vorig{x}, \Vloc{x}} \\
& \qquad \text{such that for all $\Veim{y} = [ \Vdr{y},  \Vorig{y}, \Vloc{x} ]$} \\
& \qquad \qquad \Postdot{\Vdr{x}} = \Postdot{\Vdr{y}} \implies \Veim{x} = \Veim{y} .
\end{split}
\end{equation*}

If \Veim{x} is Leo unique, then the symbol \Vdr{x}
and $\Postdot{\Vdr{x}}$ are
also said to be \dfn{Leo unique} in Earley set \Vloc{x}.
In the definition
it is important to emphasize that \Veim{y} ranges over all the dotted
rules of Earley set \Ves{x},
even those which are ineligible for Leo memoization.

Let \var{n} be the length of a Leo sequence.
In \Earley, each such sequence would be expanded in every
Earley set that is the origin of an EIM included in the
sequence, and the total number of EIM's would be
\order{\var{n}^2}.

With Leo memoization, a single EIM stands in for the sequence.
There are \Oc{} Leo items per Earley set,
so the cost of the sequence is \Oc{} per Earley set,
or \On{} for the entire sequence.
If, at evaluation time,
it is desirable to expand the Leo sequence,
only those items actually involved in the parse
need to be expanded.
All the EIM's of a potential right-recursion
will be in one Earley set and the number of EIM's
will be \On{},
so that even including expansion of the Leo sequence
for evaluation, the time and space complexity of
the sequence remains \On{}.

In Leo's original algorithm, any penult
was treated as a potential right-recursion.
\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\begin{equation*}
\Vdr{candidate} = [\Vrule{candidate}, \var{i}]
\end{equation*}
for Leo memoization,
\Vdr{candidate} must be a penult and
\Vrule{candidate} must be right-recursive.

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be infinitely
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff is therefore very limited.
One future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
justify inclusion in the Leo memoizations.
Such cases are unusual, but may occur.

Omission of a memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
Later in this \doc{} we will
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

TODO: Can Leo altitude be simply ``altitude''?

\begin{definition}
\label{def:altitude}
\begin{RaggedRight}
Intuitively,
\dfn{Leo-adjusted altitude},
or simply \dfn{Leo altitude},
is altitude adjusted to reflect Leo memoization.
We write \Alt{\Vinst{inst}} for the Leo altitude of \Vinst{inst}.
More formally,
\begin{itemize}
\item The Leo altitude of a terminal symbol instance is 0.
\item The Leo altitude of a prediction is the altitude of its top-down cause.
\item The Leo altitude of the start rule is 1.
\item
The Leo altitude of a read EIM is
altitude of its bottom-up cause, plus 1.
Since the bottom-up cause of a read EIM is always
a terminal symbol instance,
its Leo altitude will always be 1.
\item The Leo altitude of null-scan EIM is that of
its top-down cause.
This case applies if the null-scan is memoized,
its top-down cause is memoized,
or both.
\item A possible Leo altitude of an
unmemoized reduction EIM
is the Leo altitude of its bottom-up cause, plus 1.
\item A possible Leo altitude of an
memoized reduction EIM
is the Leo altitude of the bottom-up cause.
\item
The Leo altitude of an EIM is the minimum of its possible
values.
\end{itemize}
\par
\end{RaggedRight}

\end{definition}

When we say \dfn{altitude cause},
we mean whichever of its causes
-- bottom-up or top-down is used to calculate its possible
altitudes.

\begin{theorem}
\label{predot-therefore-altitude}
If an EIM has a predot symbol, it
it has a defined altitude cause.
If the predot symbol is null,
the altitude cause is the top-down cause.
If the predot symbol is telluric,
the altitude cause is the bottom-up cause.
\end{theorem}

\begin{proof}
The theorem can be proved by
going through Definition \ref{def:altitude}
case by case.
\end{proof}

\begin{theorem}
\label{cause-therefore-altitude}
Let \Vinst{i} be an instance.
If \Vinst{i} has a defined altitude cause,
call it \Vinst{cuz},
and the altitude
of \Vinst{cuz} is defined,
then
the altitude
of \Vinst{i} is defined.
\end{theorem}

\begin{proof}
The theorem can be proved by
going through Definition \ref{def:altitude}
case by case.
\end{proof}

\begin{theorem}
\label{t:leo-silo-altitude}
The altitude of an unmemoized quasi-confirmed
EIM is one plus that of the
most recent unmemoized bottom-up cause in its silo.
\end{theorem}

\begin{proof}
Let \var{silo} be an altitude silo
Let $\Veim{top} = \var{silo}[\var{n}]$
be an unmemoized reduction.
Let $\Veim{bottom} = \var{silo}[\var{m}]$
be its most recent unmemoized bottom-up cause.
This is, $\var{m} < \var{n}$,
and where
$$\text{$\forall\;\var{i}, \var{m} < \var{i} < \var{n}$,
$\var{silo}[\var{i}]$ is memoized.}$$

By the definition of altitude
(Definition \ref{def:altitude})
for
unmemoized reductions,
$\Alt{\var{silo}[\var{n}]} = \Alt{\var{silo}[\Vdecr{n}]} + 1$.
For $\var{silo}[\var{i}], \var{m} < \var{i} < \var{n}$,
$\var{silo}[\var{i}]$ is either a reduction or a null-scan,
and is memoized,
so that by the Definition \ref{def:altitude},
$\Alt{\var{silo}[\var{i}]} = \Alt{\var{silo}[\Vdecr{i}]}$.
Therefore
$$\Alt{\var{silo}[\var{n}]} = \Alt{\var{silo}[\var{m}]} + 1$$
and
\begin{equation*}
\Alt{\Veim{top}} = \Alt{\Veim{bottom}} + 1.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}
\label{altitude-from-telluric-base}
The Leo altitude of a
confirmed EIM is that of its telluric base.
\end{theorem}

\begin{proof}
Let the EIM be
\begin{equation*}
\Veim{z} = [ \Vdr{z}, \Vorig{z}, \Vloc{z} ].
\end{equation*}
The proof is by induction on the dot position,
moving the dot forward from the telluric base of \Veim{z}
until we reach the dot position of \Veim{z}.
Since \Veim{z} is a confirmed rule, we know that it
has a telluric base.
As the basis of the induction,
we note
that the theorem is obvious if \Veim{z} is its own telluric base.

As the step of the induction,
we assume that the Leo altitude of \Veim{x} is that of its
telluric base,
and we seek to show that the Leo altitude of
\begin{equation*}
\Veim{y} = [ \Next{\Vdr{x}}, \Vorig{x}, \Vloc{y} ]
\end{equation*}
is also that of its telluric base,
By the definition of telluric base,
if \Veim{y} is not its own telluric base,
it is a null-scan EIM,
so that $\Vloc{y} = \Vloc{z}$.
Using the of top-down cause,
Definition \ref{def:causes},
we see that \Veim{y} is the top-down cause
of \Veim{z}.

By the definition of Leo altitude,
\ref{def:altitude},
the Leo altitude of a null-scan EIM is
that of its top-down cause.
So the Leo altitude of \Veim{y} is the
same as the Leo altitude of \Veim{x}.
This shows the step, the induction,
and the theorem.
\end{proof}

\begin{theorem}
\label{t:ethereal-altitude}
All the EIMs in an etheral closure have the same
altitude.
\end{theorem}

\begin{proof}
This follows directly from
Theorem \ref{altitude-from-telluric-base}.
\end{proof}

We now define
the altitude derivation of a telluric base.
Let the telluric base be
\begin{multline}
\Veim{base} = \\
[ [ \Vsym{base} \de \Vstr{pre} \cat \Vsym{tell} \mydot \Vstr{post} ], \Vloc{i}, \Vloc{k} ]
\end{multline}
We say the altitude derivation of \Veim{base} is
its rightmost derivation before \Vloc{k},
\begin{align}
\label{def:altitude-derivation-step-n}
& \Vmkl{i} \Vstr{pre} \cat \Vsym{tell} \Vmkr{k} \Vstr{post} \\
\label{def:altitude-derivation-step-0}
  \destar & \Vmkl{i} \Vstr{pre-tt} \Vsym{tt} \Vmkm{k} \Vstr{nulls} \cat \Vstr{post},
\end{align}
where \Vsym{tt} is a telluric terminal
and $\Vstr{nulls} \destar \epsilon$.
Because all Marpa symbols are productive,
we know there is such a derivation,
and that it has a finite length.
We also know that $\Vsym{tt} = \CVw{\Vdecr{k}}$.
We will call the length $\var{n}+1$.
We call \eqref{def:altitude-derivation-step-n}, Step \var{n},
and we call \eqref{def:altitude-derivation-step-0}, Step 0.

\begin{definition}
\label{def:altitude-silo}
The \dfn{altitude silo} is a sequence of parse instances,
based on an altitude derivation.
There is one element in the silo
for each step of an altitude derivation.

Call a silo, \var{silo}.
We say that
\begin{equation}
\label{eq:altitude-silo-10}
\var{silo}[0] = \mkl{\Vdecr{k}} \Vsym{term} \Vmkr{k}.
\end{equation}
where $\Vsym{term} = \Cw[\Vdecr{k}]$.

Consider a step of the underlying
altitude derivation.
Let the rightmost symbol instance of Step \var{i} be
$\Vmkl{j} \Vsym{A} \Vmkr{k}$.

For Step \var{i}, $1 \le \var{i} \le \var{n}$,
if \Vsym{A} is a non-terminal
where $\Vmkl{j} \Vstr{A-rhs} \Vmkr{k}$
are its direct descendants
in Step \Vdecr{i},
we say that
\begin{equation}
\label{eq:altitude-silo-20}
\var{silo}[\var{i}] = [ [ \Vsym{A} \de \Vstr{A-rhs} \mydot ], \Vloc{j}, \Vloc{k} ].
\end{equation}

For Step \var{i}, $1 \le \var{i} \le \Vdecr{n}$,
if \Vsym{A} is nulling
wheer
$\var{silo}[\var{i}+1] = [ \Vdr{above}, \Vloc{j}, \Vloc{k}]$,
we say that
\begin{equation}
\label{eq:altitude-silo-30}
\var{silo}[\var{i}] = [ \Prev{\Vdr{above}}, \Vloc{j}, \Vloc{k} ].
\end{equation}
\end{definition}

\begin{theorem}
\label{altitude-silo-causes}
Definition \ref{def:altitude-silo} is well-formed
and for every \var{i}, $1 \le \var{i} \le \var{n}$,
\var{silo}[\Vdecr{i}] is the altitude cause of
\var{silo}[\var{i}].
\end{theorem}

For writing rightmost derivations before \Vloc{k},
there are two conventions for indicating that a derivation
step has expanded a null symbol into its direct descendants.
Usually we remove the symbol, on the idea that it
has expanded into zero terminal symbols.

\begin{proof}
In this proof,
when we refer to the rightmost symbol,
we will mean the rightmost symbol of a derivation
step before \Vloc{k}.
We will
use the definition of validity of an EIM
to go back and forth,
between derivation steps,
and the EIMs they validate.
By the definition of validity of an EIM,
if \Vsym{rm} is the rightmost symbol before \Vloc{k}
in derivation Step \var{x},
then \Vsym{rm} will be the predot symbol
in any EIM validated by Step \var{x},
and vice versa.

We note that
Definition \ref{def:altitude-silo} deals with three cases:
Step 0, non-terminal rightmost symbols
and nulling rightmost symbols.
By the definition of the altitude derivation,
it starts will a telluric base
and stops when the rightmost symbol
is a telluric terminal.
The point at which it stops is Step 0
by definition, so the case of telluric terminals
is handled by explicitly specifying the behavior
at Step 0.
Since the altitude derivation starts with a
telluric base and never discards or moves past
a telluric terminal,
it will always have a predot symbol.

We now look at the three cases separately.
\textbf{The first case}
is Definition \ref{def:altitude-silo} for
non-terminal predot symbols.
For Step \var{i},
this requires that there be a Step \Vdecr{i}.
Since we only examine Steps 1 and greater
for this first case,
we know there is a Step \Vdecr{i}.
Call the EIM in \eqref{eq:altitude-silo-20},
\Veim{up}.
By
Theorem \ref{t:symbolic-causes-from-effect}
and Theorem \ref{t:valid-eim-from-symbol},
we see that \Veim{up}
is the bottom-up cause of any EIM with
a non-terminal predot symbol.
By
\eqref{predot-therefore-altitude},
we see that \Veim{up}
is the altitude cause of any EIM with
a non-terminal predot symbol.
We note that its right location is \var{k}.

\textbf{The second case}
is Definition \ref{def:altitude-silo} for
nulling predot symbols.
For Step \var{i},
this requires that there be a Step $\var{i}+1$.
By the definition of an altitude derivation,
Step \var{n} has a telluric predot symbol,
so we know we have
a Step $\var{i}+1$.
Call the EIM in \eqref{eq:altitude-silo-20},
\Veim{down}.
By
Theorem \ref{t:symbolic-causes-from-effect},
we know that \Veim{down} must be the top-down
cause of $\var{silo}[\var{i}+1]$.
By
\eqref{predot-therefore-altitude},
we see that \Veim{down}
is the altitude cause of any EIM with
a nulling predot symbol.
We note that its right location is \var{k}.

\textbf{The third case} is
is Definition \ref{def:altitude-silo} for
Step 0, the only place where the altitude
derivation can have a telluric terminal
as its rightmost symbol.
We have noted above, that all the EIMs in \var{silo}
has a right location of \var{k}.
We know from 
Theorem \ref{t:symbolic-causes-from-effect}
that the bottom-up cause must have a right location
of \var{k} if its effect does.
We also know from
Theorem \ref{t:symbolic-causes-from-effect},
that for a telluric terminal, this
bottom-up cause must be
that in
\eqref{eq:altitude-silo-10}.
By
\eqref{predot-therefore-altitude}
we know that the bottom-up is also the altitude
cause of any EIM with a telluric terminal
as its predot symbol.
This concludes the three cases and shows the
theorem.
\end{proof}

\begin{theorem}
\label{t:base-altitude-is-finite}
If \Veim{base} is a valid telluric base EIM,
its Leo altitude is defined and finite.
\end{theorem}

\begin{proof}
By the definition of altitude derivation,
there is an altitude derivation for \Veim{base}.
We use that altitude derivation as the basis of the
altitude silo, \var{silo}.

From the Definition \ref{def:altitude-silo}, we know
that $\Veim{base} = \var{silo}[\var{n}]$
We show that the Leo altitude of \var{silo}[\var{n}]
is defined and finite,
by induction on \var{i}.

Our induction hypothesis is
$$ \text{\Alt{\var{silo}[\var{i}]} is defined and finite}. $$
We take 0 as the basis of the induction.
From Definition \ref{def:altitude-silo}, we know
that \var{silo}[0] is a telluric terminal
symbol instance, so that its Leo altitude is 0.

As the step of the induction, we assume that
$$ \text{\Alt{\var{silo}[\var{i}]} is defined and finite} $$
to show that
\begin{equation}
\label{eq:base-altitude-is-finite-30}
\text{\Alt{\var{silo}[\var{i}+1]} is defined and finite}.
\end{equation}
Theorem \eqref{altitude-silo-causes} shows that
$\var{silo}[\var{i}]$ is the altitude cause of
$\var{silo}[\var{i}+1]$.
Therefore,
from Theorem
\ref{cause-therefore-altitude}
and the assumption for the induction step,
therefore,
we have
\eqref{eq:base-altitude-is-finite-30}.
This shows the step, the induction
and the theorem.
\end{proof}

\begin{theorem}
\label{t:quasi-confirmed-altitude-is-finite}
If \Veim{confirmed} is a valid, quasi-confirmed EIM,
its Leo altitude is defined and finite.
\end{theorem}

\begin{proof}
\Veim{confirmed} must have a telluric base.
Call this \Veim{base}.
By theorem \ref{t:base-altitude-is-finite},
the Leo altitude of \Veim{base}
is defined and finite.
By theorem
\ref{altitude-from-telluric-base},
$$\Alt{\Veim{base}} = \Alt{\Veim{confirmed}}.$$
Therefore the altitude of 
\Alt{\Veim{confirmed}} is defined and finite.
\end{proof}

\begin{theorem}
\label{t:altitude-is-finite}
If \Veim{eim} is a valid EIM,
its Leo altitude is defined and finite.
\end{theorem}

\begin{proof}
If \Veim{eim} is quasi-confirmed,
then this theorem follows from
Theorem \ref{t:quasi-confirmed-altitude-is-finite}.
If \Veim{eim} is the start EIM,
this theorem follows from
the definition of Leo altitude,
\ref{def:altitude}.

It remains to show the theorem for predictions.
Also by Definition \ref{def:altitude},
a prediction's altitude is that of its top-down cause.
The top-down cause of a prediction may be another prediction.
Let, without loss of generality,
\Veim{pred} be a prediction EIM.
By the definition of EIM validity, \Veim{pred}
is ultimately derived from the start EIM.
And by the definition of top-down causes, there is a top-down
cause for every EIM except the start EIM,
so there is a chain of top-down causes from the start EIM to 
\Veim{pred}
\begin{equation}
 \var{chain}[0] \ldots \var{chain}[\var{n}]
\end{equation}
where $\var{chain}[0]$ is the start EIM and
$\var{chain}[\var{n}] = \Veim{pred}$.
In this chain, there will be at least one top-down cause which
is not a prediction.
Call the last confirmed EIM in \var{chain},
$\var{chain}[\var{last-confirmed}]$.
Every EIM in the chain $\var{chain}[\var{i}]$,
such that $\var{last-confirmed} < \var{i} \le \var{n}$,
is a prediction,
so that,
by the definition of Leo altitude for predictions,
we know that
$$ \Alt{\var{chain}[\Vdecr{i}]} = \Alt{\var{chain}[\var{i}]}.$$
From this, it is easy to show by induction that
$$ \Alt{\var{chain}[\var{last-confirmed}]} = \Alt{\var{chain}[\var{n}]}
= \Alt{\Veim{pred}}.
$$
Therefore, 
\Alt{\Veim{pred}} is finite and defined.
This shows the theorem for predictions.

It remains to show the theorem for quasi-predictions.
Any quasi-prediction, by the definition of ethereal closure,
is in the ethereal closure of a prediction.
By Theorem \ref{t:ethereal-altitude}, therefore,
the altitude of quasi-predictions is defined and finite.

We have shown the theorem for the case of a valid start EIM,
for valid quasi-predictions,
and for valid quasi-completions.
Therefore we have shown it
for all valid EIMs.
\end{proof}

\begin{theorem}
\label{t:leo-quasi-complete}
Only quasi-complete EIMs are
Leo-memoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:leo-telluric-base}
If an EIM is memoized,
its telluric base is also Leo-memoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:read-eim-not-leo}
Read EIMs and the EIMs in the
ethereal closure of a read EIM
are never Leo-memoized.
\end{theorem}

\begin{proof}
Assume for a reductio,
that an EIM in the ethereal closure of a read EIM
is memoized.
Then, by Theorem \ref{t:leo-telluric-base},
its telluric base must be memoized.
This telluric base will be a read EIM.
Call this EIM, \Veim{scanned}.

By the definition of a read EIM,
the predot symbol of \Veim{scanned}
is a terminal.
Since \Veim{scanned} is memoized,
by Theorem \ref{t:leo-quasi-complete},
it must be quasi-complete, so the predot
symbol of \Veim{scanned} must also be its penult.
Therefore the quasi-penult of \Veim{scanned} is
a terminal.

Only EIMs with right recursive rules are memoized.
From the definition of right recursion,
a right recursive rule must have a right recursive
symbol as its quasi-penult.
But a terminal cannot be recursive symbol.

Therefore \Veim{scanned} cannot have a right recursive rule,
and therefore \Veim{scanned} is not memoized,
which is contrary to the assumption for the reductio.
This shows the reduction and the theorem.
\end{proof}

\begin{theorem}
\label{t:earley-set-0-is-Leo-free}
No EIM in Earley set 0 is memoized.
\end{theorem}

\begin{proof}
By definition,
a quasi-complete EIM has at least one
telluric predot symbol,
so its current location cannot be location 0.
Therefore no quasi-complete EIM occurs in
Earley set 0.
But,
by Theorem \ref{t:leo-quasi-complete},
only quasi-complete EIMs are memoized.
Therefore no EIM in Earley set 0
is memoized.
\end{proof}

\begin{theorem}
\label{t:ethereal-closure-is-Leo-disjoint}
Let \Veim{eim2}
be an EIM in the ethereal closure of \Veim{eim1}.
Then \Veim{eim1} is Leo-memoized if and only if
\Veim{eim2} is Leo-memoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

A \dfn{non-trivial Leo top item} is
is an unmemoized item with a memoized bottom-up
cause.

\begin{theorem}
\label{t:memoized-effects}
If an effect is unmemoized,
all of its causes are unmemoized,
with one possible exception:
a non-trivial Leo top item.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:memoized-causes}
If an effect is memoized,
either it has a Leo memo,
or its bottom-up cause is memoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:leo-top-from-memo}
Let \Vleo{memo} be a Leo memo
and \Veim{up} a matching EIM with altitude \var{i}.
Their \Veim{effect} is valid, unmemoized,
and has altitude $\var{i}+1$.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:leo-memo-from-top}
Let \Veim{top} be a valid non-trivial Leo top item.
Then it will have a valid Leo memo,
then a bottom-up cause which matches the Leo memo
will be valid and unmemoized.
\end{theorem}

\begin{proof}
TODO.
\end{proof}

\begin{theorem}
\label{t:accept-eim-not-memoized}
The accept EIM is never memoized.
\end{theorem}

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

\subsection{Complexity}

Alongside the pseudocode of this section
are observations about its space and time complexity.
In what follows,
we will charge all time and space resources
to Earley items,
or to attempts to add Earley items.
We will show that,
to each Earley item actually added,
or to each attempt to add a duplicate Earley item,
we can charge amortized \Oc{} time and space.

At points, it will not be immediately
convenient to speak of
charging a resource
to an Earley item
or to an attempt to add a duplicate
Earley item.
In those circumstances,
we speak of charging time and space
\begin{itemize}
\item to the parse; or
\item to the Earley set; or
\item to the current procedure's caller.
\end{itemize}

We can charge time and space to the parse itself,
as long as the total time and space charged is \Oc.
Afterwards, this resource can be re-charged to
the initial Earley item, which is present in all parses.
Soft and hard failures of the recognizer use
worst-case \Oc{} resource,
and are charged to the parse.

We can charge resources to the Earley set,
as long as the time or space is \Oc.
Afterwards,
the resource charged to the Earley set can be
re-charged to an arbitrary member of the Earley set,
for example, the first.
If an Earley set is empty,
the parse must fail,
and the time can be charged to the parse.

In a procedure,
resource can be ``caller-included''.
Caller-included resource is not accounted for in
the current procedure,
but passed upward to the procedure's caller,
to be accounted for there.
A procedure to which caller-included resource is passed will
sometimes pass the resource upward to its own caller,
although of course the top-level procedure does not do this.

For each procedure, we will state whether
the time and space we are charging is inclusive or exclusive.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource charges passed up from called procedures.
Inclusive time or space includes
resource passed upward to the
current procedure from called procedures.

Earley sets may be represented by \Ves{i},
where \var{i} is the Earley set's location \Vloc{i}.
The two notations should be regarded as interchangeable.
The actual implementation of either
should be the equivalent of a pointer to
a data structure containing,
at a minium,
the Earley items,
a memoization of the Earley set's location as an integer,
and a per-set-list.
Per-set-lists will be described in Section \ref{s:per-set-lists}.

\begin{algorithm}[th]
\caption{Marpa Top-level}
\label{alg:top}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initialization}{}
\label{line:top-20}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\label{line:top-30}
\State \Call{Read pass}{$\var{i}, \var{w}[\Vdecr{i}]$}
\label{line:top-33}
\If{$\size{\Ves{i}} = 0$}
\State reject \Cw{} and return
\EndIf
\State \Call{Reduction pass}{\var{i}}
\label{line:top-40}
\EndFor
\If{$[\Vdr{accept}, 0] \in \Etable{\Vsize{w}}$}
\State accept \Cw{} and return
\EndIf
\State reject \Cw{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets.
Inclusive time and space for the final loop to
check for \Vdr{accept} is charged to
the Earley items at location \size{\Cw}.
Overhead is charged to the parse.
All these resource charges are obviously \Oc.

\subsection{Ruby Slippers parsing}
This top-level code represents a significant change
from previous versions of Earley's algorithm.
\call{Read pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when the scanning of tokens that start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.


\begin{algorithm}[th]
\caption{Initialization}
\label{alg:initial}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add EIM set}{$\dr{start}, 0, 0$}
\label{line:initial-10}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}
\label{p:initial-op}

\subsubsection{Initialization complexity}
\label{p:initial-op-complexity}
Inclusive time and space is \Oc{}
and is charged to the parse.

\subsubsection{Initialization correctness}
\label{p:initial-op-correct}

\begin{theorem}
\label{t:initial-op-correct}
Initialization is correct.
\end{theorem}

\begin{proof}
From Theorem
\ref{t:earley-set-0-is-Leo-free},
we know that Leo memoization has
no effect on the set of
EIMs in Earley set 0.

Since the bottom-up causes of both
reads and reductions
has an input length of greater than 0,
these EIMs cannot appear in Earley set 0.
All EIMs in Earley set 0 are therefore
\begin{itemize}
\item the start Earley item,
\item predictions, or
\item null-scans.
\end{itemize}

The set of start Earley items that we will
add to Earley set 0 is the singleton set
\begin{multline}\label{eq:initial-op-correct}
\left\lbrace \bigl[ [ \Vsym{accept} \de \mydot \Vsym{start} ], 0, 0 \bigr] \right\rbrace \\
\text{where $[ \Vsym{accept} \de \mydot \Vstr{start} ]$ is the start rule.}
\end{multline}
We first show that this set is correct.
It is consistent by theorem \ref{t:start-eim-is-valid}.
It is complete because the start rule is by definition unique.
Therefore the set of start EIMs
added by
Algorithm \ref{alg:initial} to Earley set 0
is correct.

TODO: Prove these next assertions.

The predictions and null-scans
must have a top-down cause in the same
Earley set.
Together, the predictions and null-scans
are exactly the EIMs with null transitions.
So null transition EIMs
must have some other EIM in Earley set 0
as either a direct or an indirect cause.
The start Earley item is
the only remaining possibility,
and it is in Earley set 0.
Therefore the start Earley item is the direct
or indirect cause of all other EIMs in Earley set 0.

From these considerations, we see that Earley 0
consists of the start Earley item and the transitive
closure of null transition from it.
By Theorem \ref{t:ethereal-closure-op-correct},
this is exactly the set of EIMs added to Earley set 0
in line
\ref{line:initial-10}
of Algorithm \ref{alg:initial}.
\end{proof}

\begin{algorithm}[th]
\caption{Marpa Read pass}
\label{alg:read-pass}
\begin{algorithmic}[1]
\Procedure{Read pass}{$\Vloc{i},\Vsym{up}$}
\State Note: Each pass through this loop is an EIM attempt
\For{each $\Veim{down} \in \var{transitions}((\var{i} \subtract 1),\Vsym{up})$}
\label{line:read-pass-18}
\State $[\Vdr{down}, \Vloc{origin}] \gets \Veim{down}$
\label{line:read-pass-20}
\State $\Vdr{effect} \gets \GOTO(\Vdr{down}, \Vsym{up})$
\State \Call{Add EIM set}{$\Vdr{effect}, \Vloc{origin}, \Vloc{i}$}
\label{line:read-pass-40}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Read pass}
\label{p:read-op}

\subsubsection{Read pass complexity}
\label{p:read-op-complexity}

\var{transitions} is a set of tables, one per Earley set.
The tables in the set are indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
since the number of Earley sets grows with
the length of the parse,
it cannot be assumed that Earley sets can be indexed by location
in \Oc{} time.
For the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be in \Oc{} time,
\Vloc{l} must represent a link directly to the Earley set.
In the case of scanning,
the lookup is always in the previous Earley set,
which can easily be tracked in \Oc{} space
and retrieved in \Oc{} time.
Inclusive time and space can be charged to the
\Veim{down}.
Overhead is charged to the Earley set at \Vloc{i}.

\subsubsection{Read pass correctness}
\label{p:read-op-correct}

\begin{theorem}
\label{t:read-op-correct}
Call the
EIMs at \Vloc{i}
whose bottom-up cause is a terminal
symbol instance,
the ``EIMs read at \Vloc{i}``.
Let the Earley tables for all locations
\Vloc{h},
$\var{h} < \Vloc{i}$ be correct.
Then
Algorithm \ref{alg:read-pass}
at \Vloc{i}
adds all and only
the ethereal closure of
the EIMs read at \Vloc{i}.
\end{theorem}

\begin{proof}
We first assure ourselves that Leo
memoization has no effect on the read pass.
The bottom-up causes
in Algorithm
\ref{alg:read-pass}
are terminal symbol instances.
Leo memoizations are of EIMs --
terminal instances are not Leo-memoized.

The top-down causes
in Algorithm
\ref{alg:read-pass}
must be quasi-incomplete because it is the result
of the \var{transitions} function,
which only memoizes EIMs with a telluric postdot
symbol --
see line
\ref{line:memoize-transitions-20}
in Algorithm
\ref{alg:memoize-transitions}.
A quasi-complete EIM cannot have a
telluric postdot symbol,
so therefore
\Veim{down} at line
\ref{line:read-pass-20}
must be quasi-incomplete.
By Theorem
\ref{t:leo-quasi-complete},
no quasi-incomplete EIM
is Leo-memoized.
Therefore none of the EIMs used
in Algorithm
\ref{alg:memoize-transitions}
will be overlooked because
of Leo memoization.

By Theorem \ref{t:read-eim-not-leo},
read EIMs are never Leo-memoized.
We have now shown that none of the parse instances
referenced in Algorithm \ref{alg:read-pass},
are Leo-memoized.
We will therefore ignore Leo memoization in the
rest of this proof.

By the definition of \Cw{},
there is only one
terminal symbol instance
with right location \Vloc{i}.
Without loss of generality, let this instance be
\begin{equation}
\label{eq:read-op-correct-40}
\mkl{\var{i} \subtract 1} \,\, \Vsym{up} \Vmkr{i}
\end{equation}
The EIMs read at \Vloc{i}.
are the EIMs whose bottom-up cause
is \eqref{eq:read-op-correct-40}.

The definition of matching top-down cause
requires that \Veim{down} have a postdot symbol
of \Vsym{up} and a right location of
${\var{i} \subtract 1}$.
By theorem
\ref{t:memoize-transitions-correct},
the \var{transitions} function returns all of these for
use as the value of \Veim{down}
in the loop at line
\ref{line:read-pass-18}.

From this, we observe that
all and only
the matching pairs of causes
for the EIMs read at \Vloc{i}
are used to add EIMs
in the loop at line
\ref{line:read-pass-40}.
Call this the ``cause-correctness'' observation.

From the cause-correctness observation and
Theorem
\ref{t:effect-from-symbolic-causes}
we know that the EIMs we attempt to add
at line
\ref{line:read-pass-40}
are consistent.
From the cause-correctness observation and
Theorem
\ref{t:symbolic-causes-from-effect}
we know that the EIMs we attempt to add
at line
\ref{line:read-pass-40}
are complete.

Since the
the EIMs we attempt to add
at line
\ref{line:read-pass-40}
are consistent and complete,
we know that,
at line
\ref{line:read-pass-40},
we attempted to add EIMs from
a correct set of causes
for the EIMs read at \Vloc{i}.
By Theorem
\ref{t:ethereal-closure-op-correct}
we know that the
Algorithm
\ref{alg:eim-set}
used
at line
\ref{line:read-pass-40}
adds the ethereal closure of
the EIM that is its argument.
Therefore, we did add
all of, and only, the EIMs
in the ethereal closure of the
EIMs read at \Vloc{i},
as required for the theorem.
\end{proof}

\begin{algorithm}[th]
\caption{Reduction pass}
\label{alg:reduction-pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\State Note: \Vtable{i} may include EIM's added by
\State \hspace{2.5em} by \Call{Reduce one up-cause}{} and
\State \hspace{2.5em} the loop must traverse these
\label{line:reduction-pass-18}
\For{each completed Earley item $\Veim{up} \in \Vtable{i}$}
\label{line:reduction-pass-20}
\State $[\Vdr{up}, \Vloc{origin}, \Vloc{dummy}] \gets \Veim{up}$
\State \Comment It is always the case that $\Vloc{dummy} = \Vloc{i}$
\State \Call{Reduce one up-cause}{\Vloc{i}, \Vloc{origin}, \LHS{\Vdr{lhs}}}
\EndFor
\label{line:reduction-pass-50}
\State \Call{Memoize transitions}{\Vloc{i}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduction pass}

\subsubsection{Reduction pass complexity}
\label{p:reduction-op-complexity}

The loop over \Vtable{i} must also include
any items added by \call{Reduce one up-cause}{}.
This can be done by implementing \Vtable{i} as an ordered
set and adding new items at the end.

Exclusive time is clearly \Oc{} per
\Veim{work},
and is charged to the \Veim{work}.
Additionally,
some of the time required by
\call{Reduce one up-cause}{} is caller-included,
and therefore charged to this procedure.
Inclusive time from \call{Reduce one up-cause}{}
is \Oc{} per call,
as will be seen in section \ref{p:reduce-one-up-cause},
and is charged to the \Veim{work}
that is current
during that call to \call{Reduce one up-cause}{}.
Overhead may be charged to the Earley set at \Vloc{i}.

\subsubsection{Reduction pass correctness}
\label{p:reduction-op-correct}

\begin{theorem}
\label{t:reduction-op-correct}
Assume that
\begin{equation}
\label{eq:reduction-op-correct-5}
\myparbox{
for all locations \Vloc{h},
and all symbols in \Cg{}, \Vsym{sym},
 if $\var{h} < \Vloc{current}$, then \Ves{h} is correct
and $\var{transitions}(\var{h}, \Vsym{sym})$ is correct.
}
\end{equation}
Also assume that
\begin{equation}
\label{eq:reduction-op-correct-8}
\myparbox{the ethereal closure of the set of EIMs read at \Vloc{current} is correct.}
\end{equation}
Let \Veimset{reduced-base} be
the set of
unmemoized EIMs at \Vloc{current}
which are reductions --
that is,
EIMs whose bottom-up cause is an EIM.
Let \Veimset{reduced-closure} be
the ethereal closure of \Veimset{reduced}.
If \eqref{eq:reduction-op-correct-5}
and \eqref{eq:reduction-op-correct-8},
then after Algorithm \ref{alg:reduction-pass}
runs,
the set of EIMs at \Vloc{current}
is correct for membership in \Veimset{reduced-closure}.
\end{theorem}

\begin{proof}
EIMs added as a result of
Algorithm \ref{alg:reduction-pass}
are added by Algorithm \ref{alg:earley-reduction-op}
at line \ref{line:earley-reduction-op-20};
or by Algorithm \ref{alg:leo-reduction-op}
at line \ref{line:leo-reduction-op-20}.

\textbf{Effect validity}:
From Theorem
\ref{t:effect-from-symbolic-causes},
we know that the telluric base EIMs we attempt to add
at line
\ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}
are valid if their causes are valid.
From Theorem
\ref{t:leo-top-from-memo},
we know that the telluric base EIMs we attempt to add
at line
\ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}
are valid if their causes are valid.
Combining both cases, we know that
every attempt to
add a telluric base EIM by Algorithm \ref{alg:reduction-pass}
is valid if its causes are valid.

By Theorem
\ref{t:ethereal-closure-op-correct}
we know that the
Algorithm
\ref{alg:eim-set}
used
at line
\ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}
and
at line
\ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}
adds the ethereal closure of the telluric base EIMs
added at those lines.
Based what we have laid in this part of the proof
on \textbf{effect validity},
we can state that
\begin{equation}
\label{eq:reduction-op-correct-12}
\myparbox{
if the causes are valid, every attempt to
by Algorithm \ref{alg:leo-reduction-op} to
add their effect EIM adds
the ethereal closure of the effect EIM.
}
\end{equation}

\textbf{Top-down correctness}:
Consider an arbitrary top-down cause of
a reduced EIM at \Vloc{current}.
Call this EIM, \Veim{down}.
From line \ref{line:reduction-pass-20}
of Algorithm \ref{alg:reduction-pass},
we see that \Veim{up},
the bottom-up cause of a reduced EIM,
must be a completed EIM.
Call the length of \Veim{up}, \var{len}.
Since all EIMs are telluric,
\begin{equation}
\label{eq:reduction-op-correct-15}
\var{len} \ge 1.
\end{equation}
Using Theorem
\ref{t:symbolic-causes-from-effect},
we see that the right location of top-down cause must be
\begin{align}
\label{eq:reduction-op-correct-18}
& \Right{\Veim{down}} = \var{current} \subtract \var{len} & \\
\therefore \quad &
\Right{\Veim{down}} = \var{current} \subtract 1 \qquad & \text{using \eqref{eq:reduction-op-correct-15}} \\
\therefore \quad & \Right{\Veim{down}} < \var{current}, &
\end{align}
and from
\eqref{eq:reduction-op-correct-5},
an assumption for the theorem,
we see that \Veim{down} is correct.
Since the choice of \Veim{down} as a top-down cause at
\Vloc{current} was without loss of generality,
we can state that
\begin{equation}
\label{eq:reduction-op-correct-21}
\myparbox{
All of the top-down causes used
by Algorithm \ref{alg:leo-reduction-op} are
correct.
}
\end{equation}

\textbf{Iteration sets}:
For the purposes of this proof we divide the EIMs
added by 
Algorithm \ref{alg:reduction-pass} into a sequence
of sets,
$$\var{iter}[0], \var{iter}[1], \ldots \var{iter}[\var{n}].$$
$\var{iter}[0]$ is the set of EIMs
at line
\ref{line:reduction-pass-18} of Algorithm \ref{alg:reduction-pass},
before its main loop
from line
\ref{line:reduction-pass-20}
to line \ref{line:reduction-pass-50}.
$\var{iter}[\var{i}]$ is the set of EIMs
added during the \var{i}'th iteration of the main loop
of Algorithm \ref{alg:reduction-pass}.
Iterations are numbered starting with 1,
so that
$\var{iter}[1]$ is the set of EIMs
added during the first iteration of the main loop.

\textbf{Consistency and altitude}:
To show that 
Algorithm \ref{alg:reduction-pass} adds only
valid EIMs,
we proceed by induction on the sets in \var{iter}.
Let the induction hypothesis be that
\begin{equation}
\label{eq:reduction-op-correct-21b}
\myparbox{the EIMs in \var{iter}[\var{i}] are valid, unmemoized
and at altitude \var{i}.}
\end{equation}
At line
\ref{line:reduction-pass-18} of Algorithm \ref{alg:reduction-pass},
\Ves{i} contains only read EIMs and their
ethereal closure.
From \eqref{eq:reduction-op-correct-8},
an assumption for the theorem,
we know that
the EIMs in \var{iter}[0] are valid and unmemoized.
By Definition \ref{def:altitude},
read EIMs have altitude 1.
Using Theorem \ref{t:ethereal-altitude},
we see that
all EIMs in the ethereal closure of read EIMs
have altitude 1.
So all EIMs in \var{iter}[0] have altitude 1.
This shows \eqref{eq:reduction-op-correct-21b} for $\var{i} = 0$,
which is the basis of our induction.

For the step, we assume
\eqref{eq:reduction-op-correct-21b}
and we seek to show that
\begin{equation}
\label{eq:reduction-op-correct-21c}
\myparbox{the EIMs in \var{iter}[\var{i}+1] are valid, unmemoized
and at altitude \var{i}+1.}
\end{equation}
For the EIMs in
$\var{iter}[\var{i}+1]$,
the top-down causes are valid by
\eqref{eq:reduction-op-correct-21}
and the bottom-up causes are valid by the assumption for the
induction step so that,
using effect-correctness \eqref{eq:reduction-op-correct-12},
we see that
\begin{equation}
\label{eq:reduction-op-correct-23}
\myparbox{
all EIMs in
$\var{iter}[\var{i}+1]$ are valid.
}
\end{equation}

We next show that
all EIMs in
$\var{iter}[\var{i}+1]$
are unmemoized.
Again using
\eqref{eq:reduction-op-correct-21},
the induction step,
and \eqref{eq:reduction-op-correct-12},
we see that all of the causes
of EIMs in
$\var{iter}[\var{i}]$ are unmemoized.
We need to show that all of the EIMs in
$\var{iter}[\var{i}+1]$ are unmemoized.
By Theorem \ref{t:memoized-causes},
every memoized
effect either has a memoized cause,
or else has a Leo memo.
So if $\Veim{x} \in \var{iter}[\var{i}+1]$ is
memoized,
\Veim{x} must have a Leo memo.
From line
\ref{line:reduce-one-up-cause-25}
of Algorithm
\ref{alg:reduce-one-up-cause}
and from Theorem
\ref{t:leo-top-from-memo},
we see that if \Veim{x} has a Leo memo,
line \ref{line:reduce-one-up-cause-30}
of Algorithm
\ref{alg:reduce-one-up-cause},
will add a valid, ummemoized EIM.
So
\begin{equation}
\label{eq:reduction-op-correct-24}
\myparbox{
every EIM $\Veim{x} \in \var{iter}[\var{i}+1]$ is
unmemoized.
}
\end{equation}

To determine their altitude, we look at
the EIMs added
to $\var{iter}[\var{i}+1]$ by cases.
There are two cases:
\begin{enumerate}
\item
\label{case:reduction-op-correct-24}
Case \ref{case:reduction-op-correct-24}:
Those added at
line \ref{line:earley-reduction-op-20}
of Algorithm \ref{alg:earley-reduction-op}; and
\item
\label{case:reduction-op-correct-25}
Case \ref{case:reduction-op-correct-25}:
those added
at line \ref{line:leo-reduction-op-20}
of Algorithm \ref{alg:leo-reduction-op}.
\end{enumerate}

For Case \ref{case:reduction-op-correct-24},
the EIMs added are
the ethereal closure of
the reduced EIMs whose bottom-up cause
is in $\var{iter}[\var{i}]$.
By Definition \ref{def:altitude}
and Theorem \ref{t:ethereal-altitude},
the EIMs of
Case \ref{case:reduction-op-correct-24}
have altitude $\var{i}+1$.
For Case \ref{case:reduction-op-correct-25},
the EIMs added are
the ethereal closure of
the Leo reduced EIMs whose bottom-up cause
is in $\var{iter}[\var{i}]$.
By Theorems
\ref{t:leo-top-from-memo}
and \ref{t:ethereal-altitude},
the EIMs of
Case \ref{case:reduction-op-correct-24}
have altitude $\var{i}+1$.
In both cases, the EIMs added
have altitude $\var{i}+1$,
and therefore
\begin{equation}
\label{eq:reduction-op-correct-29}
\myparbox{
all EIMs in
$\var{iter}[\var{i}+1]$
have altitude $\var{i}+1$.
}
\end{equation}

Using 
\eqref{eq:reduction-op-correct-23},
\eqref{eq:reduction-op-correct-24}
and
\eqref{eq:reduction-op-correct-29},
we have
\eqref{eq:reduction-op-correct-21c},
the step of the induction,
and the induction.
From this we conclude that
\begin{equation}
\label{eq:reduction-op-correct-30}
\forall \;
  \var{i}, \text{$\var{iter}[\var{i}]$ is consistent.}
\end{equation}
and
\begin{equation}
\label{eq:reduction-op-correct-31}
\forall \;
  \var{i}, \Alt{\var{iter}[\var{i}} = \var{i}.
\end{equation}

\textbf{Completeness of the iteration sets}:
To show that 
after
Algorithm \ref{alg:reduction-pass} every set of
\var{iter} is complete,
we proceed by induction on the sets in \var{iter}.
Let the induction hypothesis be that
\begin{equation}
\label{eq:reduction-op-correct-35}
\myparbox{the EIMs in \var{iter}[\var{i}] are complete.}
\end{equation}
From \eqref{eq:reduction-op-correct-8},
an assumption for the theorem,
we have
\eqref{eq:reduction-op-correct-21b} for $\var{i} = 0$,
and this is the basis of our induction.

For the step, we assume
\eqref{eq:reduction-op-correct-35}
and we seek to show
\begin{equation}
\label{eq:reduction-op-correct-40}
\myparbox{the EIMs in \var{iter}[\var{i}+1] are complete.}
\end{equation}
For the EIMs in
$\var{iter}[\var{i}+1]$,
the top-down causes are complete by
\eqref{eq:reduction-op-correct-21}
and the bottom-up causes are complete by the assumption for the
induction step.
So we have complete sets of unmemoized, valid causes for
both bottom-up causes and top-down causes.

We now show that
Algorithm \ref{alg:reduction-pass}
pairs every bottom-up cause with
all of its matching top-down causes.
From line 
\ref{line:reduction-pass-20}
of Algorithm
\ref{alg:reduction-pass}
we see that all valid unmemoized bottom-up causes
are used in an outer loop.
From Theorem
\ref{t:memoize-transitions-correct}
and
line
\ref{line:reduce-one-up-cause-20}
of Algorithm
\ref{alg:reduce-one-up-cause},
we see that each 
of these unmemoized bottom-up causes
is paired with the matching set of valid top-down causes.

It remains to show that
no unmemoized, valid EIMs are omitted
from $\var{iter}[\var{i}+1]$
because their causes
are memoized.
By Theorem
\ref{t:memoized-effects},
all valid unmemoized items have valid unmemoized causes except
for non-trivial Leo top items.
Without loss of generality,
let a non-trivial Leo top item be
\Veim{top}.
In the case of \Veim{top},
by Theorem \ref{t:leo-memo-from-top},
there will be an valid unmemoized bottom-up cause
which matches the Leo memo,
so that
line \ref{line:reduce-one-up-cause-30}
of Algorithm
\ref{alg:reduce-one-up-cause}
will add the ethereal closure of \Veim{top}.
Therefore all unmemoized, valid EIMs are added
to
$\var{iter}[\var{i}+1]$.

This shows
\eqref{eq:reduction-op-correct-40},
the step of the induction,
and the induction.
From this we conclude that
\begin{equation}
\label{eq:reduction-op-correct-50}
\forall \;
  \var{i}, \text{$\var{iter}[\var{i}+1]$ is complete.}
\end{equation}

\textbf{Completeness}:
We have shown completeness for all of the Earley sets in
\eqref{eq:reduction-op-correct-50}.
The theorem requires that we show
completeness for \Ves{current} for
membership in \var{reduced-closure}
From
Theorems \ref{eq:reduction-op-correct-30},
and \ref{eq:reduction-op-correct-50},
we know that every iteration set is correct for
membership in \var{reduced-closure} --
contains all and only the valid EIMs.
By Theorem
\ref{t:altitude-is-finite}
we know that every valid EIM has a defined,
finite altitude,
and by
Theorem \ref{eq:reduction-op-correct-31},
we know every valid EIM is in the
iterations set whose index is the same
as the EIM's altitude.

It remains to show that the iteration sets
produced by
by Algorithm
\ref{alg:reduction-pass} capture all of the
reductions necessary.
The loop starting a line
\ref{line:reduction-pass-20}
of Algorithm
\ref{alg:reduction-pass} stops at the first
empty iteration set.
This is adequate if all iteration sets after
the first empty iteration set are also empty:
\begin{equation}
\label{eq:reduction-op-correct-60}
\var{iter}[\var{i}] = \emptyset \land \var{i} \le \var{j}
    \implies \var{iter}[\var{j}] = \emptyset.
\end{equation}
Showing \eqref{eq:reduction-op-correct-60} is the same
as showing that
\begin{equation}
\label{eq:reduction-op-correct-63}
\nexists \, \var{x}, 
    \var{iter}[\var{x}] = \emptyset \land
    \var{iter}[\var{x}+1] \neq \emptyset
\end{equation}
Suppose, for a reductio, that there was a
\var{x} that did not satisfy
\eqref{eq:reduction-op-correct-63}.
All EIMs in
$\var{iter}[\var{x}+1]$ will have altitude
$\var{x}+1$
and will be
\begin{enumerate}
\item
\label{case:reduction-op-correct-66}
Case \ref{case:reduction-op-correct-66}:
in the ethereal closure added
by Algorithm \ref{alg:earley-reduction-op}
at line \ref{line:earley-reduction-op-20}; or
\item
\label{case:reduction-op-correct-70}
Case \ref{case:reduction-op-correct-70}:
in the ethereal closure added
by Algorithm \ref{alg:leo-reduction-op}
at line \ref{line:leo-reduction-op-20}.
\end{enumerate}
In Case \ref{case:reduction-op-correct-66},
by Definition \ref{def:altitude},
they will require a bottom-up cause with
altitude \var{x}.
In Case \ref{case:reduction-op-correct-70},
also by Definition \ref{def:altitude},
they will again require a bottom-up cause with
altitude \var{x}.
But $\var{iter}[\var{x}] = \emptyset$ by assumption
for the reductio.
So there is no bottom-up cause that can create
the EIM's in either
Case \ref{case:reduction-op-correct-66}
or Case \ref{case:reduction-op-correct-70}.
This shows the reductio,
\eqref{eq:reduction-op-correct-63},
and therefore 
\eqref{eq:reduction-op-correct-60}.

\begin{sloppypar}
\textbf{Correctness}:
Let
$$ \var{reduction-pass-eims} = \bigcup_\var{i} \var{iter}[\var{i}]. $$
From the considerations in the part on \textbf{Completeness}
we conclude that
\var{reduction-pass-eims}
is complete for \Veimset{reduced-closure}.
From
\eqref{eq:reduction-op-correct-30},
we know that
\var{reduction-pass-eims} is consistent.
Therefore,
\var{reduction-pass-eims}
is correct for \Veimset{reduced-closure}.

\end{sloppypar}
\end{proof}

\begin{algorithm}[th]
\caption{Memoize transitions}
\label{alg:memoize-transitions}
\begin{algorithmic}[1]
\Procedure{Memoize transitions}{\Vloc{i}}
\For{every \Vsym{postdot}, a telluric postdot symbol of $\Ves{i}$}
\label{line:memoize-transitions-20}
\State Note: \Vsym{postdot} is ``Leo eligible" if it is
\State \hspace\algorithmicindent  Leo unique and its rule is right recursive
\If{\Vsym{postdot} is Leo eligible}
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to a LIM
\Else
\State Set $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\State \hspace\algorithmicindent to the set of EIM's at \Vloc{i} that have
\State \hspace\algorithmicindent \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize transitions}

The \var{transitions} table for \Ves{i}
is built once all EIMs have been
added to \Ves{i}.
We first look at the resource,
excluding the processing of Leo items.
The non-Leo processing can be done in
a single pass over \Ves{i},
in \Oc{} time per EIM.
Inclusive time and space are charged to the
Earley items being examined.
Overhead is charged to \Ves{i}.

We now look at the resource used in the Leo processing.
A transition symbol \Vsym{transition}
is Leo eligible if it is Leo unique
and its rule is right recursive.
(If \Vsym{transition} is Leo unique in \Ves{i}, it will be the
postdot symbol of only one rule in \Ves{i}.)
All but one of the determinations needed to decide
if \Vsym{transition} is Leo eligible can be precomputed
from the grammar,
and the resource to do this is charged to the parse.
The precomputation, for example,
for every rule, determines if it is right recursive.

One part of the test for
Leo eligibility cannot be done as a precomputation.
This is the determination whether there is only one dotted
rule in \Ves{i} whose postdot symbol is
\Vsym{transition}.
This can be done
in a single pass over the EIM's of \Ves{i}
that notes the postdot symbols as they are encountered
and whether any is enountered twice.
The time and space,
including that for the creation of a LIM if necessary,
will be \Oc{} time per EIM examined,
and can be charged to EIM being examined.

\begin{theorem}
\label{t:memoize-transitions-correct}
Algorithm \ref{alg:memoize-transitions}
is correct.
\end{theorem}

\begin{proof}
TODO: Make sure this accounts for the correctness of Leo memos.
TODO: Make sure this accounts for the completeness of all top-down causes.
\end{proof}

\begin{algorithm}[th]
\caption{Reduce one up-cause}
\label{alg:reduce-one-up-cause}
\begin{algorithmic}[1]
\Procedure{Reduce one up-cause}{\Veim{up}}
\State Note: Each pass through this loop is an EIM attempt
\State $\Vloc{orig} \gets \Origin{\Veim{up}}$
\State $\Vsym{lhs} \gets \LHS{\Veim{up}}$
\State $\Vloc{current} \gets \Current{\Veim{up}}$
\For{each $\var{down} \in \var{transitions}(\var{orig},\var{lhs})$}
\label{line:reduce-one-up-cause-20}
\State \Comment \var{down} is a ``postdot item'', either a Leo memo or an EIM
\If{\var{down} is a Leo memo}
\label{line:reduce-one-up-cause-25}
\State Perform a \Call{Leo reduction operation}{}
\label{line:reduce-one-up-cause-30}
\State \hspace\algorithmicindent for operands \var{current}, \Vleo{down}
\Else
\State Perform a \Call{Earley reduction operation}{}
\label{line:reduce-one-up-cause-50}
\State \hspace\algorithmicindent for operands \var{current}, \Veim{down}, \var{lhs}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduce one up-cause}
\label{p:reduce-one-up-cause}

To show that
\begin{equation*}
\var{transitions}(\Vloc{origin},\Vsym{lhs})
\end{equation*}
can be traversed in \Oc{} time,
we note
that the number of symbols is a constant
and assume that \Vloc{origin} is implemented
as a link back to the Earley set,
rather than as an integer index.
This requires that \Veim{work}
in \call{Reduction pass}{}
carry a link
back to its origin.
As implemented, Marpa's
Earley items have such links.

Inclusive time
for the loop over the EIM attempts
is charged to each EIM attempt.
Overhead is \Oc{} and caller-included.

\begin{algorithm}[th]
\caption{Earley reduction operation}
\label{alg:earley-reduction-op}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{current}, \Veim{down}, \Vsym{up}}
\State $\Vdr{effect} \gets \GOTO(\DR{\Veim{down}}, \Vsym{up})$
\State \Call{Add EIM set}{\Vdr{effect}, \Origin{\Veim{down}}, \var{current}}
\label{line:earley-reduction-op-20}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Earley Reduction operation}
\label{p:reduction-op}

Exclusive time and space is clearly \Oc.
\call{Earley reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.

\begin{algorithm}[th]
\caption{Leo reduction operation}
\label{alg:leo-reduction-op}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{current}, \Vleo{down}}
\State $[\Vdr{down}, \Vsym{up}, \Vloc{origin}] \gets \Vleo{down}$
\State $\Vdr{effect} \gets \GOTO(\DR{\Vdr{down}}, \Vsym{up})$
\State \Call{Add EIM set}{\Vdr{effect}, \Vloc{origin}, \var{current}}
\label{line:leo-reduction-op-20}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Leo reduction operation}
\label{p:leo-op}

Exclusive time and space is clearly \Oc.
\call{Leo reduction operation}{} is always
called as part of an EIM attempt,
and inclusive time and space is charged to the EIM
attempt.

\begin{algorithm}[!htp]
\caption{Add EIM set}\label{alg:eim-set}
\begin{algorithmic}[1]
\Procedure{Add EIM set}{$\Vdr{base}, \Vloc{origin}, Vloc{i}$}
\State $\Veim{confirmed} \gets [\Vdr{base}, \Vloc{origin}]$
\If{\Veim{base} is new}
\State Add \Veim{base} to \Vtable{i}\label{line:eim-set-10}
\EndIf
\State $\Vdrset{next} \gets \GOTO(\Vdr{base}, \epsilon)$\label{line:eim-set-20}
\For{every $\Vdr{next} \in  \Vdrset{next}$}
\If{\Vdr{next} is a quasi-prediction}
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{i}, \Vloc{i}]$
\label{line:eim-set-25}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:eim-set-30}
\EndIf
\Else
\State $\Veim{next} \gets [\Vdr{next}, \Vloc{origin}, \Vloc{i}]$
\label{line:eim-set-35}
\If{\Veim{next} is new}
\State Add \Veim{next} to \var{table}\label{line:eim-set-40}
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Adding a set of Earley items}
\label{p:add-eim-set}

\subsubsection{Complexity}
\label{p:ethereal-closure-op-complexity}

This operation adds the ethereal closure
of a confirmed EIM
item.
Inclusive time and space is charged to the
calling procedure.

By theorem \ref{t:ethereal-closure-Oc},
computing the ethereal closure is \Oc{}.
We show that other time charged is also \Oc{}
by singling out the two non-trivial cases:
checking that an Earley item is new,
and adding it to the Earley set.
\Marpa{} checks whether an Earley item is new
in \Oc{} time
by using a data structure called a PSL.
PSL's are the subject of Section \ref{s:per-set-lists}.
An Earley item can be added to the current
set in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

The space required for added EIM added is at most
for the \Vdr{base},
one for every transition over a null postdot symbol,
and
one for every prediction.
The number of EIMs that result from
transitions over null postdot symbols is limited
by the maximum length of the RHS of a rule,
which is constant for a given \Cg{}.
At any \Vloc{i}, the number of predictions is
at most the number of rules in \Cg{}.
The number of EIMs that result from
predictions
is therefore constant for a given \Cg{}.
Summing the space, we see that all space
requirement are constant, so that the space
is \Oc{} per call.

\subsubsection{Prediction EIM complexity}
\label{p:prediction-op-complexity}

Looking specifically at predictions,
From the discussion in
\ref{p:ethereal-closure-op-complexity}, we see that
no time or space is ever charged
to a predicted Earley item.
At most one attempt to add a \Veim{predicted} will
be made per attempt to add a \Veim{confirmed},
so that the total resource charged
remains \Oc.

\subsubsection{Null transition correctness}
\label{p:prediction-op-correct}

\begin{theorem}
\label{t:ethereal-closure-op-correct}
Algorithm \ref{alg:eim-set} adds all and only the EIMs
for the ethereal closure of \Veim{base}.
\end{theorem}

\begin{proof}
We first examine the effect Leo memoization will
have
on Algorithm \ref{alg:eim-set}.
We note the
Algorithm \ref{alg:eim-set} is only called when
\Veim{base} is not Leo memoized.
By Theorem \ref{t:ethereal-closure-is-Leo-disjoint},
we see that if \Veim{base} is not Leo memoized,
none of the other EIMs in its ethereal closure
will be Leo memoized.
Therefore,
we may ignore Leo memoization in what follows.

By inspection, we see that
Algorithm \ref{alg:eim-set} adds items
at lines
\ref{line:eim-set-10},
\ref{line:eim-set-30}
and \ref{line:eim-set-40}.
That the addition of \Veim{base}
at line \ref{line:eim-set-10}
is complete,
consistent and therefore correct,
follows directly from
inspection of
the pseudo-code.

Other than \Veim{base} itself,
all EIMs in the ethereal closure of
\Veim{base} are the product of
a series of null scans
and predictions.
These operations never change the current location --
it will always be that of \Veim{base}.
The origin only changes if the operation is a prediction --
the origin of a predictions is the same as its current
location.
This remains true for the null scan of a predictions --
its origin is that of its top-down cause, but since
that top-down cause is a prediction,
the origin is the same as if it was a prediction.
By induction, we see that the origin of
all quasi predictions must be the same as
the current location of the prediction.

Let \Veim{new} be a EIM in the ethereal
closure,
other than \Veim{base} itself.
From the preceding analysis,
we see that
the current location of
\Veim{new}
depends
only
on its value in \Veim{base}.
The origin of \Veim{new} depends
on two things:
whether or not \Veim{new} is a quasi-prediction,
and the appropriate location value in \Veim{base}.

There is, therefore,
for each dotted rule,
only one correct set of values
for origin and current location.
These locations are
the ones
used by Algorithm \ref{alg:eim-set}
in lines
\ref{line:eim-set-25}
and \ref{line:eim-set-35}.
Therefore the set of correct EIMs corresponds
one-to-one
with the dotted rules
and, from inspection of
Algorithm \ref{alg:eim-set},
this is the set added by
Algorithm \ref{alg:eim-set}
at lines
\ref{line:eim-set-30}
and \ref{line:eim-set-40}.

It remains to show that the set of dotted rules on
which the EIMs added
at lines \ref{line:eim-set-30}
and \ref{line:eim-set-40}
are based
is correct.
The EIMs added at lines
\ref{line:eim-set-30}
and \ref{line:eim-set-40} are based on
the dotted rules found
at line
\ref{line:eim-set-20}.
By theorem \ref{t:ethereal-closure-dr-correct},
line \ref{line:eim-set-20} of
Algorithm \ref{alg:eim-set} the
transitive closure of null transitions from
the dotted rule \Veim{base} found
by line \ref{line:eim-set-20} is
complete, consistent and therefore correct.
\end{proof}

\begin{theorem}\label{t:quasi-completion-correct}
Let \Veim{base} be a quasi-completion.
If Algorithm \ref{alg:eim-set} adds \Veim{base},
it also adds all null transitions from it,
including the the completion EIM.
\end{theorem}

\begin{proof}
For null transitions,
the result follows directly
from theorem \ref{t:ethereal-closure-op-correct}.
By the definition of completion EIM,
a completion EIM is the result of null transition
from any of its quasi-completions,
so that case also follows
from theorem \ref{t:ethereal-closure-op-correct}.
\end{proof}

\begin{theorem}\label{t:prediction-correct}
If Algorithm \ref{alg:eim-set} adds \Veim{base},
it also adds all predictions which are null transitions
from it,
including the valid quasi-prediction EIMs.
\end{theorem}

\begin{proof}
The predictions
and quasi-predictions are
null transitions form \Veim{base},
so the result follows directly
from theorem \ref{t:ethereal-closure-op-correct}.
\end{proof}

\subsection{Per-set lists}
\label{s:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, x]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a link direct to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to or very similar to PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this \doc{}.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
the PSL for every previous Earley set, \Vloc{i},
tracks the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{\Cdr},
which is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's each time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

As before,
where \Ves{i} is an Earley set,
let \Vloc{i} be its location,
and vice versa.
\Vloc{i} is an integer which is
assigned as Earley sets are created.
We can easily assign a zero-based numbering
to the dotted rules of the grammar,
call it $\ID{\Vdr{x}}$,
and this can be used as the integer ID of a dotted rule.
Let $\PSL{\Ves{x}}{\var{y}}$
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}.

Consider the case where Marpa is building \Ves{j}
and wants to check whether Earley item \Veim{x} is new,
where $\Veim{x} = [ \Vdr{x}, \Vorig{x} ]$.
To check if \Veim{x} is new,
Marpa checks
\begin{equation*}
\var{time-stamp} = \PSL{\Ves{x}}{\ID{\Vdr{x}}}
\end{equation*}
If the entry has never been used,
we assume that $\var{time-stamp} = \Lambda$.
If $\var{time-stamp} \ne \Lambda \land \var{time-stamp} = \Vloc{j}$,
then \Veim{x} is not new,
and will not be added to the Earley set.

If $\Vloc{p} = \Lambda \lor \var{time-stamp} \ne \Vloc{j}$,
then \Veim{x} is new.
\Veim{x} is added to the Earley set,
and a new time-stamp is set, as follow:
\begin{equation*}
\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets \Vloc{j}.
\end{equation*}

\subsection{Complexity summary}

For convenience, we summarize
the complexity results
of this section here,
as theorems.

\begin{theorem}
\label{t:added-eim-charge}
The time and space charged to an Earley item
which is actually added to the Earley sets
is \Oc.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

\begin{theorem}
\label{t:dup-eim-time}
The time charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

For evaluation purposes, \Marpa{} adds a link to
each EIM that records each attempt to
add that EIM,
whether originally or as a duplicate.
Traditionally, complexity results treat parsers
as recognizers, and such costs are ignored.
This will be an issue when the space complexity
for unambiguous grammars is considered.

\begin{theorem}
\label{t:dup-eim-space}
The space charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc{} if links are included,
zero otherwise.
\end{theorem}

\begin{proof}
The theorem follows from collecting the results
in the complexity discussions of this section.
\end{proof}

\begin{theorem}
\label{t:prediction-time}
No space or time is charged to predicted Earley items,
or to attempts to add predicted Earley items.
\end{theorem}

\begin{proof}
As noted in Section \ref{p:add-eim-set},
the time and space used by predicted Earley items
and attempts to add them is charged elsewhere.
\end{proof}

\section{Correctness}
\label{s:correctness}

We are now is a position to show that Marpa is correct.
\begin{theorem}
\label{t:marpa-is-correct}
\textup{ $\myL{\Marpa,\Cg} = \myL{\Cg}$ }
\end{theorem}

\begin{proof}
We proceed by induction on the Earley sets.
As the basis of the induction,
we note that
Algorithm \ref{alg:top}
at line \ref{line:top-20},
calls Algorithm \ref{alg:initial}.
By Theorem
\ref{t:initial-op-correct},
after
line \ref{line:top-20},
Earley set 0 is correct.

As the step of the induction,
we assume that we
are
at line \ref{line:top-30}
of
Algorithm \ref{alg:top},
about to process
the Earley set at \Vloc{i}.
We assume for the induction step
that
\begin{equation}
\label{eq:marpa-is-correct-20}
\forall \, \Vloc{h} \mid 0 \le \var{h} < \Vloc{i} \implies \text{\Vtable{h} is correct.}
\end{equation}

Line
\ref{line:top-33}
executes the read pass for Earley set \var{i}.
From
\eqref{eq:marpa-is-correct-20}
and Theorem
\ref{t:read-op-correct},
we see that,
after the execution
of line
\ref{line:top-33},
\begin{equation}
\label{eq:marpa-is-correct-40}
\text{the ethereal closure of the read EIMs at \Vloc{i} is correct.}
\end{equation}

Next,
line \ref{line:top-40}
executes the reduction pass for Earley set \var{i}.
From
\eqref{eq:marpa-is-correct-20},
\eqref{eq:marpa-is-correct-40}
and Theorem
\ref{t:reduction-op-correct},
we see that,
after the execution
of line
\ref{line:top-40},
\begin{equation}
\label{eq:marpa-is-correct-50}
\text{the ethereal closure of the reduced EIMs at \Vloc{i} is correct.}
\end{equation}

By assumption for the step,
Earley set \Vloc{i} cannot be Earley set 0,
so we know vacously that
\begin{equation}
\label{eq:marpa-is-correct-53}
\text{the ethereal closure of start EIMs at \Vloc{i} is correct.}
\end{equation}
From
\eqref{eq:marpa-is-correct-40},
\eqref{eq:marpa-is-correct-50}
and
\eqref{eq:marpa-is-correct-53}
we see that the ethereal closure of
the set of telluric EIMs at \Vloc{i} is correct.
By theorem
\ref{t:ethereal-closure-op-correct}
we know that the set of ethereal EIMs at \Vloc{i}
is correct.
Since every EIM is either telluric or ethereal,
we know that Earley set \Vloc{i} is correct.
This shows the step of the induction,
and the induction.

We know from the induction
and line \ref{line:top-30}
of Algorithm \ref{alg:top},
that the Earley set at \loc{\Vsize{w}}
is correct.
Therefore,
by Theorem
\ref{t:algorithm-correct},
it contains
the accept EIM if and only if
\Cw{} is in the language of the grammar,
$\var{L}(\Cg)$, so that
\begin{equation}
\label{eq:marpa-is-correct-60}
\Veim{accept} \in \Vtables{Marpa} \equiv \Cw \in \var{L}(\Cg).
\end{equation}

From \eqref{eq:def-implementation-accepts},
we know that an algorithm accepts an input if
and only if the accept EIM is in its tables.
By Theorem \ref{t:accept-eim-not-memoized},
we know that the accept EIM is not memoized.
Using
\eqref{eq:def-implementation-accepts} and
\eqref{eq:marpa-is-correct-60}, we have
\begin{equation*}
\Cw{} \in \var{L}(\alg{Marpa}, \Cg) \equiv \Cw \in \var{L}(\Cg).
\end{equation*}
\end{proof}

\section{Linear complexity results}
\label{s:linear}

\subsection{Nulling symbols}
\label{s:nulling}

Recall that Marpa grammars,
without loss of generality,
contain neither empty rules or
properly nullable symbols.
This corresponds directly
to a grammar rewrite in the \Marpa{} implementation,
and its reversal during \Marpa's evaluation phase.
For the correctness and complexity proofs in this \doc{},
we assume an additional rewrite,
this time to eliminate nulling symbols.

Elimination of nulling symbols is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether a grammar \Cg{} accepts
an input \Cw{}
will not depend on the nulling symbols in its rules.

In its implementation,
\Marpa{} does not directly rewrite the grammar
to eliminate nulling symbols.
But nulling symbols are ignored in
creating the dotted rules,
and must be restored during \Marpa's evaluation phase,
so that the implementation and
this simplification for theory purposes
track each other closely.

\subsection{Comparing Earley items}

\begin{definition}
An Earley item
$\Veim{x} = [\Vdr{x}, \Vorig{x}]$
\dfn{corresponds}
to another Earley item \Veim{y}
if and only if
$\Veim{y} = [\Vdr{y}, \Vorig{x}].$
\end{definition}

\begin{definition}
The Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{consistent} if and only if
all of its EIM's correspond to
EIM's in
\EVtable{\Leo}{i}.
\end{definition}

\begin{definition}
The Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{complete} if and only if for every
Earley item in \EVtable{\Leo}{i}
there is a corresponding Earley item in
\EVtable{\Marpa}{i}.
\end{definition}

\begin{definition}
A Marpa Earley set is \dfn{correct}
if and only that Marpa Earley set is complete
and consistent.
\end{definition}

\subsection{Complexity of each Earley item}

For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
We showed that this rewrite
is without loss of generality
in Section \ref{s:nulling},
when we examined correctness.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

\begin{lemma}\label{l:nulling-rewrite}
All time and space required
to rewrite the grammar to eliminate nulling
symbols, and to restore those rules afterwards
in the Earley sets,
can be allocated
to the Earley items
in such a way that each Earley item
requires \Oc{} time and space.
\end{lemma}

\begin{proof}
The time and space used in the rewrite is a constant
that depends on the grammar,
and is charged to the parse.
The reversal of the rewrite can be
done in a loop over the Earley items,
which will have time and space costs
per Earley item,
plus a fixed overhead.
The fixed overhead is \Oc{}
and is charged to the parse.
The time and space per Earley item
is \Oc{}
because the number of
rules into which another rule must be rewritten,
and therefore the number of Earley items
into which another Earley item must be rewritten,
is a constant that depends
on the grammar.
\end{proof}

\begin{theorem}\label{t:O1-time-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item,
and each attempt to
add a duplicate Earley item,
requires \Oc{} time.
\end{theorem}

\begin{theorem}\label{t:O1-space-per-eim}
All space in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item
requires \Oc{} space and,
if links are not considered,
each attempt to add a duplicate
Earley item adds no additional space.
\end{theorem}

\begin{theorem}\label{t:O1-links-per-eim}
If links are considered,
all space in \Marpa{} can be allocated
to the Earley items
in such a way that each Earley item
and each attempt to
add a duplicate Earley item
requires \Oc{} space.
\end{theorem}

\begin{proof}[Proof of Theorems
\ref{t:O1-time-per-eim},
\ref{t:O1-space-per-eim},
and \ref{t:O1-links-per-eim}]
These theorems follows from the observations
in Section \ref{s:pseudocode}
and from Lemma \ref{l:nulling-rewrite}.
\end{proof}

The same complexity results apply to \Marpa{} as to \Leo,
and the proofs are very similar.
\Leo's complexity results\cite{Leo1991}
are based on charging
resource to Earley items,
as were the results
in Earley's paper\cite{Earley1970}.

Earley\cite{Earley1970} shows that,
for unambiguous grammars,
every attempt to add
an Earley item will actually add one.
In other words, there will be no attempts to
add duplicate Earley items.
Earley's proof shows that for each attempt
to add a duplicate,
the causation must be different --
that the EIM's causing the attempt
differ in either their dotted
rules or their origin.
Multiple causations for an Earley item
would mean multiple derivations
for the sentential form that it represents.
That in turn would mean that
the grammar is ambiguous,
contrary to assumption.

\begin{theorem}
\label{t:tries}
For an unambiguous grammar,
let \var{z} be the number of EIMs
in all the Marpa tables,
\begin{equation*}
    \var{z} = \Rtablesize{\Marpa}.
\end{equation*}
and let \var{tries} be the
number of attempts to add
Leo memos and Earley items.
Then $$\var{tries} = \order{\var{z}}.$$
\end{theorem}

\begin{proof}
Let
\var{leo-tries} be the number of attempt to add Leo memos,
\var{telluric-tries} be the number of attempts to add telluric EIMs,
and \var{ethereal-tries}
be the number of attempts to add ethereal EIMs.
Then
\begin{multline}
\label{eq:tries-5}
\var{tries} =
\var{telluric-tries} + \var{ethereal-tries} + \var{leo-tries}.
\end{multline}

We consider first the telluric EIMs.
Let
\var{start-tries} be the number of attempts to add the start EIM,
\var{read-tries} be the number of attempts to add read EIMs,
and \var{reduction-tries} be the number of attempts to add reduction EIMs.
\begin{multline}
\label{eq:tries-10}
\var{telluric-tries} = \\
\var{read-tries} + \var{reduction-tries} + \var{start-tries},
\end{multline}

Let \var{start-tries} be the number of attempts to add the initial item to
the Earley sets.
It is clear from the pseudocode
that there will be no attempts to add duplicate start EIM's.
and that
\begin{equation}
\label{eq:tries-15}
\var{start-tries} = 1 = \Oc
\end{equation}

Let \var{read-tries} be the number of attempted read operations in
Earley set \Vloc{j}.
Marpa attempts a read operation,
in the worst case,
once for every EIM in the Earley set
at $\Vloc{i} \subtract 1$.
Therefore, the number of attempts
to add read EIMs at Earley set \Vloc{i}
must be less than equal to \bigsize{\Etable{\var{i} \subtract 1}},
and
the number
of actual Earley items at
$\Vloc{i} \subtract 1$.
\begin{equation}
\label{eq:tries-18}
\var{read-tries} \le \sum\limits_{i=1}^{n}{ \bigsize{\Etable{\decr{i}}} } = \order{\var{z}}
\end{equation}

The most complicated case is Earley reduction.
Recall that \Ves{i} is the current Earley set.
Consider the number of reductions attempted.
\Marpa{} attempts to add an Earley reduction result
once for every pair of matching causes,
\begin{equation*}
[\Veim{down}, \Veim{up}].
\end{equation*}
Let
\begin{equation*}
\begin{split}
& \Veim{up} = [ \Vdr{up}, \Vloc{up-origin}, \var{i} ]  \\
 \land \quad & \Vsym{transition} = \LHS{\Vdr{up}}. \\
\end{split}
\end{equation*}

We now attempt to
put an upper bound on number of possible matching pairs of
causes.
We have $\Veim{up} \in \Ves{i}$,
and therefore
\begin{equation}
\label{eq:tries-20}
\text{there are at most
$\bigsize{\Vtable{i}}$ choices for \Veim{up}.}
\end{equation}

Let the number of dotted rules be \Vsize{\Cdr}.
We can show that the number of possible choices of
\Veim{down} is at most \Vsize{\Cdr}, by a reductio.
Suppose, for the reductio,
there were more than \Vsize{dr} possible choices of \Veim{down}.
Then there are two possible choices of \Veim{down} with
the same dotted rule.
Call these \Veim{choice1} and \Veim{choice2}.
We know, by the definition of matching causes, that
\begin{align*}
 & \Veim{down} \in \Ves{up-origin}
   \quad \text{and therefore we have} \\
 & \Veim{choice1} \in \Ves{up-origin} \quad \text{and} \\
 & \Veim{choice2} \in \Ves{up-origin}.
\end{align*}
Since all EIM's in an Earley set must differ,
and
\Veim{choice1} and \Veim{choice2} both share the same
dotted rule,
so they must differ in their origin.
But two different origins would produce two different derivations for the
reduction,
and by
Theorem \ref{eq:def-implementation-accepts},
this would mean that the parse was ambiguous.
This is contrary to the assumption for the theorem
that the grammar is unambiguous.
This shows the reductio
and that
\begin{equation}
\label{eq:tries-25}
\myparbox{
the number of choices for \Veim{down},
compatible with \Vorig{up}, is as most \Vsize{dr}.
}
\end{equation}

From
\eqref{eq:tries-20}
and
\eqref{eq:tries-25},
we see that the number of possible matching
cause pairs for reductions at the Earley set at \Vloc{i}
is
\begin{equation}
\label{eq:tries-27}
\Vsize{\Cdr} \times \bigsize{\Vtable{i}}.
\end{equation}
Since \Vsize{\Cdr} is a constant that depends on \Cg{},
\begin{align}
\label{eq:tries-28}
\var{reduction-tries} & = \sum\limits_{i=0}^{n}{\Vsize{\Cdr} \times \bigsize{\Vtable{i}}} \\
  & = \Vsize{\Cdr} \times \sum\limits_{i=0}^{n}{\bigsize{\Vtable{i}}} \\
  & = \Oc \times \order{\var{z}} \\
  & = \order{\var{z}}
\end{align}

We have now set a limit on the number of attempts
for all of the telluric EIMs.
We return to
\eqref{eq:tries-10}.
Using
\eqref{eq:tries-15},
\eqref{eq:tries-18}
and
\eqref{eq:tries-28},
\begin{align}
\label{eq:tries-30}
\var{telluric-tries} & = \var{read-tries} + \var{reduction-tries} \\
\notag & \qquad \qquad + \var{start-tries}  \\
  & = \Oc + \order{\var{z}} + \order{\var{z}} \\
  & = \order{\var{z}}
\label{eq:tries-33}
\end{align}

Recall that \var{ethereal-tries} was the number of attempts to add ethereal
items in
Earley set \Vloc{j}.
\Marpa{} includes ethereal closure
in its read and reduction operations.
Ethereal EIMs are only added if their telluric base EIM
is new,
so that there will never be duplicate attempts
to add ethereal EIMs for the same telluric base EIM.
The ethereal EIMs which might be added for a given
telluric base are predictions and null-scans.
The number of null-scans is limited by the RHS length,
call it \var{rhs-max},
of the longest rule in \Cg.
The number of predictions is limited by the number of rules
in Cg{}, \Vsize{\Crules}.
so that
for every telluric Earley item added,
\begin{equation*}
\var{ethereal-tries} = (\var{rhs-max} + \Vsize{Crules})  \times \var{telluric-tries}
\end{equation*}
and because \var{rhs-max} and
\Vsize{\Crules} are constants the depend on \Cg{},
\begin{equation}
\label{eq:tries-42}
\var{ethereal-tries} = \Oc \times \var{telluric-tries}.
\end{equation}
And using
\eqref{eq:tries-30},
\begin{equation}
\label{eq:tries-45}
\var{ethereal-tries} = \order{\var{z}}.
\end{equation}

Recall that \var{leo-tries} was the number of attempted Leo reductions in
Earley set \Vloc{j}.
For Leo reduction,
we note that by its definition,
duplicate attempts at Leo reduction cannot occur.
From the pseudo-code of Sections \ref{p:reduce-one-up-cause}
and \ref{p:leo-op},
we know there will be at most one Leo reduction for
every Earley item in the Earley tables,
so that
\Vloc{j}.
\begin{equation}
\label{eq:tries-50}
\var{leo-tries} = \order{\var{z}}
\end{equation}

Returning to
\eqref{eq:tries-5}:
\begin{multline}
\var{tries} =
\var{telluric-tries} + \var{ethereal-tries} + \var{leo-tries}.
\end{multline}
We have, using the results of
\eqref{eq:tries-33},
\eqref{eq:tries-45}
and
\eqref{eq:tries-50},
\begin{equation}
\var{tries} =
\order{\var{z}}
+ \order{\var{z}}
+ \order{\var{z}}
= \order{\var{z}}.\qedhere
\end{equation}
\end{proof}

As a reminder,
we follow tradition by
stating complexity results in terms of \var{n},
setting $\var{n} = \Vsize{\Cw}$,
the length of the input.

\begin{theorem}\label{t:leo-right-recursion}
Either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let the constant \var{c} be the number
of symbols.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
(Since for the purposes of these
complexity results we ignore nulling symbols,
the rightmost symbol of a string will also be its rightmost
telluric symbol.)
So part of the rightmost derivation must take the form
\begin{equation*}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation*}
But the first step of this derivation sequence must use a rule of the
form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
the number of Earley items produced by
\Leo{} when parsing input \Cw{} with an LR-regular grammar \Cg{} is
\begin{equation*}
\order{\Vsize{\Cw}} = \order{\var{n}}.
\end{equation*}
\Marpa{} may produce more Earley items than \Leo{}
because
\Marpa{} does not apply Leo memoization to Leo sequences
which do not contain right recursion.

By the definition of an EIM,
and the construction of a Leo sequence,
it can be seen that a Leo sequence
corresponds step-for-step with a
right derivation.
It can therefore be seen that
the number of EIM's in the Leo sequence
and the number of right derivation steps
in its corresponding right derivation
will be the same.

Consider one EIM that is memoized in \Leo{}.
If not memoized because it is not a right recursion,
this EIM will be expanded to a sequence
of EIM's.
How long will this sequence of non-memoized EIM's
be, if we still continue to memoize EIM's
which correspond to right recursive rules?
The EIM sequence, which was formerly a memoized Leo sequence,
will correspond to a right
derivation that does not include
any steps that use right recursive rules.
By Theorem \ref{t:leo-right-recursion},
such a
right derivation can be
of length at most \var{c1},
where \var{c1} is a constant that depends on \Cg{}.
As noted, this right derivation has
the same length as its corresponding EIM sequence,
so that each EIM not memoized in \Marpa{} will expand
to at most \var{c1} EIM's.

The number of EIM's per Earley set
for an LR-regular grammar in a \Marpa{} parse
is less than
\begin{equation*}
    \var{c1} \times \order{\var{n}} = \order{\var{n}}.
\end{equation*}

LR-regular grammars are unambiguous, so that
by Theorem \ref{t:tries},
the number of attempts that \Marpa{} will make to add
EIM's is less than or equal to
\var{c2} times the number of EIM's,
where \var{c2} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is
\begin{equation*}
    \var{c2} \times \order{\var{n}}
    = \order{\var{n}}.\qedhere
\end{equation*}
\end{proof}

\section{Other complexity results}
\label{s:other-complexity}

\begin{theorem}
\label{t:eim-count}
For a context-free grammar,
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} = \order{\var{n}^2}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
By Theorem \ref{t:es-count},
the size of the Earley set at \Vloc{i}
is $\order{\var{i}}$.
Summing over the length of the input,
$\Vsize{\Cw} = \var{n}$,
the number of EIM's in all of \Marpa's Earley sets
is
\begin{equation*}
\sum\limits_{\Vloc{i}=0}^{\var{n}}{\order{\var{i}}}
= \order{\var{n}^2}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:ambiguous-tries}
For a context-free grammar,
the number of attempts to add
Earley items is $\order{\var{n}^3}$.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries},
we see that the only bound that required
the assumption that \Cg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Recall that \var{z} was the total number
of Earley items in the Earley tables.
By Theorem \ref{t:eim-count},
\begin{equation*}
\var{z} = \Rtablesize{\Marpa} = \order{\var{n}^2}.
\end{equation*}

Looking again at \var{reduction-tries}
for the case of ambiguous grammars,
the only place we used the assumption that \Cg{}
was unambiguous was in
the count of matching pairs.
\begin{equation*}
[\Veim{down}, \Veim{up}].
\end{equation*}
We need to look at this again.
We did not use the fact that the grammar was unambigous in counting
the possibilities for \Veim{up}, but
we did make use of it in determining the count of possibilities
for \Veim{down}.
We know still know that
\begin{equation*}
\Veim{down} \in \Ves{up-origin},
\end{equation*}
where
\Vloc{up-origin} is the origin of \Veim{up}.

In the proof of Theorem \ref{t:tries},
we calculated
the number of possible matching
cause pairs for reductions at the Earley set at \Vloc{i}
in
\eqref{eq:tries-27}
as
\begin{equation}
\label{eq:ambiguous-tries-27}
\Vsize{\Cdr} \times \bigsize{\Vtable{i}},
\end{equation}
where
\Vsize{\Cdr} was the number of dotted rules.
In showing that the number of dotted rules bounded the choice
of \Veim{down},
we used the assumption that \Cg{} was unambiguous.
Relaxing this assumption,
and looking at the worst case,
every EIM in \Ves{up-origin} is a possible
match, so that
the number of possibilities for \Veim{down} now grows to
\size{\Ves{up-origin}}, and
\eqref{eq:ambiguous-tries-27} becomes
\begin{equation}
\label{eq:ambiguous-tries-30}
\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}.
\end{equation}

Revisiting \eqref{eq:tries-28},
it now becomes
\begin{equation}
\label{eq:ambiguous-tries-28}
\var{reduction-tries} = \sum\limits_{i=0}^{n}{\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}}.
\end{equation}
Our best bound for the size of the Earley sets involved comes from
Theorem \ref{t:es-count}, and is
$\Vsize{\Cw} = \var{n}$,
so that
\eqref{eq:ambiguous-tries-28}
becomes
\begin{align}
\label{eq:ambiguous-tries-33}
\var{reduction-tries} & = \sum\limits_{i=0}^{n}{\bigsize{\Vtable{up-origin}} \times \bigsize{\Vtable{i}}} \\
& = \order{\var{n}^3}.
\label{eq:ambiguous-tries-35}
\end{align}

We now reexamine the count of the number
of attempts to add Leo memos and Earley items,
this time without assuming ambiguity.
Our worst case cannot be worse than the sum of
the entire count under the assumption of unambiguity,
plus any counts that needed to be revised
when the assumption is relaxed.
The count assuming unambiguity was
$\order{\var{z}}$,
and from Theorem \ref{t:es-count} we know
that
\begin{equation}
\label{eq:ambiguous-tries-36}
\order{\var{z}} = \order{\var{n}^2}.
\end{equation}
The one count that we needed to revise
in order to relax the assumption of unambiguity is
\eqref{eq:ambiguous-tries-35}.

Therefore the count for the ambiguous case is not worse
than
\begin{equation*}
\var{tries} = \order{\var{n}^2} + \order{\var{n}^3} = \order{\var{n}^3}.
\qedhere
\end{equation*}
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
By assumption, \Cg{} is unambiguous, so that
by Theorem \ref{t:tries},
and Theorem \ref{t:eim-count},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{}
for unambiguous grammars is \order{\var{n}^2}.
\end{proof}

\begin{RaggedRight}
\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}
\end{RaggedRight}

\begin{proof}
By Theorem \ref{t:O1-time-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\begin{RaggedRight}
\begin{theorem}\label{t:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}
\end{RaggedRight}

\begin{proof}
By Theorem \ref{t:O1-space-per-eim}
and Theorem \ref{t:eim-count}.
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{t:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{RaggedRight}
\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}
\end{RaggedRight}

\begin{proof}
By Theorem \ref{t:O1-links-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\bibliographystyle{plain}

\begin{thebibliography}{10}
\RaggedRight

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Marpa-2013}
Jeffrey~Kegler.
\newblock Marpa, a practical general parser: the recognizer.
\newblock \url{https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2015: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\bibitem{Wich2005}
Klaus Wich.
\newblock Ambiguity functions of context-free grammars and languages.
\newblock Ph.D. Thesis, Universitt Stuttgart, 2005.
\newblock \url{http://elib.uni-stuttgart.de/opus/volltexte/2005/2282}

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}
